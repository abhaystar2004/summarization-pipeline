["\nThis is a semi-public forum for participating in Ethereum\u2019s research efforts, including but not limited to:\n\nProof-of-Stake\nScaling solutions\nEVM improvements\nLow-level protocol improvements\nEconomics\n\nprotocol economics\nResource pricing economics\n\n\nOther second-level features\n\nThis is not the place for:\n\ngeneric ethereum discussion.  For that visit r/ethereum.\ndiscussing specific EIPs.  For that visit the Ethereum Magicians forum.\ntechnical questions and ELI5s. For that visit the StackExchange.\n\nIf your signal-to-noise ratio gets too low, you will be banned from ethresear.ch.  So please keep discussions information-rich.  Posting on this site is accepting releasing your submitted content into the public domain (CC0).\nForum Features!\n1. LaTeX Equations\nThis forum supports \\LaTeX equations between $dollar signs$.  The default LaTeX style is the \u201cinline\u201d style which looks like \\sum_{k=0}^n {n \\choose k} = 2^n , in text this is\n$ \\sum_{k=0}^n {n \\choose k} = 2^n $\n\nHowever, if you start your equation with $$ on it\u2019s own beginning and teminating line like,\n$$\n\\sum_{k=0}^n {n \\choose k} = 2^n\n$$\n\nit looks like this\n\n\\sum_{k=0}^n {n \\choose k} = 2^n\n\n\n2. Graphviz diagrams\nSee the documentation for a list of examples to build your graph.\n[graphviz engine=dot]\ndigraph {\n  concentrate=true;\n  a[color=red, style=filled, fillcolor=pink];\n  b[shape=diamond];\n  a -> b;\n  b -> c;\n  c -> a;\n  d -> c;\n  e -> c;\n  e -> a;\n  a -> e;\n}\n[/graphviz]\n\n\n\n\n\n\na\n\na\n\n\n\nb\n\nb\n\n\n\na->b\n\n\n\n\n\ne\n\ne\n\n\n\na->e\n\n\n\n\n\n\nc\n\nc\n\n\n\nb->c\n\n\n\n\n\nc->a\n\n\n\n\n\nd\n\nd\n\n\n\nd->c\n\n\n\n\n\ne->c\n\n\n\n\n\n\n3. YUML diagrams\nYUML diagrams allow making nice little graphs within your posts.\nThey have an idiosyncratic but fairly simple markup language.\nExample:\n\n[yuml]\n[foo{bg:cornsilk}]--[baz]\n[foo]->[bar{bg:orange}]\n[baz]-.->[qux]\n[qux]--label>[bar]\n[/yuml]\n\n\n[yuml]\n[Vote Message|+Source hash;-Source height;+Target hash;-Target height|+Withdraw();+Last_commit]\n[/yuml]\n\n\n4. Images!\nUnsurprisingly, we also support images.\nu1614\u00d7800 57.9 KB\n\nUseful sites\n\nEthereum dot org - https://ethereum.org/\nUpgrading Ethereum by Ben Edgington - https://eth2book.info/\n\n", "\nBy me, @rishotics and team\nSpecial thanks to Rand Hindi and the Zama team for their feedback and suggestions\nIntroduction\nBlockchain and DeFi systems promote transparency and decentralization but expose user transaction data to the public. This visibility poses significant privacy risks, deterring privacy-conscious users and opening the door to security threats like front-running attacks.\nPrivacy is a fundamental pillar of the open-source ecosystem, yet it is often overlooked in DApps. This article explains one of our approach to seamlessly integrate privacy into existing applications rather than creating isolated, fragmented ecosystems where privacy is treated as an afterthought. With billions of USD locked in ecosystems like Ethereum, Solana, and Bitcoin, privacy solutions are not even a part of their near-term roadmap. This leaves users with no choice but to engage with these systems without privacy, exposing sensitive financial data.\nSeveral privacy solutions, including shielded pools, hardware solutions, and threshold encryption, have been attempted to tackle these issues but face significant limitations. Shielded pools, while effective at concealing transaction details, create barriers to adoption due to complex user interfaces and regulatory challenges. Threshold encryption solutions suffer from complex key management and scalability issues in decentralized environments. Consequently, these solutions often sacrifice usability and compliance, limiting their effectiveness in real-world applications.\nPrivacy-enhancing technologies like FHE, TEE, and MPC offer a novel approach by enabling computations on encrypted data without decryption. Thus, they preserve privacy while addressing the scalability and regulatory challenges that have limited previous solutions. The issue is how to use these PETs with existing EVM stacks or Dapps[2].\nTo make privacy accessible to all users, we need to focus on two key areas:\n\nAdapting existing application: to be compatible with privacy-enhancing technologies\nIf building new application: within existing ecosystems using privacy-preserving technologies\n\nVery few efforts have been made to introduce privacy in existing applications. Our approach tries to tackle the above challenges and provides a generalised way of interacting with current applications.\nIn this post, we will explore how to incorporate privacy into existing Defi applications on EVM-compatible chains using PETs like FHE. The overall architecture combines off-chain co-processors with on-chain smart contracts. For these off-chain co-processors to interact effectively with on-chain states, we need a framework that enables smart contracts to work seamlessly with encrypted data.\nWe also discuss the concept of encrypted ERC20 tokens, which provide a privacy-enhanced alternative to standard ERC20 tokens. As, Recently, Circle and Inco published a report delving deeper into the topic of encrypted ERC20s. To be precise, Our framework is not tied to any specific encrypted ERC20 standard, making it adaptable for use across multiple standards.\nBackground and Related Work\nCurrent Transaction Flow\nThe current transaction process\u2014from a user\u2019s wallet \u2192 mempool \u2192  block is entirely transparent. This transparency aligns with the core purpose of a public blockchain, where data included in the ledger should be visible to all participants. However, this openness deters many people from entering the space, as not everyone wants their data to be visible to the entire world.\nThere are various stages in the execution process where privacy can be introduced, and each stage comes with its own set of complexities. Encrypting the transaction as soon as the wallet signs it makes the most sense, as valuable information can then be hidden on the client side.\nThe challenge lies in modifying the existing infrastructure and achieving community acceptance for these changes. Solutions include encrypted mempools, encrypted solving, private RPC providers, and block building within TEEs, among others. Let\u2019s explore some of the solutions that other teams have worked on in the past.\nSome Previous Privacy Solutions\nEncrypted Mempools\nTeams are already working on encrypted mempool solutions. Threshold-encrypted mempools use threshold encryption to protect transaction details in the mempool until they are ready to be included in a block. This prevents unauthorized parties from viewing transaction details (e.g., sender, receiver, or amount) while the transaction is still pending, addressing issues like front-running in MEV situations. Users can submit transactions with the assurance that their details will remain confidential until the block is confirmed.\nHowever, encrypted mempools has high barrier to entry due to it\u2019s unique cryptographic (Time lock puzzle, Threshold encryption / decryption) or hardware requirements (TEEs).\nMost threshold encryption schemes require an initial setup phase that involves distributed key generation, which can be costly in terms of time and resources. In large, decentralized environments, this setup can be challenging\u2014especially when committee members join or leave, requiring key re-shares or even a complete rerun of the setup.\nShielding Pools\nCurrent solutions that provide protection for on-chain interactions often lack user-friendliness from a UX standpoint.\nUsing shielded addresses and pools introduces significant complexity to achieving on-chain privacy. Shielded pools enable users to store and transact assets without revealing transaction details\u2014such as the sender, receiver, or amount\u2014on the blockchain. Zero-knowledge (ZK) proofs facilitate these shielded transactions by validating their legitimacy without disclosing any actual data. This ensures that network participants can verify the validity of a transaction without knowing who sent or received the funds or the amount transferred.\nWhen a user transfers assets into a shielded pool, those assets are \u201cshielded,\u201d with transaction details (amount, sender, receiver) encrypted and hidden from the public ledger. ZK proofs are then used to confirm that the user holds a valid balance and is authorized to spend it, without revealing any specifics. Users can transfer assets between shielded addresses without exposing details within the shielded pool. All transactions remain hidden, with ZK proofs ensuring compliance with transaction rules, such as maintaining balance integrity and confirming valid ownership. If a user chooses to move assets back to a transparent (non-shielded) address, they can withdraw funds. However, this typically results in a \u201cprivacy break,\u201d as the withdrawal amount becomes visible unless transferred to another shielded pool.\nWithout proper checks, shielded pools also raise compliance and regulatory concerns, leaving users uncertain. These pools obscure transaction details, complicating the tracing of funds and the identification of involved parties. Regulators are concerned that shielded pools could facilitate money laundering by concealing illicit funds. Financial institutions and regulated entities must comply with anti-money laundering (AML) regulations, which require the detection and reporting of suspicious activities. Shielded pools limit transaction visibility, making it challenging to verify the origin of funds and adhere to AML standards.\nSome Preliminaries\nDifferential Privacy\nDifferential privacy is a mathematical framework used to quantify and ensure the privacy of individuals within a dataset [1].\nThe core idea of differential privacy is to ensure that it is difficult to determine whether any specific individual\u2019s data is included in a dataset, even when analyzing the output of an algorithm applied to that dataset. A randomized algorithm is said to satisfy (\u03f5,\u03b4) - differential privacy if the inclusion or exclusion of an individual\u2019s data changes the probability of any specific output only slightly.\nIn the context of differential privacy, \u03f5 controls the privacy loss, quantifying the maximum difference in output probabilities for neighboring datasets (datasets differing by only one individual). \u03b4 represents the probability of a small relaxation in the privacy guarantee, allowing for a slight chance of greater privacy compromise. This framework ensures that the algorithm\u2019s output remains nearly indistinguishable for neighboring datasets, thereby limiting the information leakage about any single data point.\nDifferential privacy has become a widely adopted standard for privacy-preserving data analysis, offering robust privacy guarantees while enabling valuable statistical insights.\nTorus-based Fully Homomorphic Encryption\nTFHE is a FHE scheme optimised explicitly for fast binary gate computations. Unlike traditional FHE methods that rely on more complex lattice structures, TFHE operates over the torus, efficiently performing encrypted computations with lower noise accumulation and faster bootstrapping times.\nAs a result, TFHE has emerged as a promising solution for secure, privacy-preserving computation in real-time applications.\nSolution\nEncrypted ERC20 Tokens\nEncrypted ERC20 standard for privatizing user token balances. Any token balance intended for homomorphic computation on-chain would need to be wrapped within this encrypted ERC20 standard. This approach can serve as a foundation for building various privacy-focused solutions, such as private payments, private auctions, dark pools, and more.\nThis standard implements necessary interfaces which is used to implement necessary compliance checks, which include selective disclosure of specific ciphertext requested and a few other checks.\nTo learn more about Encrypted ERC20 you can read this article by Circle [3]\nDifferential Privacy with Order Aggregation and Batch Settlements\nWe propose a solution leveraging differential privacy to enable order-solving for encrypted orders. This allows users to place encrypted orders (orders with encrypted tokens) and have them processed on-chain without revealing their details. External parties cannot determine the exact order details associated with a specific user.\nBatching is a core component of this solution. The challenge with processing a single encrypted order directly through the protocol is that once decrypted, the amount the user intended to hide becomes visible. To mitigate this, we aggregate multiple orders using the additive homomorphic properties of certain privacy-enhancing technologies (PETs), such as Fully Homomorphic Encryption (FHE). The encrypted amounts are summed and deposited as an aggregated value with a designated manager. The manager\u2019s role is to decrypt this aggregated value via a secure wrapper (obtaining the decrypted tokens amountIn values) so that the resulting assets can interact with the appropriate solver protocol.\nBy batching encrypted orders, we introduce a level of noise into each order, effectively preserving the privacy of individual users\u2019 order details.\nbatching flow1920\u00d71070 77.2 KB\nThe design is inspired by Zswap DEX of Penumbra [5], which uses sealed-bid batch swaps. The price at which these orders are settled is identical, as there is only one transaction per epoch.\nOnce the order is solved, the return token amount belonging to the user is calculated homomorphically using the ratio of the input amount to the output amount (the amount received upon solving the order). This calculation is performed homomorphically in the encrypted space, ensuring that no one can fully determine how many tokens a particular user will receive, thereby preserving privacy.\nsolving flow1920\u00d71077 54.8 KB\nEnd to End flow Order placing \u2192 Order Aggregation \u2192 Order Solving \u2192 Distribution\nEnd to end solving flow1920\u00d7766 67 KB\nMathematical Formulation\nWe are proposing two methods for mitigation for the privacy in applications:\n\nEncrypting Assets: Assets held by the user is encrypted via publicly verifiable encryption scheme.\nBatching Orders: Choosing a size of n of orders to batch prior execution.\n\nIndividually these solutions don\u2019t provide enough privacy guarantees from an adversary POV but together it introduced differential privacy which provides probabilistic indistinguishability for a particular user\u2019s order.\nMost DeFi action on-chain can be defined as a Tokens going in ( T_{in} ) and tokens coming out ( T_{o} ), which means that any solving action \\pi can be written as\n\nT_{o} = \\pi(T_{in}, G, V) \\\\\nG: \\text{Gas consumed} \\\\\nV: \\text{Value transferred}\n\nBy changing the domain of interaction for the user with the protocol with we can introduce a middle smart contract M which does this interaction on the users behalf. Now M has the task of receiving orders from n users and aggregating them i \\in [1,n]\n\n\\pi_{M} = \\sum_{i=1}^{n}{\\pi_{i}} \n\nWe can write the encrypted value of  T_{in}  for a user  i  as  C^{i}  where  C^{i}  can be represented as\n\nC^{i} = (A_0^i,...,A_{k-1}^i,B^i) \n\nThe above representation is how a lattice based homomorphically encrypted plaintext looks like.\nNow since encryption is homomorphic in nature we can simply sum the individual ciphertexts to form the aggregate ciphertext C^{\\pi_M}\n\nC^{\\pi_M} =  (\\sum_{i=0}^n A_0^i,..., \\sum_{i=0}^n A^i_{k-1}, \\sum B^i) \n =  (\\sum_{i=0}^n {C^{i}} ) \n\nIn this process we need to perform programmable bootstrapping multiple times which reduces the noise which is getting accumulated in every addition.\nThe decrypted amount is now used further for interaction with the DeFi protocol.\nConclusion\nPrivacy in blockchain and DeFi ecosystems is becoming increasingly crucial to protect user data and secure transaction processes. While various solutions\u2014such as shielded pools, threshold encryption, differential privacy, and fully homomorphic encryption\u2014offer unique approaches, they also present challenges in terms of usability, compliance, and technical implementation.\nExploring these privacy-preserving techniques highlights the potential for integrating privacy into existing blockchain applications while balancing transparency and regulatory requirements. As privacy solutions continue to evolve, they promise to foster a more inclusive, secure, and user-centric blockchain ecosystem, empowering users to engage confidently in decentralized platforms.\nReferences\n[1] Differential Privacy in Constant Function Market Makers by Tarun Chitra and Guillermo Angeris and Alex Evans\n[2] Zama\u2019s fhEVM co-processor\n[3] Unveiling the Confidential ERC-20 Framework: Compliant Privacy on Public Blockchains using FHE\n[4] TFHE: Fast Fully Homomorphic Encryption over the Torus by Ilaria Chillotti and Nicolas Gama and Mariya Georgieva and Malika Izabach\u00e8ne\n[5] ZSwap Penumbra\n", "\nFaster block/blob propagation in Ethereum\nAcknowledgements to @n1shantd, @ppopth and Ben Berger for discussions and feedback on this writeup and @dankrad for many useful discussions.\nAbstract\nWe propose a change on the way we broadcast and transfer blocks and blobs in the P2P network, by using random linear network coding. We show that we can theoretically distribute the block consuming 5% of the bandwidth and with 57% of the number of network hops (thus half the latency per message) of the time it takes on the current gossipsub implementation. We provide specific benchmarks to the computational overhead.\nIntroduction\nThe current gossipsub mechanism for distribution of blocks roughly works as follows. The proposer picks a random subset (called its Mesh) of D=8 peers among all of its peers and broadcasts its block to them. Each peer receiving a block performs some very fast preliminary validation: mostly signature verification, but most importantly not including state transition nor execution of transactions. After this fast validation, the peer rebroadcasts its block to another D peers. There are two immediate consequences from such a design:\n\nEach hop adds at least the following delay: one full block transfer from one peer to the next one (including both network ping latency, essentially bandwidth independent, plus transfer of the full block, bound by bandwidth).\nPeers broadcast unnecessarily a full block to other peers that have already received the full block.\n\nWe propose to use random linear network coding (RLNC) at the broadcast level. With this coding, the proposer would split the block in N chunks (eg. N=10 for all simulations below) and instead of sending a full block to ~8 peers, it will send a single chunk to ~40 peers (not one of the original chunks, but rather a random linear combination of them, see below for privacy considerations). Peers still need to download a full block, or rather N chunks, but they can get them in parallel from different peers. After they have received these N chunks that each is a random linear combination of the original chunks composing the original block, peers need to solve a linear system of equations to recover the full block.\nA proof of concept implementation highlights the following numbers\n\nProposing a block takes extra 26ms that are CPU bound and can be fully parallelized to less than 2ms on a modern laptop (Apple M4 Pro)\nVerifying each chunk takes 2.6ms.\nDecoding the full block takes 1.4ms.\nWith 10 chunks and D=40, each node sends half the data than with current gossipsub and the network broadcasts a 100KB block in half the time with benefits increasing with block size.\n\nThe protocol\nFor an in-depth introduction to network coding we refer the reader op. cit. and this textbook. We here mention minimal implementation details for the proof of concepts benchmarks cited above. In the case of block propagation (~110KB), latency or number of network hops dominate the propagation time, while in the case of large messages like blobs in full DAS, bandwidth dominates the propagation time.\nWe consider a finite field \\mathbb{F}_p of prime characteristic. In the example above we choose the Ristretto scalar base field as implemented by the curve25519-dalek rust crate. The proposer takes a block, which is an opaque byte slice, and interprets it as a vector of elements in \\mathbb{F}_p. A typical ethereum block is about B = 110KB at the time of writing, given that each Ristretto scalar takes a little less than 32 bytes to encode, a block takes about B/32 = 3520 elements of \\mathbb{F}_p. Dividing into N=10 chunks, each chunk can be viewed as a vector in \\mathbb{F}_p^{M}, where M \\sim 352. The block is thus viewed as N vectors v_i \\in \\mathbb{F}^M_p, i=1,...,N. The proposer chooses a subset of D\\sim 40 peers at random. To each such peer it will send one vector of \\mathbb{F}_p^M together with some extra information to validate the messages and prevent DOS on the network. We explain the proposer\nThe Proposer\nWe will use Pedersen commitments to the Ristretto elliptic curve E as implemented by the above mentioned rust crate. We assume that we have already chosen at random a trusted setup of enough elements G_j \\in E, j = 1, ..., K with K \\gg M. We choose a standard basis \\{e_j\\}_{j=1}^M for \\mathbb{F}_p^M. So each vector v_i can be written uniquely as\nv_i = \\sum_{j=1}^M a_{ij} e_j,\nfor some scalars a_{ij} \\in \\mathbb{F}_p. To each vector v_i we have a Pedersen commitment\n C_i = \\sum_{j=1}^M a_{ij}G_j \\in E. \nFinally for each peer in the subset of size D \\sim 40 the proposer chooses uniformly random a collection of scalars b_i, i=1, ...,N and sends the following information to the peer\n\nThe vector v = \\sum_{i=1}^N b_i v_i \\in \\mathbb{F}_p^M. This is of size 32M bytes and it\u2019s the content of the message.\nThe N commitments C_i, i=1,...,N. This is 32N bytes.\nThe N coefficients b_i, i=1, ...,N. This is 32N bytes.\nA BLS signature to the hash of the N commitments C_1 || C_2 || ... || C_N, this is 96 bytes.\n\nA signed message is the collection of elements 1\u20134 above. We see that there are 64N \\sim 640 extra bytes sent on each message as a sidecar.\nReceiving Peers\nWhen a peer receives a message as in the previous section, the verification goes as follows\n\nIt verifies that the signature is valid for the proposer and the hash of the receiving commitments.\nIt writes the receiving vector v = \\sum_{j=1}^M a_j e_j and then computes the Pedersen commitment C = \\sum_{j=1}^M a_j G_j.\nThe received coefficients b_i are a claim that v = \\sum_{i=1}^N b_i v_i. The peer computes C'= \\sum_{i=1}^N b_i C_i, and then verifies that C = C'.\n\nPeers keep track of the messages that they have received, say they are the vectors w_i, i = 1,...,L for L < N. They generate a subspace W \\subset \\mathbb{F}_p^M. When they receive v, they first check that if this vector is in W. If it is, then they discard it as this vector is already a linear combination of the previous ones. The key of the protocol is that this is very unlikely to happen (for the numbers above the probability of this happening is much less than 2^{-256}). As a corollary of this, when the node has received N messages, then it knows that it can recover the original v_i, and thus the block, from the messages w_i, i=1,...,N.\nNotice also that there is only one signature verification that is needed, all incoming messages have the same commitments C_i and the same signature over the same set of commitments, thus the peer may cache the result of the first valid verification.\nSending Peers\nPeers can send chunks to other peers as soon as they receive one chunk. Suppose a node holds w_i, i=1,...,L with L \\leq N as in the previous section. A node also keeps track of the scalar coefficients they received, thus they know the chunks they hold satisfy\n$$ w_i = \\sum_{j=1}^N b_{ij} v_j \\quad \\forall i,$$\nfor some scalars b_{ij} \\in \\mathbb{F}_p they save in their internal state. Finally, nodes also keep the full commitments C_i and the signature from the proposer that they have validated when they validated the first chunk they received.\nThe procedure by which a node sends a message is as follows.\n\nThey choose randomly L scalars \\alpha_i \\in \\mathbb{F}_p, i=1,...,L.\nThey form the chunk w = \\sum_{i=1}^L \\alpha_i w_i.\nThey form the N scalars a_j, i=1,...,N by\n a_j = \\sum_{i=1}^L \\alpha_i b_{ij}, \\quad \\forall j=1,...,N. \n\nThe message they send consists of the chunk w, the coefficients a_j and the commitments C_i with the signature from the proposer.\nBenchmarks\nThe protocol has some components that are in common with gossipsub, for example the proposer needs to make one BLS signature and the verifier has to check one BLS signature. We record here the benchmarks of the operations that need to be carried in addition to the usual gossipsub operations. These are the CPU overhead that the protocol has on nodes. Benchmarks have been carried on a Macbook M4 Pro laptop and on an Intel i7-8550U CPU @ 1.80GHz.\nParameters for these benchmarks were N=10 for the number of chunks and the total block size was considered to be 118.75KB. All benchmarks are single threaded and all can be parallelized\nProposer\nThe proposer needs to perform N Pedersen commitments. This was benchmarked to be\n\n\n\n\nTiming\nModel\n\n\n\n\n[25.588 ms 25.646 ms 25.715 ms]\nApple\n\n\n[46.7ms 47.640 ms 48.667 ms]\nIntel\n\n\n\nNodes\nA receiving node needs to compute 1 Pedersen commitment per chunk and perform a corresponding linear combination of the commitments supplied by the proposer. The timing for these were as follows\n\n\n\n\nTiming\nModel\n\n\n\n\n[2.6817 ms 2.6983 ms 2.7193 ms]\nApple\n\n\n[4.9479 ms 5.1023 ms 5.2832 ms]\nIntel\n\n\n\nWhen sending a new chunk, the node needs to perform a linear combination of the chunks it has available. Timing for these were as follows\n\n\n\n\nTiming\nModel\n\n\n\n\n[246.67 \u00b5s 247.85 \u00b5s 249.46 \u00b5s]\nApple\n\n\n[616.97 \u00b5s 627.94 \u00b5s 640.59 \u00b5s]\nIntel\n\n\n\nWhen decoding the full block after receiving N chunks, the node needs to solve a linear system of equations. Timings were as follows\n\n\n\n\nTiming\nModel\n\n\n\n\n[2.5280 ms 2.5328 ms 2.5382 ms]\nApple\n\n\n[5.1208 ms 5.1421 ms 5.1705 ms]\nIntel\n\n\n\nOverall CPU overhead.\nThe overall overhead for the proposer on the Apple M4 is 26ms single threaded while for the receiving nodes it is 29.6ms single threaded. Both processes are fully parallelizable. In the case of the proposer, it can compute each commitment in parallel, and in the case of the receiving node these are naturally parallel events since the node is receiving the chunks in parallel from different peers. Running these process in parallel on the Apple M4 leads to 2.6ms in the proposer side and 2.7ms in the receiving peer. For real life applications it is reasonable to consider these overheads as zero compared to the network latencies involved.\nOptimizations\nSome premature optimizations that were not implemented consist on inverting the linear system as the chunks come, although the proof of concept cited above does keep the incoming coefficient matrix in Echelon form. Most importantly, the random coefficients for messages do not need to be in such a large field as the Ristretto field. A small prime field like \\mathbb{F}_{257} suffices. However, since the Pedersen commitments take place in the Ristretto curve, we are forced to perform the scalar operations in the larger field. The implementation of these benchmarks chooses small coefficients for the linear combinations, and these coefficients grow on each hop. By controlling and choosing the random coefficients correctly, we may be able to bound the coefficients of the linear system (and thus the bandwidth overhead in sending the blocks) to be encoded with say 4 bytes instead of 32.\nThe simplest way to perform such optimization would be to work over an elliptic curve defined over \\mathbb{F}_q with q = p^r for some small prime p. This way the coefficients can be chosen over the subfield \\mathbb{F}_p \\subset \\mathbb{F}_q.\nPrivacy considerations the implementation in the PoC linked above considers that each node, including the proposer, picks small coefficients to compound its linear transformation. This allows a peer receiving a chunk with small coefficients to recognize the proposer of the block. Either the optimization above is employed to keep all coefficients small by performing an algorithm like Bareiss\u2019 expansions or we should allow the proposer to choose random coefficients from the field \\mathbb{F}_p.\nSimulations\nWe performed simulations of block propagation under some simplifying assumptions as follows.\n\nWe choose a random network modeled as a directed graph with 10000 nodes and each node having D peers to send messages to. D is called the Mesh size in this note and was chosen varying on a large range from 3 to 80.\nPeers where chosen randomly and uniformly on the full node set.\nEach connection was chosen with the same bandwidth of X MBps (this is typically assumed to be X=20 in Ethereum but we can leave this number as a parameter)\nEach network hop, incurs in an extra constant latency of L milliseconds (this is typically measured as L=70 but we can leave this number as a parameter)\nThe message size is assumed to be B KB in total size.\nFor the simulation with RLNC, we used N=10 chunks to divide the block.\nEach time a node would send a message to a peer that would drop it because of being redundant (for example the peer already had the full block), we record the size of the message as wasted bandwidth.\n\nGossipsub\nWe used the number of peers to send messages D=6. We obtain that the network takes 7 hops in average to propagate the full block to 99% of the network, leading to a total propagation time of\n$$ T_{\\mathrm{gossipsub, D=6}} = 7 \\cdot (L + B/X), $$\nin milliseconds.\ngossipsub-total-theorical1712\u00d7982 70.6 KB\nWith D=8 the result is similar\nT_{\\mathrm{gossipsub, D=8}} = 6 \\cdot (L + B/X), \nThe wasted bandwidth is 94,060 \\cdot B for D=6 and 100,297 \\cdot B for D=8.\nFor low values of B, like the current Ethereum blocks, latency dominates the propagation, while for larger values, for example propagating blobs after peer-DAS, bandwidth becomes the main factor.\nRLNC\nSingle chunk per peer\nWith random linear network coding we can use different strategies. We simulated a system in which each node will only send a single chunk to all of the peers in their mesh of size D, this way we guarantee that the latency incurred is the same as in gossipsub: a single latency cost of L milliseconds per hop. This requires the mesh size to be considerably larger than N, the number of chunks. Notice that for a gossipsub mesh size of D_{gossipsub} (for example 8 in current Ethereum), we would need to set D_{RLNC} = D_{gossipsub} \\cdot N to consume the same bandwidth per node, this would be 80 with the current values.\nWith a much more conservative value of half this bandwidth, that is D=40 we obtain\nT_{RLNC, D=40} = 4 \\cdot \\left(L + \\frac{B}{10 X} \\right), \nwith a wasted bandwidth of 29,917\\cdot B. Assuming the same bandwidth as today we obtain with D=80 we get the impressive\nT_{RLNC, D=80} = 3 \\cdot \\left(L + \\frac{B}{10 X} \\right), \nwith a wasted bandwidth of 28,124\\cdot B, which is 28% of the corresponding wasted bandwidth in gossipsub.\nDifferences\nFor the same bandwidth sent per node, we see that the propagation time differs both by dividing the latency in two (there are 3 hops vs 6) and by propagating the block faster consuming a tenth of the bandwidth per unit of time. In addition the wasted bandwidth by superfluous messages gets slashed to 28% of the gossipsub wasted messages. Similar results are obtained for propagation time and wasted bandwidth but reducing the bandwidth sent per node by a half.\nIn the lower block size end, latency is dominant and the 3 hops vs 6 on gossipsub make most of the difference, in the higher block size end, bandwidth performance is dominant. For much larger blocksizes CPU overhead in RLNC gets worse, but given the order of magnitude of the transmission times, these are negligible.\nrlnc-gossipsub1712\u00d7982 87.7 KB\nMultiple chunks per peer\nIn the single chunk per peer approach in production, nodes with higher bandwidth could choose to broadcast to more peers. At the node level this can be implemented by simply broadcasting to all the current peers and node operators would simply chose the number of peers via a configuration flag. Another approach is to allow nodes to send multiple chunks to a single peer, sequentially. The results of these simulations are exactly the same as the above, but with much lower D as expected. For example with D=6, which would never broadcast a full block in the case of a single chunk sent per peer. The simulation takes 10 hops to broadcast the full block. With D=10 the number of hops is reduced to 9.\nConclusions, omissions and further work\nOur results show that one expects considerable improvement in both block propagation time and bandwidth usage per node if we were to use RLNC over the current routing protocol. These benefits become more apparent the larger the block/blob size or the shorter the latency cost per hop. Implementation of this protocol requires substantial changes to the current architecture and it may entail a new pubsub mechanism altogether. In order to justify this we may want to implement the full networking stack to simulate under Shadow. An alternative would be to implement Reed-Solomon erasure coding and routing, similar to what we do with Peer-DAS. It should be simple to extend the above simulations to this situation, but op. cit already includes many such comparisons.\n", "\nMany thanks to @linoscope, @donnoh, @Brecht, @sui414, @pascalst,and @cshg for their valuable feedback and comments.\nHow to Lose $200K every Two Weeks\ntl;dr In this post, we analyze the economics of based rollups using total anarchy as a method of sequencing blocks. Focusing on the only live based rollup, Taiko, we highlight the inefficiencies of total anarchy. Specifically, we identify a critical inefficiency in L2 block building that resembles a priority gas auction (PGA), where competing proposers rush to include transactions before Taiko Labs\u2019 proposer. This results in L2 blocks with redundant transactions being posted on-chain on L1, reducing the value of Taiko\u2019s blocks and increasing its economic costs. As a result, Taiko Labs often incurs expenses to prove blocks with few or no profitable transactions.\nThrough a two-week analysis of block data, we observe that the market is dominated by four major proposers (including Taiko Labs). Our findings indicate that Taiko Labs faces significant losses due to consistently losing the PGA. Over this period, Taiko Labs lost approximately 83.9 ETH, which, at an average Ethereum price of $3,112, translates to a total loss of roughly $261,096 in just two weeks. This underscores the urgent need for better proposer incentives and mechanisms to mitigate these inefficiencies.\nIntroduction\nBased rollups aim to enhance Ethereum scalability by integrating L2 operations with L1 for improved data availability and security. They leverage L1 for sequencing and settlement, avoiding the need for centralized sequencers, which promotes decentralization.\nHowever, the total anarchy model used for sequencing in Taiko introduces significant inefficiencies. In this model, where block posting lacks hierarchy or coordination, any user can act as an L2 proposer and post blocks without restriction, promoting maximum permissionless participation. While this approach aligns with decentralization principles, it also introduces systemic challenges.\nVitalik described total anarchy as:\n\n\u201cTotal anarchy: anyone can submit a batch at any time. This is the simplest approach, but it has some important drawbacks. Particularly, there is a risk that multiple participants will generate and attempt to submit batches in parallel, and only one of those batches can be successfully included. This leads to a large amount of wasted effort in generating proofs and/or wasted gas in publishing batches to chain.\u201d\n\nThese drawbacks materialize in Taiko, where multiple L2 blocks are submitted for the same L1 slot, resulting in redundant transactions. Redundant blocks consume valuable L1 space, inflate fees, and diminish economic efficiency.\nInefficiency Caused by Redundant Transactions\nIn rollups using total anarchy, redundant transactions occur when multiple L2 blocks containing the same transactions are published to L1. These blocks may be submitted within the same L1 slot or across different slots. In such cases, both blocks are submitted to L1, consuming valuable blob space and incurring L1 fees for the L2 proposer. The first block processed on L1 is executed to determine the updated L2 state. Any redundant transactions in the second block, already included in the first, are invalidated, as their state transitions have already been applied. Unique transactions in the second block remain valid and still affect the L2 state.\nThe proposer of the second block faces significant economic inefficiencies. They incur the full cost of posting and proving the block but only earn rewards for valid, non-redundant transactions. This dynamic discourages proposers from submitting redundant blocks. Additionally, posting two blocks to the same L1 slot reduces the effective throughput of the network by occupying valuable block space with redundant data blobs, increasing congestion and costs.\nTaiko\u2019s Architecture and the Economics of redundant Blocks\nTaiko exemplifies a based rollup using total anarchy as its sequencing design, prioritizing simplicity and decentralization. In this model, anyone can collect transactions from the L2 mempool, build a bundle (which becomes the L2 block when proposed by the L1 proposer), and submit it to L1 alongside data blobs containing transaction payloads. These blocks may include transactions or remain empty (containing only a single anchor transaction) to ensure chain continuity during low-demand periods. After block submission, proposers must generate and post a validity proof to confirm the block\u2019s correctness, which incurs additional L1 transaction costs.\nSimplified Overview of Taiko Architecture799\u00d7669 38.2 KB\nEven empty blocks must be proven to maintain the chain\u2019s liveness and avoid slashing penalties. This requirement places a significant economic burden on fallback proposers like Taiko Labs during periods of low activity. When Taiko Labs includes profitable transactions, higher-bidding competitors often outpace it in the PGA environment, resulting in diminished rewards and economic challenges.\nPriority Gas Auction Dynamics in Taiko\nPGAs presents a recurring challenge in Taiko Labs\u2019 operation. Competing searchers exploit Taiko Labs\u2019 open block submission process by outbidding its proposer, using higher fees to ensure their block is executed first. Driven by economic incentives, these proposers monitor pending blocks and submit their own for the same L1 slot, offering higher transaction fees to secure inclusion.\nWhen multiple blocks overlap in content, the first valid block determines the network\u2019s state. Redundant transactions between Taiko Labs\u2019 block and an earlier block are excluded, forcing Taiko Labs to bear the cost of proposing and proving blocks without proportional rewards. This creates a situation where Taiko Labs incurs the full cost of sequencing blocks but receives minimal or no profit, further straining the network\u2019s economic sustainability.\nThese inefficiencies are especially pronounced during high-demand periods, when the PGA environment is most competitive. However, during low-demand periods, the Taiko Labs proposer is forced to maintain liveness by posting and proving blocks that may contain some transactions but are not full. While these blocks may offer some rewards, in most cases they cannot cover the L1 costs, making them unprofitable. As a result, PGAs not only redirect rewards to more sophisticated proposers but also undermine the incentives necessary to maintain the network\u2019s liveness, placing a disproportionate economic burden on fallback proposers like Taiko Labs.\n\nAnalysis\nMethodology\nFor this analysis, we evaluate proposer profitability by comparing their earnings with the costs incurred. The block rewards from L2 blocks represent the earnings, while the L1 publication costs and proving costs are considered the losses. For Taiko Labs proposer, the base fee associated with each block is included in its earnings.\n\nTaiko Proposer Net Profit:\n\\text{(L2 Priority Fees + Base Fee)\u2212(L1 Publication Costs + Proving Costs)}\nOther Proposers Net Profit:\n\\text{L2 Priority Fees \u2212 (L1 Publication Costs + Proving Costs)}\n\nThe analysis is based on blocks created between November 7, 2024, and November 22, 2024, covering Block IDs 538304 to 593793. This represents 9.34% of all blocks on the Taiko chain since genesis at the time of writing. This dataset provides insights into the economic performance of proposers who processed more than 500 blocks during this period.\nAnalysis of Proposer Rewards, Costs, and Profitability\nThe graph below presents an overview of rewards, costs, and profits for major proposers, highlighting the economic dynamics within the system. Taiko Labs, as the primary fallback proposer, is used as the baseline for evaluating profitability.\nProposer Profit Breakdown5952\u00d72927 321 KB\nKey Observations\n\nTaiko Labs\u2019 Proposer (0x000000633b68f5D8D3a86593ebB815b4663BCBe0)\n\nNet Profit: -83.9 ETH (approximately $261,096 at $3,112/ETH).\nTaiko Labs frequently incurs economic losses due to its fallback role in maintaining liveness and being outbid in the PGA environment by competitors, most notably Proposer A (0x41F2F55571f9e8e3Ba511Adc48879Bd67626A2b6) and Proposer B (0x66CC9a0EB519E9E1dE68F6cF0aa1AA1EFE3723d5). By offering higher fees and securing earlier blocks, these proposers reduce Taiko Labs\u2019 ability to capture profitable transactions. The result is often blocks filled with only an anchor transaction, leaving Taiko Labs to incur the proving costs without receiving commensurate rewards.\n\n\nMost Profitable Proposer A (0x41F2F55571f9e8e3Ba511Adc48879Bd67626A2b6)\n\nNet Profit: 26.0 ETH (approximately $80,912 at $3,112/ETH).\nThis proposer routinely outbids the Taiko Labs proposer, capturing the majority of profitable transactions by winning the PGA race and extracting more value.\n\n\nSecond most profitable Proposer B (0x66CC9a0EB519E9E1dE68F6cF0aa1AA1EFE3723d5)\n\nNet Profit: 17.5 ETH (approximately $54,460 at $3,112/ETH).\nThis proposer also strategically outbids Taiko Labs, securing profitable transactions and achieving substantial net gains. While slightly less profitable than Proposer A, Proposer B still efficiently balances rewards and costs.\n\n\nThird most profitable proposer C (0x9a5Cc6E3A3325CDc19fC76926CC9666c80139C09)\n\nNet Profit: 6.6 ETH (approximately $20.540 at $3,112/ETH).\nAlthough Proposer C posts a similar number of blocks as Proposer B, it earns only about half the profit. This discrepancy likely arises from less sophisticated bidding strategies, reducing overall profitability.\n\n\nSmall-Scale Proposers\n\nSmaller-scale proposers exhibit lower overall activity. However, they often manage to remain profitable or near break even due to proportionally lower costs, benefiting from a more cautious approach within the PGA landscape.\n\n\n\nTaiko Labs Proposer Outbid by Other Proposers\nIn this section, we analyzed instances where the two top-earning proposers outbid Taiko Labs. This happens when a proposer submits a block faster than Taiko Labs and secures its execution on L1 first.\nTaiko Labs' outbid by Proposer A1920\u00d71055 138 KB\nThis graph illustrates each instance where Proposer A outpaced the Taiko proposer in posting a block.\n\nY-Axis: Represents the reward associated with each block (sum of L2 transaction fees).\nX-Axis: Represents the size of the posted block.\nTimeframe: Over the two-week period analyzed, this occurred 4,621 times.\n\nIn our analysis, we examined instances where blocks proposed by Proposer A were immediately followed by blocks proposed by Taiko Labs\u2019 proposer.\nProfitability Comparison:\n\nProposer A:\n\nIn blue on the graph, we observe all 4,285 profitable blocks proposed (92.7% profitability), while the not profitable blocks are shown in ligthblue.\n\n\nTaiko Proposer:\n\nIn red, the graph shows that the Taiko Labs proposer achieved only 103 profitable blocks (2.2% profitability with 4,518 blocks resulting in 97.8% of blocks being unprofitable), with the not profitable blocks shown in pink.\n\n\nEconomic Impact on Taiko:\n\nThe total loss incurred by the Taiko Labs proposer, as a result of consistently being outbid by Proposer A, amounted to 18.37 ETH.\n\n\n\nTaiko Labs' outbid by Proposer B1920\u00d71064 139 KB\nSimilarly, we analyzed cases where blocks proposed by Proposer B preceded those proposed by the Taiko Labs proposer. This occurred 4,870 times during the observation period.\nProfitability Comparison:\n\nProposer B:\n\nProposed 4,333 profitable blocks (89.0% profitability) in blue, with not profitable blocks shown in lightblue.\n\n\nTaiko Proposer:\n\nAchieved 132 profitable blocks (2.7% profitability with 4,738 blocks resulting in 97.3% of blocks being unprofitable) in red, with not profitable blocks in pink.\n\n\nEconomic Impact on Taiko:\n\nThe total loss incurred by the Taiko Labs proposer in these cases was 18.25 ETH.\n\n\n\nTransaction Distribution Analysis\nTo further investigate proposer behavior, we analyzed the distribution of transactions per block using a Kernel Density Estimation (KDE) graph. This visualizes how proposers allocate transactions across blocks, highlighting differences in their strategies.\nTransactions Count KDE1920\u00d71020 103 KB\nKey Observations\n\nTaiko Labs\u2019 Behavior (0x000000633b68f5D8D3a86593ebB815b4663BCBe0):\n\nLow-Transaction Blocks (<100 Transactions): Approximately 24% (6,652 blocks) of Taiko Labs\u2019 blocks fall into this category, with 99% resulting from being outbid in the PGA environment. When Taiko Labs\u2019 blocks are outbid by competing proposers, overlapping transactions in subsequent blocks are invalidated, leaving only the anchor transaction in the final block.\nHigh-Transaction Blocks (>1500 Transactions): Around 24.1% of Taiko Labs\u2019 blocks exceed this threshold, showing occasional success in capturing profitable opportunities.\nTaiko Labs\u2019 fallback role and the penalties from redundant blocks lead to significant economic inefficiencies.\n\n\nProfit-Focused Proposers:\n\nProposers A and B focus almost exclusively on high-transaction blocks, with over 46\u201358% of their blocks exceeding 1500 transactions.\nBoth proposers frequently outbid Taiko Labs by submitting blocks with higher transaction fees, diminishing Taiko Labs\u2019 reward opportunities.\n\n\n\nAnalysis of Block Profitability by Major Proposers\nWe continue our analysis by evaluating the number of profitable blocks proposed by each proposer and examining the distribution of these results in Taiko.\nProposers profitability (1)4453\u00d72317 266 KB\nThis graph illustrates the profitability of blocks published by major proposers (processing more than 500 blocks) during the analyzed period. It categorizes blocks into two groups: profitable blocks (green) and unprofitable blocks (red), highlighting the proportion of each for individual proposers.\nKey Observations\n\nTaiko Proposer (0x000000633b68f5D8D3a86593ebB815b4663BCBe0)\n\nProfitable Blocks: 19.1%\nUnprofitable Blocks: 80.9%\nThe majority of Taiko Labs\u2019 blocks are unprofitable, reflecting its role in maintaining liveness, even if that requires posting unprofitable blocks. This outcome supports the hypothesis that Taiko Labs is consistently outbid in the PGA environment on profitable blocks, leaving it to act as a fallback proposer during periods of lower profitability.\n\n\nProposer A (0x41F2F55571f9e8e3Ba511Adc48879Bd67626A2b6)\n\nProfitable Blocks: 93.4%\nUnprofitable Blocks: 6.6%\nThis proposer maintains a highly efficient operation, focusing almost exclusively on profitable blocks, which suggests selective block proposal during high-demand periods.\n\n\nProposer B (0x66CC9a0EB519E9E1dE68F6cF0aa1AA1EFE3723d5)\n\nProfitable Blocks: 89.8%\nUnprofitable Blocks: 10.2%\nAnother highly efficient proposer, demonstrating strong profitability, likely by focusing on full blocks.\n\n\nMixed Strategy Proposer (0x9a5Cc6E3A3325CDc19fC76926CC9666c80139C09)\n\nProfitable Blocks: 69.8%\nUnprofitable Blocks: 30.2%\nThis proposer might not be as sophisticated as the other proposers, occasionally posting unprofitable blocks.\n\n\nSmaller Proposers (e.g., 0x2802E30d61d5ac0879c4F0c2825201a3D9C250Ef)\n\nProfitable Blocks: 96.8%\nUnprofitable Blocks: 3.2%\nThis proposer began operations later in the analyzed period, starting at block 580181. Despite its smaller scale, it demonstrates a highly sophisticated strategy, focusing exclusively on profitable opportunities and avoiding unprofitable blocks entirely. This could indicate that it is one of the most advanced actors, strategically entering only when conditions are favorable.\n\n\n\nInsights\nThis analysis reveals how competing proposers, driven by their own economic interests, create challenges for Taiko Labs. A critical issue arises when Taiko Labs posts blocks on L1 with low-priority fees, enabling more sophisticated actors to outbid them in the PGA environment. Our findings indicate that over 80% of Taiko Labs\u2019 posted blocks were unprofitable, and being outbid occurred in more than half of the blocks proposed by Taiko. This highlights the economic inefficiencies Taiko Labs faces as it strives to maintain network liveness in an environment where competing proposers exploit its fallback role.\n\nPossible Solutions\nUsing total anarchy for sequencing requires guarantees of execution to prevent redundant transactions. This approach can be challenging because, from the L1 perspective, transactions are executing correctly.\nOne potential solution is to add the L2 block ID field in the L2 block proposal function, causing the block proposal to revert if the target is missed due to competition from other proposers. While this still incurs a transaction cost for proposing, it avoids the expense of proving the block. Taiko Labs could potentially use revert protection to prevent conflicting blocks from getting on-chain. By doing this, they could avoid wasting transaction fees. However, it\u2019s worth noting that revert protection introduces a trust assumption on the builder. Another problem might be when you have blocks with the same ID that don\u2019t have redundant transactions.\nAnother possible solution is execution preconfirmations. However, ensuring execution guarantees on the L2 side adds complexity to the preconfirmation process. Having a single preconfer can provide guarantees that they will not publish conflicting blocks for the same slot, as doing so could result in slashing penalties. This mechanism can significantly reduce redundant submissions and lower L1 fee wastage. However, it also introduces execution complexity, posing challenges that must be addressed to ensure efficient implementation.\nThe solution that might be the easiest to implement involves the use of execution tickets. Execution Tickets, or others leader election mechanisms like based preconfirmation, provide a deterministic system to elect a single block proposer per slot. This approach minimizes conflicts and redundancy by ensuring that only one proposer is responsible for block submission at any given time.\nExecution tickets have several advantages. By eliminating redundant block submissions, they reduce wasted resources and align proposer incentives with the system\u2019s overall efficiency. However, implementing such a system introduces challenges to ensure fair and reliable leader election.\n\nDiscussion and Conclusion\nWhile total anarchy encourages permissionless participation, it struggles to meet the efficiency demands of based rollups due to redundant blocks and the competitive PGA environment. Taiko serves as a compelling case study, illustrating the economic costs associated with inefficient block space utilization on L1.\nPotential solutions such as execution preconfirmations could address these inefficiencies but add system complexity. Alternatively, introducing a leader election mechanism could reduce redundant blocks by adding structure, though it might also introduce centralization risks. A balanced approach could retain permissionless participation while penalizing harmful behavior, aligning decentralization with practical efficiency.\n\nFuture Work\n\nProfitability Analysis: Investigate whether Taiko Labs has ever been profitable or if competing proposers were consistently capturing the profits instead.\nProof Costs: Evaluate the impact of off-chain proof generation costs on net profitability.\nProposer Behavior: Study proposer strategies in detail. Initial decoding of a few L1 blobs revealed no instances of proposers directly copying Taiko Labs\u2019 proposer blobs, but further analysis is needed to confirm patterns.\n\nAcknowledgments\nI would like to express my sincere gratitude to Flashbots for awarding the grant that made this work possible and for supporting my ongoing research on this topic. I also extend my thanks to the PBS Foundation for their initial support of this research.\nFAQ:\nHow does Taiko Labs post blocks on L1?\nCurrently, the Taiko proposer operates openly by observing the public L2 mempool and publishing their blocks on the L1 mempool. Since everything is done publicly, outpacing the Taiko Labs sequencer by submitting blocks faster is relatively straightforward, provided you can generate the proof for the blocks you publish or find a prover willing to generate the proof on your behalf.\nData Collection?\nTo collect data, we listened to events from the contract responsible for proving and proposing: 0x06a9Ab27c7e2255df1815E6CC0168d7755Feb19a. From these events, we extracted the Taiko block ID in which the L1 block was recorded and the L1 transaction hash.\nUsing the transaction hash, it was straightforward to check the transaction fees associated with each transaction via RPCs. For L2 transaction fees and the L2 base fee, we used the L2 block ID and calculated the results based on the block reward. While this method might not be the fastest, acquiring data for Taiko has proven to be challenging and relatively slow.\nIn future posts, we aim to find a faster way to collect data for all chains.\nEncrypted Mempool as a Solution?\nAn encrypted mempool wouldn\u2019t solve the problem, as blocks with redundant transactions would still occur. Over time, this could lead to a monopoly where the most competitive and sophisticated searcher consistently posts blocks faster than others.\nAre Proposer A and Proposer B Outbidding Each Other?\nWe found only 57 occurrences where these two proposers published blocks in immediate succession, indicating that direct PGA-style competition between them is relatively rare. Proposer A published first in 31 instances, making all of those blocks 100% profitable for Proposer A but only 54.8% profitable for Proposer B. Conversely, Proposer B published before Proposer A in 26 instances, and in those cases, both proposers\u2019 blocks were profitable 80.8% of the time. Further analysis will be conducted for other proposers in a subsequent post.\nHow Can I Identify Blocks Impacted by Outbidding?\nYou can view it by simply checking TaikoScan. Often, when blocks are empty or contain fewer than 100 transactions, it suggests the proposer was outbid in the PGA environment. Even blocks with a higher number of transactions might have been affected; in such cases, comparing the block\u2019s costs to its rewards is the only way to confirm. For a more in-depth analysis, decoding the blob is the most reliable approach.\nWas Taiko Labs ever profitable?\nTo answer this question definitively, further analysis is needed. However, the intuition suggests that Taiko Labs becomes profitable under specific conditions. For other proposers, profitability occurs when  \\text{L2  Priority Fees \u2212 (L1 Publication Costs + Proving Costs) > 0}. If this condition is not met, they avoid publishing blocks that would result in a loss.\nIn contrast, the Taiko Labs proposer earns an additional Base Fee, making its profitability condition:\n\\text{(L2  Priority Fees + Base Fee) \u2212 (L1 Publication Costs + Proving Costs) > 0}\nWhen this condition holds, Taiko Labs is profitable, as the Base Fee offsets the publication and proving costs that would otherwise make the block unprofitable for other proposers.\n", "\nAhead-of-Time Block Auctions To Enable Execution Preconfirmations\nThanks to Burak Oz, Bo Du, Ladislaus, Domothy, Alejandro Ranchal-Pedrosa, Justin Drake for discussion or review. Final work is the author\u2019s and reviews are not an endorsement! Also thank you to the teams who participated in Sequencing Week.\nThis post assumes familiarity with preconfirmations. This post is opening a potential design direction for discussion and not an endorsement or necessarily a reflection of any team\u2019s roadmap.\nSummary\nExecution preconfirmations have two problems in order to ensure they are more profitable for the proposer than today\u2019s existing PBS pipeline:\n\nAdversarial flow: how to prevent searchers from submitting transactions via execution preconfirmations, which extract money from the builder and proposer\nPricing: how to price blockspace and mitigate the futures risk of the block becoming more valuable later\n\nThere are two approaches, one where the gateway is an unsophisticated entity, and the other where the gateway is a sophisticated builder.\nThe unsophisticated gateway addresses this issue by having pricers compete to take on the futures risk on behalf of the gateway. This avoids further gateway centralization.\nThe sophisticated builder gateway addresses this issue by designating block builders as the preconfirmation gateway. Builders can detect and block adversarial top of block ToB activity, preventing searchers from exploiting MEV opportunities. Moreover, builders are uniquely positioned to accurately price slots within the block.\nPricer System\nIn this system, the gateway acts as an unsophisticated auction house. It holds a bidding process where pricers can bid to underwrite the futures risk of a preconfirmation request, in exchange for receiving the preconfirmation bid. Under this design, for every incoming transaction, a bidding process will occur in which a pricer agrees to take on its futures risk in exchange for a fee.\nphoto_2024-12-31 20.01.44993\u00d7503 30.1 KB\nEthereum Sequencing Call #13\nThe pricer will be exposed to the futures risk of the contract, in exchange for accepting the tip, they will be exposed to the future risk of block space value. The key issue with the pricer system is: how to compare the value of a block with an execution constraint versus a block without the execution constraint. It could be possible to check bundles that affect a certain state in a BuilderNet-style TEE, and check the value of the block with and without the constraint to figure out the opportunity cost of transaction inclusion. This enables appropriately moving the futures risk onto the pricer.\nCurrent Design\nThis is a sketch of the components of the preconfirmation pipeline.\nCreated During Sequencing Week1280\u00d7759 50.2 KB\nDesign suggested at Sequencing Week\nIn this design, the proposer delegates the right to issue a preconfirmation transaction to an external party, the gateway, a block builder, which is issued the right to add transactions to the top of block ToB, along with an inclusion list constraint on Rest of Block RoB. The gateway submits an ordered list that must be added to ToB and an unordered list that must be included in RoB.\nThis protects the proposer from DDOS attacks and allows the gateway to have more complicated and computationally expensive logic. Research by Burak Oz and others [1]  have shown that roughly 60-70% of block builder profits is earned by off-chain agreements including searcher flow and exclusive order flow. Therefore, pricing the preconfirmation is a challenging task, as only 30\u201340% of the data required to price the transaction is on-chain. Certain proxies like cex-dex volatility can be used to attempt to predict the off-chain MEV; however, it requires the gateway to perform complex decision-making.\nThe gateway must:\n\nsuccessfully predict the on-chain MEV extracted in the block\nsuccessfully predict the off-chain MEV extracted via off-chain agreements\ntake on the futures risk of selling blockspace now that may increase in value later on\n\nexecution preconfirmations are particularly difficult because the gateway must predict which contentious state is MEV-valuable, and therefore deny transactions that affect certain pieces of contentious state\n\n\navoid being gamed by searchers and other entities that may seek to buy blockspace for less than its value, thereby extracting value from the proposer\n\nThe entity with the best data on the value of the upcoming block is the builder themselves. We propose merging the role of the builder and the preconfer into the gateway, we ask block builders who reap high MEV rewards to also take on the role of predicting the value of the upcoming block, and therefore pricing the preconfirmation.\nExecution preconfirmations are especially valuable because they allow for synchronous composability between L2s and the L1 in advance of a block, they also allow the gateway to act as a shared block builder between the L2s and L1. They allow following transactions to act on the output state of the first transaction, creating continuous block building, massively improving UX.\nAhead of Time Slot Auction\nThe gateway can buy the rights to build the top of the block either Just In Time JiT just ahead of the slot start time, or up to 32 slots in advance. We believe it\u2019s better to hold an auction closer to the slot time, as the gateway has more data, and can bid higher.  Meanwhile the existing PBS pipeline is preserved, the gateway can auction off space in the RoB that has not yet been auctioned.\nThe block can be sold in the following positions:\n\nbid for gateway rights (ToB and RoB inclusion constraint) 32 slots in advance (potentially even traded every slot)\n\nthe issue with trading the block far in advance is there will likely be a discount to the block\u2019s value vs a JIT auction. Therefore, we suspect a JIT auction slightly ahead of the slot start time is preferable.\nJIT auctions in advance of every slot prevent multi-slot MEV\n\n\nbid for gateway rights just before the upcoming block\nsell the rest of the slot via existing pbs pipeline.\n\nInstead of selling the block at the end of the block, as is done currently, we propose auctioning the block JIT before the current slot\nIn exchange for taking on the responsibility of issuing preconfirmations, the builder earns additional fees for preconfirmation risk.\nConcerns\n\nCentralization of the Gateway\n\nBy moving the auction process just ahead of the block, we simply move the existing process that occurred at the end of the block to in advance of the block, this adds minimal changes beyond what exists today. We expect the builder market to further decentralize, we can reuse these innovations to further decentralize the gateway.\n\n\nBidding in an ahead of time auction vs JIT, where as a builder you would need to discount the expected MEV given there\u2019s uncertainty. So depending on how long in advance you have to bid, it may not be optimal for the proposer.\n\nWe believe the additional profits earned from preconfirmation tips may be greater than the blockspace futures risk, so the builder and proposer will be net more profitable.\n\n\n\nReferences\n[1] Burak \u00d6z, Danning Sui, Thomas Thiery, Florian Matthes, Who Wins Ethereum Block Building Auctions and Why?\n[2] Preconfirmations Call #13\n", "\nExisting ETH staking-as-a-service providers typically require users to deposit a full balance of 32 ETH or deposit funds into inherently centralized liquid staking derivative (LSD) protocols. However, relying on a third party in LSD protocols introduces certain counterparty risks, including the potential centralization of node operators, the vulnerability of validator signing keys to ransom attacks, and the management of stake balances leading to potential ETH being stuck or lost. Furthermore, the concentration of ETH within these centralized entities amplifies the potential impact of exploited risks.\nMore significantly, as additional opportunities in the Ethereum ecosystem grow, such as Eigenlayer. Single purpose LSDs are too inflexible to provide the optionality that ETH stakers have and will continue to have. It will be important to have a UI that allows users to natively choose where they want their staked ETH to go. By removing the middleman, this direct staking process opens up possibilities for stakers to earn additional rewards and maximize their returns.\nFor these reasons we\u2019re building Casimir SelfStake to empower stakers with greater control over their assets and the ability to tap into emerging opportunities within the Ethereum ecosystem.\nCasimir SelfStake offers a different approach where stakers can directly deposit any amount of ETH to highly capable Ethereum operators through a factory smart contract model. This approach minimizes counterparty risk for users and enhances the decentralization of Ethereum staking. Validators\u2019 duties are carried out by openly registered and collateralized operators using distributed validator technology (DVT). Trustless key management is achieved through zero-coordination distributed key generation (DKG). Automated actions, such as compounding stake or handling a slash, are executed by a decentralized oracle network (DON). Furthermore, the user experience is improved through the use of account abstraction to wrap staking contract actions.\nWe\u2019re looking for feedback, discussion, and mentorship from the broader community on this project. We believe this methodology can help enable a more decentralized and trustless approach to Ethereum staking without sacrificing scalability, usability, or staking yield.\nYou can follow our work here: GitHub - consensusnetworks/casimir: \ud83c\udf0a Decentralized staking and asset management\n", "\nContent note: preliminary research. Would love to see independent replication attempts.\nCode: https://github.com/ethereum/research/tree/master/correlation_analysis\nOne tactic for incentivizing better decentralization in a protocol is to penalize correlations. That is, if one actor misbehaves (including accidentally), the penalty that they receive would be greater the more other actors (as measured by total ETH) misbehave at the same time as them. The theory is that if you are a single large actor, any mistakes that you make would be more likely to be replicated across all \u201cidentities\u201d that you control, even if you split your coins up among many nominally-separate accounts.\nThis technique is already employed in Ethereum slashing (and arguably inactivity leak) mechanics. However, edge-case incentives that only arise in a highly exceptional attack situation that may never arise in practice are perhaps not sufficient for incentivizing decentralization.\nThis post proposes to extend a similar sort of anti-correlation incentive to more \u201cmundane\u201d failures, such as missing an attestation, that nearly all validators make at least occasionally. The theory is that larger stakers, including both wealthy individuals and staking pools, are going to run many validators on the same internet connection or even on the same physical computer, and this will cause disproportionate correlated failures. Such stakers could always make an independent physical setup for each node, but if they end up doing so, it would mean that we have completely eliminated economies of scale in staking.\nSanity check: are errors by different validators in the same \u201ccluster\u201d actually more likely to correlate with each other?\nWe can check this by combining two datasets: (i) attestation data from some recent epochs showing which validators were supposed to have attested, and which validators actually did attest, during each slot, and (ii) data mapping validator IDs to publicly-known clusters that contain many validators (eg. \u201cLido\u201d, \u201cCoinbase\u201d, \u201cVitalik Buterin\u201d). You can find a dump of the former here, here and here, and the latter here.\nWe then run a script that computes the total number of co-failures: instances of two validators within the same cluster being assigned to attest during the same slot, and failing in that slot.\nWe also compute expected co-failures: the number of co-failures that \u201cshould have happened\u201d if failures were fully the result of random chance.\nFor example, suppose that there are ten validators with one cluster of size 4 and the others independent, and three validators fail: two within that cluster, and one outside it.\n\n\n\nThere is one co-failure here: the second and fourth validators within the first cluster. If all four validators in that clusters had failed, there would be six co-failures, one for each six possible pairs.\nBut how many co-failures \u201cshould there\u201d have been? This is a tricky philosophical question. A few ways to answer:\n\nFor each failure, assume that the number of co-failures equals the failure rate across the other validators in that slot times the number of validators in that cluster, and halve it to compensate for double-counting. For the above example, this gives \\frac{2}{3}.\nCalculate the global failure rate, square it, and then multiply that by \\frac{n * (n-1)}{2} for each cluster. This gives (\\frac{3}{10})^2 * 6 = 0.54.\nRandomly redistribute each validator\u2019s failures among their entire history.\n\nEach method is not perfect. The first two methods fail to take into account different clusters having different quality setups. Meanwhile, the last method fails to take into account correlations arising from different slots having different inherent difficulties: for example, slot 8103681 has a very large number of attestations that don\u2019t get included within a single slot, possibly because the block was published unusually late.\n\n882\u00d7227 11.8 KB\nSee the \u201c10216 ssfumbles\u201d in this python output.\n\nI ended up implementing three approaches: the first two approaches above, and a more sophisticated approach where I compare \u201cactual co-failures\u201d with \u201cfake co-failures\u201d: failures where each cluster member is replaced with a (pseudo-) random validator that has a similar failure rate.\nI also explicitly separate out fumbles and misses. I define these terms as follows:\n\nFumble: when a validator misses an attestation during the current epoch, but attested correctly during the previous epoch\nMiss: when a validator misses an attestation during the current epoch and also missed during the previous epoch\n\nThe goal is to separate the two very different phenomena of (i) network hiccups during normal operation, and (ii) going offline or having longer-term glitches.\nI also simultaneously do this analysis for two datasets: max-deadline and single-slot-deadline. The first dataset treats a validator as having failed in an epoch only if an attestation was never included at all. The second dataset treats a validator as having failed if the attestation does not get included within a single slot.\nHere are my results for the first two methods of computing expected co-failures. SSfumbles and SSmisses here refer to fumbles and misses using the single-slot dataset.\n\n\n\n\n\nFumbles\nMisses\nSSfumbles\nSSmisses\n\n\n\n\nExpected (algo 1)\n8602090\n1695490\n604902393\n2637879\n\n\nExpected (algo 2)\n937232\n4372279\n26744848\n4733344\n\n\nActual\n15481500\n7584178\n678853421\n8564344\n\n\n\nFor the first method, the Actual row is different, because a more restricted dataset is used for efficiency:\n\n\n\n\n\nFumbles\nMisses\nSSfumbles\nSSmisses\n\n\n\n\nFake clusters\n8366846\n6006136\n556852940\n5841712\n\n\nActual\n14868318\n6451930\n624818332\n6578668\n\n\n\nThe \u201cexpected\u201d and \u201cfake clusters\u201d columns show how many co-failures within clusters there \u201cshould have been\u201d, if clusters were uncorrelated, based on the techniques described above. The \u201cactual\u201d columns show how many co-failures there actually were. Uniformly, we see strong evidence of \u201cexcess correlated failures\u201d within clusters: two validators in the same cluster are significantly more likely to miss attestations at the same time than two validators in different clusters.\nHow might we apply this to penalty rules?\nI propose a simple strawman: in each slot, let p be the current number of missed slots divided by the average for the last 32 slots. That is, p[i] = \n\\frac{misses[i]}{\\sum_{j=i-32}^{i-1}\\ misses[j]}. Cap it: p \\leftarrow min(p, 4). Penalties for attestations of that slot should be proportional to p. That is, the penalty for not attesting at a slot should be proportional to how many validators fail in that slot compared to other recent slots.\nThis mechanism has a nice property that it\u2019s not easily attackable: there isn\u2019t a case where failing decreases your penalties, and manipulating the average enough to have an impact requires making a large number of failures yourself.\nNow, let us try actually running it. Here are the total penalties for big clusters, medium clusters, small clusters and all validators (including non-clustered) for four penalty schemes:\n\nbasic: Penalize one point per miss (ie. similar to status quo)\nbasic_ss: the same but requiring single-slot inclusion to not count as a miss\nexcess: penalize p points with p calculated as above\nexcess_ss: penalize p points with p calculated as above, requiring single-slot inclusion to not count as a miss\n\nHere is the output:\n                   basic          basic_ss       excess         excess_ss    \nbig                0.69           2.06           2.73           7.96           \nmedium             0.61           3.00           2.42           11.54         \nsmall              0.98           2.41           3.81           8.77           \nall                0.90           2.44           3.54           9.30\n\nWith the \u201cbasic\u201d schemes, big has a ~1.4x advantage over small (~1.2x in the single-slot dataset). With the \u201cexcess\u201d schemes, this drops to ~1.3x (~1.1x in the single-slot dataset). With multiple other iterations of this, using slightly different datasets, the excess penalty scheme uniformly shrinks the advantage of \u201cthe big guy\u201d over \u201cthe little guy\u201d.\nWhat\u2019s going on?\nThe number of failures per slot is small: it\u2019s usually in the low dozens. This is much smaller than pretty much any \u201clarge staker\u201d. In fact, it\u2019s smaller than the number of validators that a large staker would have active in a single slot (ie. 1/32 of their total stock). If a large staker runs many nodes on the same physical computer or internet connection, then any failures will plausibly affect all of their validators.\nWhat this means is: when a large validator has an attestation inclusion failure, they single-handedly move the current slot\u2019s failure rate, which then in turn increases their penalty. Small validators do not do this.\nIn principle, a big staker can get around this penalty scheme by putting each validator on a separate internet connection. But this sacrifices the economies-of-scale advantage that a big staker has in being able to reuse the same physical infrastructure.\nTopics for further analysis\n\nFind other strategies to confirm the size of this effect where validators in the same cluster are unusually likely to have attestation failures at the same time\nTry to find the ideal (but still simple, so as to not overfit and not be exploitable) reward/penalty scheme to minimize the average big validator\u2019s advantage over little validators.\nTry to prove safety properties about this class of incentive schemes, ideally identify a \u201cregion of design space\u201d within which risks of weird attacks (eg. strategically going offline at specific times to manipulate the average) are too expensive to be worth it\nCluster by geography. This could determine whether or not this mechanism also creates an incentive to geographically decentralize.\nCluster by (execution and beacon) client software. This could determine whether or not this mechanism also creates an incentive to use minority clients.\n\nMini-FAQ\nQ: But wouldn\u2019t this just lead to staking pools architecturally decentralizing their infra without politically decentralizing themselves, and isn\u2019t the latter what we care about more at this point?\nA: If they do, then that increases the cost of their operations, making solo staking relatively more competitive. The goal is not to single-handedly force solo staking, the goal is to make the economic part of the incentives more balanced. Political decentralization seems very hard or impossible to incentivize in-protocol; for that I think we will just have to count on social pressure, starknet-like airdrops, etc. But if economic incentives can be tweaked to favor architectural decentralization, that makes things easier for politically decentralized projects (which cannot avoid being architecturally decentralized) to get off the ground.\nQ: Wouldn\u2019t this hurt the \u201cmiddle-size stakers\u201d (wealthy individuals who are not big exchanges/pools) the most, and encourage them to move to pools?\nA: In the table above, the \u201csmall\u201d section refers to stakers with 10-300 validators, ie. 320-9600 ETH. That includes most wealthy people. And as we can see, those stakers suffer significantly higher penalties than pools today, and the simulation shows how the proposed adjusted reward scheme would equalize things between precisely those validators and the really big ones. Mathematically speaking, someone with 100 validator slots would only have 3 per slot, so they would not be greatly affecting the penalty factor for a round; only validators that go far above that would be.\nQ: Post-MAXEB, won\u2019t big stakers get around this by consolidating all their ETH into one validator?\nA: The proportional penalty formula would count total amount of ETH, not number of validator IDs, so 4000 staked ETH that acts the same way would be treated the same if it\u2019s split between 1 validator or 2 or 125.\nQ: Won\u2019t adding even more incentives to be online create further pressure to optimize and hence centralize, regardless of the details?\nA:The parameters can be set so that on average, the size of the incentive to be online is the same as it is today.\n", "\nAuthors: @gyllone @Madao-3 @xiangxiecrypto\n1. Introduction\nBlockchain privacy and regulatory compliance are experiencing a dynamic shift, significantly inspired by the pivotal works of @vbuterin and @ameensol, their insightful paper, Blockchain Privacy and Regulatory Compliance: Towards a Practical Equilibrium along with the enlightening forum post on permissioned privacy pools, has laid a foundational framework for understanding the intricate balance between maintaining transactional privacy and adhering to regulatory norms. These resources delve deeply into the challenges and viable solutions for synchronizing privacy with compliance in the ever-evolving blockchain landscape.\nA key takeaway from their works is the concept of \u2018honest address sets\u2019 which profoundly influences our approach to achieving a second-generation privacy protocol. In this framework, Association Set Providers (ASPs) emerge as crucial facilitators of credit within the zk-credit system, offering a structured approach to assess and attest to the creditworthiness of users. Meanwhile, membership proof ensures that all participants in a transaction are part of an honest address set, enhancing the security of our protocol and providing a safer, more trustworthy environment for users.\nIn summary, the works of Buterin and Soleimani have been instrumental in guiding our approach to developing a blockchain system that upholds both privacy and regulatory compliance. By integrating these concepts into our system, we have been able to create a more secure, efficient, and privacy-preserving second-generation protocol.\n2. Components\n2.1 Infrastructure\nUsers\u2019 fungible assets are hashed into a Sparse Binary Merkle Tree. Suppose leaf node is initialized with a constant value H'=\\mathrm{Hash}(0) until any asset occupies the slot of the leaf, then the leaf node should be updated to H=\\mathrm{Hash}(identifier \\ | \\ amount \\ | \\ commitment), where identifier is an associated tag of this asset (e.g. the user\u2019s address), amount is the quantity of the asset stored at the leaf node, and commitment = \\mathrm{Hash}(secret), with secret being the key of the user who owns the asset.\nFor an unused leaf node slot, the user can deposit an asset into the pool and take the slot while providing the asset and commitment. For withdrawal, the user should provide the withdrawal amount, new leaf hash, identifiers subset, and the snark proof.\nmerkletree1401\u00d7842 66.4 KB\n2.2 Proof of UTXO\nThe design of the withdrawal mechanism is similar to UTXO model. Users select a set of leaf nodes that indicate their assets to use as inputs, then hash the remaining balance into a new leaf node as output. The difference between the total input and the output is the amount of the withdrawal.\nNow let the ULO source list be L, for each ULO with an index i\\in L, we must first verify its existence and then calculate the corresponding leaf\u2019s nullifier within the circuit as follows:\n\n\n\\begin{aligned}\n\ncommitment_i&=\\mathrm{Hash}(secret_i)\n\n\\\\\n\nnullifier_i&=\\mathrm{Hash}(secret_i^{-1})\n\n\\\\\n\nleaf_i&=\\mathrm{Hash}(identifier_i \\ | \\ amount_i \\ | \\ commitment_i)\n\n\\\\\n\nroot&=\\mathrm{MerkleProof}(leaf_i, \\ paths)\n\n\\end{aligned}\n\n\nNext, we need to demonstrate that the total input amount is equal to the total output amount within the circuit, and then generate the leaf output. The following equations represent this:\n\n\n\\begin{aligned}\n\n\\sum_{i\\in L}amount_i&=amount_w + amount_o\n\n\\\\\n\ncommitment&=\\mathrm{Hash}(secret)\n\n\\\\\n\nleaf&=\\mathrm{Hash}(identifier \\ | \\ amount_o \\ | \\ commitment)\n\n\\end{aligned}\n\n\nWhere amount_w is the withdrawal amount, amount_o is the output amount.\nIn the above process, we set \\{nullifier_i\\}_{i\\in L}, leaf, root and amount_w as public variables of the circuit, which is constructed by Plonk.\n2.3 Proof of Innocence\n2.3.1 Why not Merkle proof\nProof of Innocence in ZKT Network lies in demonstrating that the inputs come from an arbitrary set constructed by the user, meanwhile ensuring that this set is publicly available to anyone. Typically, the set can be organized into a new Merkle Tree and the user should prove that each input is also a leaf node of the Merkle Tree (Indeed, privacy-focused solutions like Tornado Cash v2, and Privacy Pools have adopted this kind of design).\nApparently, if the set is small, the user\u2019s privacy is compromised. If the set is too large, generating the corresponding Merkle tree incurs a huge gas cost on EVM, since ZK-friendly hash functions (Poseidon Hash) are not integrated by EVM primitively while creating Merkle tree has a complexity of O(n) hash.\nWe hereby adopt the Plookup to construct proof of membership, instead of Merkle Proof. There are 2 main advantages of Plookup in our membership proof.\n\n\nIn the proving phase, we do not need to perform merkle proof for each ULO, which significantly reduces the circuit size.\n\n\nIn the verification phase, we do not need to perform hash operations in EVM. Only one elliptic curve pairing is required.\n\n\n2.3.2 Plookup embedding\nAssuming the user provides an identifier set of size m from the source ULOs, denoted as \\mathbf{t}, then we pad the set \\mathbf{t} with 0 until it satisfies the size of circuit size n.\n\n\n\\mathbf{t}=\\{\\mathrm{id}_0,...,\\mathrm{id}_{m-1},0,...,0\\}\n\n\nFurthermore, we define \\mathbf{q_T}, which satisfy q_{Ti}\\cdot t_i = 0:\n\n\nq_{Ti}=\n\n\\left\\{\n\n\\begin{matrix}\n\n0, &\\ i \\leq m\n\n\\\\\n\n1, &\\ i > m\n\n\\end{matrix}\n\n\\right.\n\n\nLet \\mathbf{f} be the query table:\n\n\nf_i = q_{Ki}\\cdot c_i=\n\n\\left\\{\n\n\\begin{matrix}\n\nc_i, &\\ \\mathrm{if \\ the} \\ i\\mathrm{\\ gate \\ is \\ a \\ lookup \\ gate}\n\n\\\\\n\n0, &\\ \\mathrm{otherwise}\n\n\\end{matrix}\n\n\\right.\n\n\n\n\nq_{Ki}=\n\n\\left\\{\n\n\\begin{matrix}\n\n1, &\\ \\mathrm{if \\ the} \\ i\\mathrm{\\ gate \\ is \\ a \\ lookup \\ gate}\n\n\\\\\n\n0, &\\ \\mathrm{otherwise}\n\n\\end{matrix}\n\n\\right.\n\n\nWhere c_i is the output witness defined in arithmetic gate of Plonk, we activate it when c_i represents the identifier witness. q_{Ki} is the selector that switches on/off the lookup gate.\nThen we just need to prove \\mathbf{f} is a subset of \\mathbf{t}, by Plookup.\nDuring the verification phase, besides the zk-SNARK proof verification, we also verify the opening proof of \\mathbf{t} at \\{1, \\omega, \\omega^2, ..., \\omega^{m-1}\\} to confirm the correctness of each identifier in \\mathbf{t}, without any hash operations.\n3. Association Set Provider\nThe Association Set Provider (ASP) mechanism allows a third party to oversee the membership list. Similar to an attestor, an ASP offers attestation services for end-users. The associated address, along with potential attested and sealed data, is submitted to a designated ASP smart contract. In ZKT Network, any entity can register as an ASP. The selection of which ASP to utilize depends on the choices made by end-users and Dapps.\nThe data category can be defined by the ASP, allowing support for a diverse range of potential data from web2, such as credit scores, KYC results, etc. For attesting any data obtained through the standard Transport Layer Security (TLS) protocol (e.g, HTTPS) and to accommodate a large volume of potential data, we recommend employing MPC-TLS style algorithms within the ASP. This approach, initially introduced by DECO and significantly improved by PADO, is detailed further in this paper. Within this framework, users can prove to the attestor that the data indeed originates from the intended sources without leaking any other information.\nWe list the basic workflow in the following figure.\nasp-workflow1920\u00d71080 88.7 KB\nThe inclusion of data in the membership list is discretionary. This flexibility arises from situations where the data entry might simply be binary (YES/NO). In such cases, the smart contract accepts addresses marked as YES, allowing the omission of unnecessary data entries. However, programmability can be introduced when the sealed data holds additional meanings. For instance, an ASP might attest a user\u2019s FICO score and store the encrypted score in the smart contract. Subsequently, Dapps can devise more adaptable withdrawal conditions. For example, users with higher FICO scores may be eligible to withdraw a larger quantity of tokens, whereas those with lower FICO scores might have access to only a smaller amount. This introduces a higher degree of flexibility for designing diverse applications.\n4. Client-Side Acceleration Solution\nIn our quest to enhance user experience and efficiency in the ZKT Network system, we\u2019ve focused on client-side computational optimization. This optimization is crucial for reducing the computational burden on user devices and accelerating transaction and verification processes.\n4.1 WebAssembly Integration\nOur integration of WebAssembly (Wasm) allows for the execution of complex cryptographic proof generation processes directly in the user\u2019s browser. By compiling our Rust code into WebAssembly, we\u2019ve significantly reduced dependency on centralized servers and improved system responsiveness and efficiency.\n4.2 Local Computation Optimization\n4.2.1 Transaction Decision Logic\nIn our UTXO model, each user\u2019s funds are represented as encrypted NOTES, each corresponding to a specific amount of assets (e.g., ETH). Our system intelligently selects the appropriate combination of NOTES to fulfill withdrawal requests. For instance, if a user has NOTES of 1 ETH, 1 ETH, 2 ETH, and 3 ETH and wishes to withdraw 2.5 ETH, our system automatically utilizes the 1 ETH, 1 ETH, and 2 ETH NOTES.\n4.2.2 Generation of New Deposit Proofs\nFollowing the utilization of certain NOTES for transactions, our system generates new deposit proofs corresponding to the remaining amount. Continuing the previous example, after spending the 1 ETH, 1 ETH, and 2 ETH NOTES, a new deposit proof for 1.5 ETH is created, ensuring secure and effective management of funds post-transaction.\n4.2.3 Implementation in Chrome Plugin and iOS App\nTo achieve this computational optimization, we\u2019ve implemented tailored strategies in our Chrome plugin and iOS app. The Chrome plugin optimizes the NOTE selection and new proof generation process, leveraging the browser\u2019s processing capabilities. In contrast, our iOS app employs multi-threading technology to accelerate these computations, taking full advantage of the high-performance capabilities of iOS devices.\n4.3 Caching Strategies\nWe have implemented caching strategies to reduce redundant computations and network requests. Key data, such as parts of the Merkle Tree, are cached on the user device for quick retrieval in subsequent transactions or verifications, reducing network traffic and significantly enhancing overall system performance.\n4.4 User Experience Enhancement\nLastly, we place a high emphasis on enhancing the user experience. This involves not only technical optimizations but also improvements in interface design. We ensure that the user interface is intuitive and the transaction process is seamless. Real-time feedback and detailed error messages enhance user trust and satisfaction.\nIn summary, our client-side acceleration solution is a key strategy for enhancing the performance of the ZKT Network system. Through these technological and methodological applications, we not only enhance the speed and efficiency of transactions but also optimize the user experience, making ZKT Network a more powerful and user-friendly blockchain privacy platform.\n5. What does it mean\uff1f\nAt ZKT Network, we\u2019re dedicated to realizing a grand vision: leading a technological revolution by balancing blockchain transaction privacy with global compliance standards. Our innovation extends beyond technical realms, exploring new frontiers in the balance of privacy and compliance.\nTherefore, we warmly invite researchers and experts in the blockchain field to participate in our project, offering feedback and suggestions to advance this sector. ZKT Network looks forward to your professional insights and collaboration to develop a more comprehensive and effective blockchain privacy solution.\nWe also encourage community members to explore innovative applications based on our framework, as per their requirements. ZKT Network is eager to imagine the future with you, face challenges together, and build a more secure, compliant, and innovative blockchain world.\nAt ZKT Network, we believe that through collective wisdom and collaborative effort, we can achieve a perfect integration of privacy protection and compliance, creating a future filled with possibilities.\n6. Summary\nThis post thoroughly explores the approaches and practices for achieving a balance between privacy protection and regulatory compliance in blockchain technology. By incorporating innovative applications of honest address sets, zk-credit, and membership proof, we demonstrate how to maintain user privacy while adhering to regulatory standards. The focus extends beyond the technical and system architecture to include engineering practices and enhancements in user experience performance. Our aim is to establish a secure, efficient blockchain system that delivers an exceptional user experience. Representing a step forward, this second-generation privacy transaction protocol signifies a better tomorrow, where privacy is not just a feature \u2013 it\u2019s the norm.\nNote: For more detailed proofs and information on the Plonk-related aspects mentioned in this article, please visit our GitHub page: https://github.com/ZKTNetwork/papers/tree/main/Advancing%20Blockchain%20Transaction%20Privacy%20and%20Compliance%3A%20Insights%20into%20Innovative%20Engineering%20Practices.\n\n\n\n\nGitHub\n\n\n\nZKTNetwork - Overview\nZKTNetwork has 7 repositories available. Follow their code on GitHub.\n\n\n\n\n\n", "\nTLDR: We highlight a special subset of rollups we call \u201cbased\u201d or \u201cL1-sequenced\u201d. The sequencing of such rollups\u2014based sequencing\u2014is maximally simple and inherits L1 liveness and decentralisation. Moreover, based rollups are particularly economically aligned with their base L1.\nDefinition\nA rollup is said to be based, or L1-sequenced, when its sequencing is driven by the base L1. More concretely, a based rollup is one where the next L1 proposer may, in collaboration with L1 searchers and builders, permissionlessly include the next rollup block as part of the next L1 block.\nAdvantages\n\n\nliveness: Based sequencing enjoys the same liveness guarantees as the L1. Notice that non-based rollups with escape hatches suffer degraded liveness:\n\n\nweaker settlement guarantees: Transactions in the escape hatch have to wait a timeout period before guaranteed settlement.\n\ncensorship-based MEV: Rollups with escape hatches are liable to toxic MEV from short-term sequencer censorship during the timeout period.\n\nnetwork effects at risk: A mass exit triggered by a sequencer liveness failure (e.g. a 51% attack on a decentralised PoS sequencing mechanism) would disrupt rollup network effects. Notice that rollups, unlike the L1, cannot use social consensus to gracefully recover from sequencer liveness failures. Mass exists are a sword of Damocles in all known non-based rollup designs.\n\ngas penalty: Transactions that settle through the escape hatch often incur a gas penalty for its users (e.g. because of suboptimal non-batched transaction data compression).\n\n\n\ndecentralisation: Based sequencing inherits the decentralisation of the L1 and naturally reuses L1 searcher-builder-proposer infrastructure. L1 searchers and block builders are incentivised to extract rollup MEV by including rollup blocks within their L1 bundles and L1 blocks. This then incentivises L1 proposers to include rollup blocks on the L1.\n\nsimplicity: Based sequencing is maximally simple; significantly simpler than even centralised sequencing. Based sequencing requires no sequencer signature verification, no escape hatch, and no external PoS consensus.\n\n\nhistorical note: In January 2021 Vitalik described based sequencing as \u201ctotal anarchy\u201d that risks multiple rollup blocks submitted at the same time, causing wasted gas and effort. It is now understood that proposer-builder separation (PBS) allows for tightly regimented based sequencing, with at most one rollup block per L1 block and no wasted gas. Wasted zk-rollup proving effort is avoided when rollup block n+1 (or n+k for k >= 1)  includes a SNARK proof for rollup block n.\n\n\n\ncost: Based sequencing enjoys zero gas overhead\u2014no need to even verify signatures from centralised or decentralised sequencers. The simplicity of based sequencing reduces development costs, shrinking time to market and collapsing the surface area for sequencing and escape hatch bugs. Based sequencing is also tokenless, avoiding the regulatory burden of token-based sequencing.\n\nL1 economic alignment: MEV originating from based rollups naturally flows to the base L1. These flows strengthen L1 economic security and, in the case of MEV burn, improve the economic scarcity of the L1 native token. This tight economic alignment with the L1 may help based rollups build legitimacy. Importantly, notice that based rollups retain the option for revenue from L2 congestion fees (e.g. L2 base fees in the style of EIP-1559) despite sacrificing MEV income.\n\nsovereignty: Based rollups retain the option for sovereignty despite delegating sequencing to the L1. A based rollup can have a governance token, can charge base fees, and can use proceeds of such base fees as it sees fit (e.g. to fund public goods \u00e0 la Optimism).\n\nDisadvantages\n\n\nno MEV income: Based rollups forgo MEV to the L1, limiting their revenue to base fees. Counter-intuitively, this may increase overall income for based rollups. The reason is that the rollup landscape is plausibly winner-take-most and the winning rollup may leverage the improved security, decentralisation, simplicity, and alignment of based rollups to achieve dominance and ultimately maximise revenue.\n\nconstrained sequencing: Delegating sequencing to the L1 comes with reduced sequencing flexibility. This makes the provision of certain sequencing services harder, possibly impossible:\n\n\npre-confirmations: Fast pre-confirmations are trivial with centralised sequencing, and achievable with an external PoS consensus. Fast pre-confirmations with L1 sequencing is an open problem with promising research avenues including EigenLayer, inclusion lists, and builder bonds.\n\nfirst come first served: Providing Arbitrum-style first come first served (FCFS) sequencing is unclear with L1 sequencing. EigenLayer may unlock an FCFS overlay to L1 sequencing.\n\n\n\nNaming\nThe name \u201cbased rollup\u201d derives from the close proximity with the base L1. We acknowledge the naming collision with the recently-announced Base chain from Coinbase, and argue this could be a happy coincidence. Indeed, Coinbase shared two design goals in their Base announcement:\n\n\ntokenlessness: \u201cWe have no plans to issue a new network token.\u201d (in bold)\n\ndecentralisation: \u201cWe [\u2026] plan to progressively decentralize the chain over time.\u201d\n\nBase can achieve tokenless decentralisation by becoming based.\n", "\nSummary & TL;DR\nThe ProbeLab team (probelab.io) is carrying out a study on the performance of Gossipsub in Ethereum\u2019s P2P network. Following from our previous post on the \u201cGossipsub Network Dynamicity through GRAFTs and PRUNEs\u201d in this post we investigate the number of messages and duplicated messages seen by our node, per topic. There is no public data on the overhead that broadcasting messages and control data over the network imply on each participating node.\nFor the purposes of this study, we have built a tool called Hermes, which acts as a GossipSub listener and tracer (GitHub - probe-lab/hermes: A Gossipsub listener and tracer.). Hermes subscribes to all relevant pubsub topics and traces all protocol interactions. The results reported here are from a 3.5hr trace.\nStudy Description: Gossipsub\u2019s design is inherently allowing for message duplicates. A brief model we develop shows that it\u2019s normal to receive each message up to 3 extra times (as a duplicate). This excludes the gossip mechanism which propagates messages through the IHAVE/IWANT control message sequence.\nTL;DR: We find that indeed duplicates through mesh stay in the order of 3 per message or below, which, however, doesn\u2019t count for duplicates through gossip. For instance, there are edge cases where a message is requested (and responded to) through an IWANT message while the actual message is already in transit. Eventually, this results in an extra duplicate. We make two recommendations:\n\nReduce the number of concurrent IWANT messages we send through a limiting factor (somewhat similar to kademlia\u2019s alpha parameter).\nLower the current heartbeat frequency (i.e., increasing the heartbeat interval) from 0.7 seconds to 1 second (as per the original protocol spec and recommendation). This would reduce the excessive IHAVE messages and reduce the chances of generating extra duplicates.\n\nBackground\nGossipSub is a routing system that can be enabled on libp2p\u2019s PubSub message broadcasting protocol. This protocol organizes the message broadcasting channels on what is commonly known as Topics, where peers subscribed to a given topic keep a particular subset of connected peers for that particular topic. This subset of peer connections per topic is also known as \u201cmesh\u201d.\nIn the case of GossipSub, the standard broadcasting mechanism of PubSub is extended with a few sets of enhancements that make it:\n\nmore efficient than what is commonly called flooding, reducing the protocol\u2019s bandwidth usage\nmore resilient, as the protocol:\n\nshares metadata of seen messages over sporadic Gossip messages (for censorship or Sybil attacks)\nkeeps a local score for each mesh-connected peer to ensure healthy and useful connections, where each peer keeps connections with the highest scoring neighbours\navoids sharing a message with peers that already sent the message to us\n\n\n\nThis all looks good on paper. However, there is still no public data on the overhead that broadcasting messages and control data over the network imply on each participating node. Even more importantly, how much room for improvement exists within the protocol and the implementations to make it more optimal.\nExpected Results\nMessage propagation through the GossipSub\u2019s mesh considers some occasional duplicates that can arrive as the message might come from different peers within the mesh:\nGiven:\n\nn as the number of nodes in the graph\nk as the mesh degree\nl as the number of connections (links) between two nodes l = \\frac{nk}{2}\n\nThe number of links used to propagate a message to all nodes in the graph can be defined as n-1 ~= n. The links form a spanning tree with the message origin as root (n is big enough compared to the initial sender link, so that it can be considered negligible).\nThe number of links not used to propagate a specific message corresponds to l-n = \\frac{n(k-2)}{2}.\nThis means that on average each node will have 1 link used to receive a message, 1 to propagate it to a peer that doesn\u2019t have it yet. And the rest k-2, to either send or receive the duplicate message.\nAssuming that \\frac{k-2}{2} links are used to send the message to peers that already have it, it means that we receive \\frac{k-2}{2} duplicate messages.\nIn the case of Ethereum, k=8, and therefore, it follows that \\frac{k-2}{2} = 3. So, the expected value is to receive 3 duplicate messages for each message.\nResults\nAs previously introduced, this report aims to provide insights on:\n\nthe number of duplicate messages that we receive per each shared message in the network,\nthe extra bandwidth that we are spending on duplicates,\nany existing unexpected behavior or potential optimization that could be applied on GossipSub.\n\n\nNOTES:\nThe numbers presented in the following sections belong to the same 3.5 hours run of Hermes as the previous studies, with the following extra configuration:\n\nThe experiment is ran on the Holesky network\nOur node was subscribed to the following topics:\n\nbeacon_block\nbeacon_aggregate_and_proof\nsync_commmittee_contribution_and_proof\nattester_slashing\nproposer_slashing\nvoluntary_exit * (check Hermes issue \u2192 Broadcasting of invalid `voluntary_exit` messages to mesh peers \u00b7 Issue #24 \u00b7 probe-lab/hermes \u00b7 GitHub)\nbls_to_execution_change\n\n\n\n\nOverall Number of Messages\nTo give a little bit of context, the report starts by taking a look at the number of messages and the respective duplicates received over time. The following graph shows the number of HANDLED events by the libp2p-host in comparison with the DELIVERED and DUPLICATED ones.\n\nNOTE: In this report we will consider the DELIVER events as unique identifier of the arrival of a message. This is because the internal event tracer at the libp2p host notifies of the arrival of a unique message at multiple levels, which in turn, makes the HANDLED and DELIVER events at the arrival of a new message the exact same notification, just at different levels of the host.\n\noverall-number-of-events2000\u00d71200 177 KB\n\nThe number of unique messages (i.e., HANDLE_MESSAGE) stays steady around the 3,000 and 3,200 unique messages per minute.\nBy looking closer into the messages per topic (not shown here), we observe that the topic with the highest message frequency is the beacon_aggregate_and_proof one, receiving over 90% of the tracked unique messages.\nThere are some duplicated spikes at the beacon_block topic that reach up to 60 duplicates  in some occasions.\nThe number of duplicates seems to vary quite wildly over time, which can be related to the number of connections per mesh (as per the analysis done further up which showed that 3 duplicates per message are expected).\n\nNumber of Duplicate Messages\nWhen it comes to the actual number of DUPLICATE messages, the following figures show that number of duplicates can oscillate over time.\nduplicates-per-topic1000\u00d7600 83.3 KB\nClearly, the beacon_block topic seems to be the only one generating the largest number of spikes at times.\nCDF of Duplicate Messages\nThe following graph shows the Cumulative Distribution Function (CDF) of the duplicates per message per topic. In the graph, we can see that:\n\nsmaller but more frequent messages like the beacon_ggregate_and_proof and sync_commitee_contributions do have fewer duplicates.\n\nbetween 32% and 45% of the messages do not have any duplicates.\n50% of the messages are received with less than 2 duplicate messages, keeping the mean lower than the theoretical target of 3 duplicates per message.\nthe upper tail shows that less than 10% of the messages get more than 4 duplicates, with a cap at 8-10 duplicates (i.e., the node\u2019s mesh size, D).\n\n\nthe case of the beacon_blocks is completely different.\n\nthere are almost no recorded messages without duplicates (1%-2%).\n54% of the messages report the expected 3  duplicates from the mesh\nTaking look at the tail of the CDF (shown in the dropdown plot further down) there are a few messages that were received up to 34 or 40 times.\n\n\n\nCDF-duplicates1000\u00d7600 29.3 KB\nCorrelation between Message Size and Number of Duplicates\nFrom the CDF above there seems to be a pattern of \u201cthe bigger the size of the message, the more duplicates it has\u201d. So we went a step further to investigate if there is indeed a correlation. The following graph shows that the correlation between the size of a message and the number of duplicates is somewhat present but is not a norm or at least doesn\u2019t follow any fixed pattern.\nThe figure is complemented by two auxiliary quartile plots or \u201cboxplots\u201d, which represent the given distribution of points of their respective axis, helping us understand that:\n\nsync_commmittee_contribution_and_proof messages are the smallest ones in size, which also correlates with the smallest ratio of duplicate messages.\nbeacon_aggregate_and_proof messages are the second ones in size, having also a bigger tail of duplicates on the Y concentration plot.\nbeacon_block messages, despite being the ones with the widest variation in size, do not follow any particular pattern that could correlate the message size with the number of duplicates.\n\nmsg-size-number-of-duplicates595\u00d7582 48.6 KB\nAs such, we conclude that there is no correlation between message size and number of duplicates.\nArrival Time of Duplicates\nReducing the number of duplicates has already been a topic of discussion in the community. There are already some proposals like gossipsub1.2  that spotted this large number of duplicated messages previously, proposing the addition of a new control IDONTWANT message that could not only notify other peers that we already got a message, but also cancel the IWANT ongoing messages.\nIn order to see how effective the IDONTWANT control message would be, we\u2019ve computed the time between the first delivery of each message and their respective first duplicate. This is done to validate that there is enough time to send the IDONTWANT message once a new message is received (prior to the message validation) and before the duplicate starts being sent over.\nThe following graph gives the time between the delivery time of a message and the time to the first duplicated message in seconds.\narrival-cdf1000\u00d7600 48.4 KB\nResults show that 50% of the duplicated beacon blocks arrive within 73 milliseconds, roughly an entire Round Trip Time (RTT) with a well connected peer. In practice, this means that the IDONTWANT message could prevent at least the other 50% of messages that arrive between 73 milliseconds and 2 seconds of the first arrival.\nWe\u2019ve spotted that a big part of the duplicated messages arrive from IWANT messages that we sent milliseconds before the arrival of the same message though the mesh.\nThe gossipsub1.2 proposal already contemplates this scenario, where the same IDONTWANT message could break or stop any ongoing responses to IWANT messages for that msgID.\nIn summary, we conclude that the IDONTWANT control message addition to Gossipsub will be a valuable enhancement that can indeed prevent the vast majority of duplicate messages.\nConclusions and takeaways\n\nThis set of conclusions have been extracted from running the go-libp2p  implementation and, although it also involves the traces of how other implementations interact with Hermes, it might be a biased conclusion from the point of view of the Go implementation.\n\n\n\nWe have identified that there is no limit on the number of peers that we simultaneously send IWANT messages to for the same msgID.\nWe identify that this has some benefits:\n\nConcurrently fetches the message from multiple actors.\nBypasses bandwidth limitations of peer(s) we have sent IWANT messages to, since we have forwarded the IWANT message to multiple peers.\n\nHowever, it also has obvious downsides:\n\n\nWe receive multiple duplicates from the peers that respond to our simultaneous IWANT request, consuming more bandwidth on both ends.\n\n\nThe message could be already on the wire through the mesh connections, so when the IWANT message responses arrive, the message was already delivered through the mesh.\n\n\nThere is no track of who we contacted for a given message, given that Gossipsub is:\n\nforwarding the message only the first time we see it, and\nremoving the peer that sent us the message from the list of peers we\u2019re broadcasting the message to and forgetting about that peer.\n\nThis makes the entire broadcasting process unaware of who sent us that message in IHAVEs, or who we are already contacting for a particular message - resulting in multiple duplicates.\n\n\nCanceling ongoing IWANTmessages with IDONTWANT messages, which is a proposal included in gossipsub1.2 is a valuable enhancement that will limit the number of duplicates.\nRecommendation 1\nWe propose having a limiting factor (somewhat similar to kademlia\u2019s alpha parameter), which would limit the number of concurrent IWANT messages we send for the same msgID.\n\n\n\n\nThe gossiping mechanism of Gossipsub acts as a backup mechanism to the broadcasting/mesh propagation part of the protocol for those messages that didn\u2019t manage to reach all nodes in the network. The more frequent gossiping is, the higher its contribution becomes to message propagation (i.e., more messages are being requested through IWANT requests because they have not reached the entirety of the network).\nAn edge case that results from very frequent gossiping (i.e., small heartbeat interval) is that messages that are already in transit, but have not been downloaded completely, are being requested through an IWANT message. This inevitably results in a duplicate message once both messages arrive at their destination.\nIt is hard to quantify how often the message responses to IWANT messages are indeed future duplicates, but it is still worth pointing out that high heartbeat frequency increases the chances of those edge cases.\nRecommendation 2\nA quick and straightforward optimization is to lower the current heartbeat frequency (i.e., increasing the heartbeat interval) from 0.7 seconds to 1 second (as per the original protocol spec and recommendation). This would reduce the excessive IHAVE messages and reduce the chances of generating extra duplicates.\n\n\n\n\nWe have spotted some edge cases that may occur due to the \u201clack\u201d of control over the triggered events at GossipSub (IHAVE/ IWANT).\nIt isn\u2019t easy to judge from the logs whether those cases are just a matter of timing, as GossipSub replies to those events as interruptions (at least in the Go implementation), or if some of those cases are caused by a bug in one of the implementations.\nWe found that the number of messages where we received multiple duplicates from the same peer to just 1% of the total number of beacon_blocks received. We, therefore, conclude that this is not critical or an issue that requires further investigation.\n\n\nFor more details and weekly network health reports on Ethereum\u2019s discv5 DHT network head over to probelab.io.\n", "\nScreenshot 2024-12-17 at 10.44.051556\u00d71562 318 KB\n^Rare picture of an includer, chilling and effortlessly collecting rewards for improving Ethereum\u2019s censorship resistance.\nby Thomas Thiery - December 17th, 2024\nThanks to Julian Ma and Barnab\u00e9 Monnot, Terence Tsao and Jacob Kaufmann for feedback and discussions on this post.\nIntroduction\nLately I\u2019ve been stumbling upon more and more discussions and research around incentives for transaction inclusion via FOCIL EIP-7805, state vs. inclusion preconfirmations, and further separating protocol roles and duties.  It has become increasingly clear to me that thinking from first principles about state and inclusion as two orthogonal dimensions could be useful in guiding future protocol development.\nDisclaimer: In this post, I\u2019ll use simplified, somewhat caricature-like definitions to differentiate inclusion transactions from state transactions, acknowledging that in practice there is much more nuance.\nInformal definitions, properties and life cycles\nInclusion transactions\nBy inclusion transactions, I mean order-invariant transactions (h/t James Prestwich) whose outcome remains the same regardless of the state on which they are executed; the only important factor is that they are included somewhere in the block. Examples include making a payment at a coffee shop or transferring tokens to a friend.\nBecause of their order-invariant property, inclusion transactions are typically sent to the public mempool. Publicly disclosing transaction information before inclusion is acceptable since no one can exploit these transactions (e.g., by frontrunning), as they don\u2019t intrinsically carry Maximal Extractable Value (MEV). To be considered valid for inclusion, these transactions must pay the base fee for each unit of gas consumed.\nIn a post-EIP-7805 world, inclusion transactions would mostly be included in Ethereum blocks by multiple IL proposers via FOCIL (using inclusion rules like time pending in the mempool, or priority fee ordering). By being publicly broadcast to the mempool, these transactions benefit from increased chances that one of multiple IL proposers will include them via their inclusion lists.\nState transactions\nBy state transactions, I mean order-dependent transactions whose outcomes change based on the state at the time of execution. For example, a transaction performing a token swap on an automated market maker like Uniswap.\nState transactions originating from regular users potentially carry Maximal Extractable Value (MEV) and can be exploited by searchers\u2014for example, through frontrunning or sandwich attacks\u2014if their content is publicly available before inclusion. These transactions are often time-sensitive and benefit from being sent through private channels to ensure MEV protection and/or receive rebates. When state transactions carry MEV and are made available to sophisticated parties like searchers, whether willingly or unwillingly, they are usually bundled together with other transactions to extract the MEV opportunity they create. Note that state transactions can also originate from sophisticated parties themselves; for example, arbitrageurs who take advantage of price differences between different exchanges to make a profit. In both cases, when state transactions generate MEV opportunities, they are usually accompanied by tips to validators (in the form of priority fees or by using coinbase transfers) in addition to the base fee.\nBecause FOCIL does not provide any guarantees on transaction ordering or inclusion during network congestion (i.e., when blocks are full), we can assume that the market structure for state transactions wouldn\u2019t change much in a post-EIP-7805 world.\n\nHere\u2019s a brief recap of the main differences between inclusion and state transactions:\n\nInclusion transactions:\n- Pay base fees to be considered for inclusion in the block.\n- Do not use priority fees to signal order preferences.\n- Benefit from being publicly broadcast to a large number of parties.\n- Do not have to rely on sophisticated actors for inclusion.\nState transactions:\n- Pay base fees to be considered for inclusion in the block.\n- Add priority fees to express preferences regarding the specific state on which they would like to be executed, which corresponds to a particular position in the block.\n- Benefit from being sent privately to one or a few sophisticated parties.\n\n\n\nNote: An inclusion transaction might pay priority fees for reasons unrelated to order preferences. For example, it may need to compensate the block producer for additional resource usage\u2014such as the extra propagation time required by blobs. Priority fees can also signal a desire for faster inclusion rather than a specific placement in the block. In other words, even if a transaction does not care about its position, it may be willing to pay more to reduce delays and be included sooner (e.g., blob transactions, fraud proof transactions).\nZooming in on Fees\nBase and Priority Fees\nSince EIP-1559 was implemented, all transactions\u2014both inclusion and state transactions\u2014must pay base fees to be considered for inclusion in a block. Priority fees, on the other hand, can serve different purposes depending on whether there is network congestion:\n\nNo Congestion: When there is enough space in the block to include all pending transactions:\n\nInclusion transactions do not generally need to pay priority fees since they will be included as long as they pay the base fee and do not care about being inserted at a specific position in the block.\nState transactions will pay priority fees to incentivize the block producer to execute them on a particular state.\n\n\nCongestion:\n\nInclusion transactions might then choose to add priority fees to increase their chances of being included in the next block\u2014for example, ahead of other inclusion transactions\u2014 rather than waiting until the network is no longer congested.\nState transactions will use priority fees regardless of whether there is network congestion.\n\n\n\nThe interesting takeaway here is that the boundary between state and inclusion transactions blurs during periods of congestion. When there isn\u2019t enough space for every transaction, simply wanting to be included anywhere in a given block conceptually becomes very similar to wanting to be executed on a specific state. Priority fees can thus be thought of as a one-size-fits-all mechanism to incentivize block producers and secure an advantageous position in a block.\nHowever, there is another obvious reason a transaction might not be included in a block even without congestion: censorship.\nCosts of censorship\nIn a post-EIP-7805 world, there are still a couple of ways to censor a transaction:\n\n\nBlock Stuffing: Given FOCIL\u2019s conditional and anywhere-in-block properties, the proposer can stuff its block up to the gas limit in order to exclude the transaction and still satisfy the IL conditions. To estimate the cost of stuffing multiple consecutive blocks, I used the following formula:\n\n\\text{Block Stuffing Cost} = (\\text{Gas Limit} - \\text{Gas Used}) \\times \\text{Base Fees} \\times \\left( \\frac{1}{0.125} \\times 1.125^N - 1 \\right)\n\nwhere N is the number of consecutive blocks.\nThe figure below estimates the average costs of block stuffing over the past three months (based on this Dune query), highlighting how market conditions and base fees influence them, and illustrating how stuffing multiple blocks in a row becomes exponentially more expensive for the attacker.\nDec 9 Screenshot from Attester-Includer Separation1986\u00d7586 66.9 KB\n\n\nMissing slot: Alternatively, the proposer can choose to skip block proposal for their assigned slot, causing them to forgo both consensus (issuance) and execution (MEV) layer rewards, which amount to approximately 0.04 ETH combined per block on average during the past three months.\nScreenshot 2024-12-18 at 18.21.351986\u00d7588 37.7 KB\n\n\nNote that in both cases above, increasing a transaction\u2019s priority fees makes it more costly for the proposer to exclude it.\n\nIL committee bribing:  Lastly, an obvious way to censor a transaction is to convince all IL proposers not to include it in their ILs. Under EIP-7805, IL proposers are not rewarded for including transactions in their ILs. In practice, convincing all IL proposers in a committee to act dishonestly and against the ethos of the Ethereum network might be difficult. With FOCIL, we only need one member of the committee to act honestly and include all transactions without censoring for the mechanism to work as expected. However, in theory, an attacker could offer a very small bribe to all 16 committee members to exclude a given transaction. If the IL proposers are rational, they might accept any bribe greater than zero.\n\nBy relying on the altruistic behavior of IL proposers, there is no way to control the cost they incur when censoring a transaction.\n\n\n\n\n\nBlock Stuffing\nMissing Slot\nIL committee bribing\n\n\n\n\nCost of censorship\n0.02 to 0.4 ETH\n\u223c 0.1 ETH\n> 0 ETH\n\n\n\nWat do?\nInclusion fees\nGiven the distinct properties and life cycles of state and inclusion transactions\u2014and the imbalance where users can tip the proposer but cannot affect the cost incurred by IL proposers when censoring transactions in protocol\u2014one option is introducing an independent inclusion fee (IF) and reward mechanism to increase inclusion guarantees and cost of censorship, while preserving the role of priority fees (PF) as proposer tips.\nThis approach allows users to craft their transactions based on network conditions (base fees) while controlling how much they are willing to pay for (1) Being executed on a specific state via PFs and (2) Increasing their inclusion guarantees via IFs, or both. In the diagram below, you can see how transactions are sent either privately to the block producer or to the public mempool, and are specifying both priority and inclusion fees. We assume that transactions are added to inclusion lists (ILs) and sorted in descending order based on inclusion fees (more on this in the next section). The block producer then orders the full payload\u2014by default according to priority fees or in any other order depending on MEV opportunities\u2014incorporating transactions from ILs and those they received privately.\nNov 21 Screenshot from Attester-Includer Separation1872\u00d7786 101 KB\nReward mechanism\nA simple way to distribute inclusion fees among IL proposers is to allocate them proportionally based on their contributions, rewarding only those who included the transactions in their ILs (i.e., conditional tips). This leads to greater incentives to include transactions no one else wants to include (e.g., \u201ccensorable transactions\u201d).\nIn the example above, IL Proposer 1 included all pending transactions from the mempool. Their rewards would thus be calculated as follows:\n\nTransaction g: Inclusion fee of 6 divided by 4 proposers = 6 \u2044 4 = 1.5\nTransaction d: Inclusion fee of 4 divided by 3 proposers = 4 \u2044 3 \u2248 1.333\nTransaction e: Inclusion fee of 1 divided by 3 proposers = 1 \u2044 3 \u2248 0.333\nTransaction f: Inclusion fee of 0 divided by 4 proposers = 0 \u2044 4 = 0\n\nAdding these up, IL Proposer 1 would receive approximately  1.5 + 1.333 + 0.333  + 0 = 3.166 in rewards for including these transactions in its IL.\nAlternative approaches to rewarding IL proposers include using issuance rather than fees,  weighting rewards based on past performance. It is also important that any such reward mechanism be independent of the existing Transaction Fee Mechanisms (TFMs)\u2014in other words, separate from both the base fee and the priority fee. Attempting to repurpose the base fee to reward IL proposers is not incentive-compatible because EIP-1559\u2019s economic design relies on burning the base fee to prevent block producers from manipulating transaction inclusion and inflating fees for personal gain. By ensuring that the base fee is never directly redistributed, the system maintains a balanced incentive structure. Similarly, relying on a model that redirects priority fees to IL proposers fails under network congestion, as block producers would then have a greater incentive to include non-IL transactions for higher direct rewards.\nRoles and participants\nIs it possible\u2014and desirable\u2014 to go further and separate protocol participants who are tasked with transaction inclusion (IL committee members) from those who focus on valuable state transactions (proposers)? Let\u2019s now imagine a post-FOCIL, post-APS (Attester-Proposer Separation) world. In this scenario, we still have IL proposers including transactions from the public mempool in their inclusion lists and being rewarded via inclusion fees. However, there\u2019s now a separation between attesters/the beacon proposer, the execution proposer and the builder (whether PBS is enshrined in the protocol or not is not relevant for this part of the discussion).\nHere\u2019s a quick overview of each participant\u2019s responsibilities:\nNov 19 Screenshot from Attester-Includer Separation1041\u00d7954 72.6 KB\n\n\nIL Committee Members:\n- Level of Sophistication: Low \u2013 IL proposers create Inclusion Lists (ILs) containing transactions pending in the public mempool.\n- Capital Requirements:\n- Medium \u2013 From 1 ETH to 2,048 ETH if/when maxEB and minEB are implemented.\n- Heavy \u2014 Capital is staked, locked upfront, and at risk of being slashed for other forms of misbehavior (e.g., proposing more than one distinct block at the same height), but not for failing IL-specific duties (e.g., IL equivocation) at least in the current version of FOCIL.\n\n\n\n\nBeacon Proposer:\n- Level of Sophistication: Low \u2013 Determines the head of the chain according to its local view and proposes the beacon block, which includes all consensus-related information such as the block header, attestations, withdrawals, deposits, and slashing penalties. Post-APS, beacon proposers are not incentivized to play timing games and can stay unsophisticated.\n- Capital Requirements:\n- Medium \u2013 From 1 ETH to 2,048 ETH if/when maxEB and minEB are implemented.\n- Heavy \u2014 Capital is staked, locked upfront, and put at risk of being slashed (e.g., for proposing more than one distinct block at the same height).\n\n\n\n\nAttesters:\n- Level of Sophistication: Low \u2013 Ensure that both the consensus and execution information contained in the full block are valid according to their views. They vote for the block if it passes all validity checks, such as being built on the correct head, containing valid transactions, and satisfying IL conditions.\n- Capital Requirements:\n- Medium \u2013 From 1 ETH to 2,048 ETH if/when maxEB and minEB are implemented.\n- Heavy \u2014 Capital is staked, locked upfront, and put at risk of being slashed (e.g., for attesting to different head blocks).\n\n\n\n\nExecution Proposer (Assuming the execution proposer does not outsource block production to a builder):\n- Level of Sophistication: High \u2013 Responsible for proposing a complete and valid execution payload to the network, with final authority over transaction inclusion and ordering, provided that the execution payload includes IL transactions. Must operate sophisticated infrastructure to implement complex strategies and efficiently extract MEV.\n- Capital Requirements:\n- High \u2013 Requires sufficient capital to secure execution proposing rights, to successfully execute advanced MEV strategies, such as non-atomic arbitrages and provide additional services like pre-confirmations.\n- Heavy - Execution proposers also need slashable capital to ensure they\u2019re disincentivized from misbehavior, but this requirement comes into play after they\u2019ve been selected (in contrast to attesters and beaucon proposers). At that point, they must gather the necessary capital to secure pre-confirmations, cover missed slot penalties, or pay their bids.\n\n\nInterestingly, by separating attesters from proposers, APS also effectively separates execution proposers from IL committee members. However, in the next section, we argue that IL committee members should be considered a separate class of participants. Because their responsibilities are limited in complexity and they are not directly involved in the system\u2019s economic security, they don\u2019t need to be subject to the same capital requirements as attesters.\nAttester-Includer Separation\nAttester Includer Separation2000\u00d71461 219 KB\nh/t Barnab\u00e9\nBuilding on the idea of unbundling roles to better align them with protocol duties, and drawing inspiration from tiered staking models like Rainbow Staking and validator selection mechanisms such as Orbit, we propose further separating attesters from IL committee members (includers).\nThinking from first principles, we would want both attesters and includers to be geographically decentralized and unsophisticated. However, there are some crucial differences between these sets of participants regarding capital requirements and the importance of their roles in securing the network:\n\nAttesters:\n\nSecurity: Overall, attesters play an extremely crucial role in securing the network by participating in consensus and ensuring liveness and finality. This critical role comes with some constraints. For example, it is important to avoid rotating attesters too quickly, as it might not be optimally secure from a consensus perspective. Similarly, we do not want validators to enter or exit the active set of attesters too rapidly, which is why we have withdrawal and deposit queues.\nCapital requirements: We want attesters to consolidate by maximizing the balance of a single validator (e.g., up to 2,048 ETH) instead of running multiple instances with lower balances (e.g., 32 ETH). This consolidation enables us to achieve high levels of economic security with a manageable number of participants, and facilitates moving towards faster finality (e.g., 3SF). Additionally, attesters must have at least some amount of ETH at stake to allow for slashing in cases where they do not fulfill their duties, whether intentionally or not. This means their capital needs to be staked and locked upfront (i.e., heavy).\n\n\nIL Committee Members:\n\nSecurity: IL committee members are not involved with consensus and don\u2019t play a role in securing the network. They are only tasked to improve censorship resistance by including transactions in ILs using their local view of the public mempool. Moreover, we only need one-out-of-n IL proposers to honestly build its IL for FOCIL to be effective and impose constraints on what transactions builders have to include in their blocks.\nCapital Requirements: Ideally, we want very low barriers to entry so that anyone wishing to contribute to Ethereum\u2019s censorship resistance can easily do so with 0.01 ETH for example, or just enough to ensure Sybil resistance and prevent participants to \u201cjust signing up\u201d but then being offline. The IL committee also rotates every slot, so there is potentially no need for queues or penalties other than missing rewards if an inclusion fee of sorts exists.\n\n\n\nThe question is: Do these differences justify moving towards two independent sets of participants, each fulfilling a specific duty, or should it remain the same set of participants?\nDec 9 Screenshot from Attester-Includer Separation (1)1526\u00d7992 66.6 KB\nWe argue that it does. By allowing anyone to join and contribute to Ethereum\u2019s censorship resistance as an \u201cincluder\u201d\u2014with minimal hardware requirements (e.g., a smartwatch), a simple, low-friction user experience (no queues), as well as light and minimal capital requirements\u2014and by rewarding them with an independent transaction fee mechanism (inclusion fees), the network can self-regulate based on the level of censorship. If many transactions are being censored, users can raise inclusion fees, thereby increasing the cost of censorship. As these higher fees get distributed among includers, more individuals will be incentivized to participate in creating inclusion lists (ILs), ultimately improving Ethereum\u2019s censorship resistance. Lastly, includers should also be able to participate in improving the network\u2019s censorship-resistant properties and uphold chain neutrality without publicly revealing their preferences via the specific transactions included in their lists. To this end, we can leverage anonymous ILs, using a combination of linkable ring signatures and anonymous broadcast protocols to protect their identities.\n", "\nSummary\nI propose a construction, based on Lamport\u2019s 99% fault tolerant consensus ideas, that I call timeliness detectors. Timeliness detectors allows online clients (ie. clients, aka users, that are connected to other clients with latency \\le \\delta) to detect, with guarantees of correctness and agreement, whether or not blocks were published \u201con time\u201d. In the event of a 51% attack, this allows at least the subset of clients that are online to come to agreement over (i) whether or not a \u201csufficiently bad\u201d 51% attack happened, and (ii) what is the \u201ccorrect\u201d chain to converge on and potentially even (iii) which validators to \u201cblame\u201d for the attack. This reduces the ability of 51% attacks to cause chaos and speeds up recovery time from an attack, as well as increasing the chance that a successful attack costs money.\nTimeliness detectors\nThe most basic construction for a timeliness detector is as follows. For every block that a client receives, the client maintains a \u201cis it timely?\u201d predicate, which states whether or not the client thinks the block was received \u201con time\u201d. The goal of this will be to try to distinguish the attacking chain from the \u201ccorrect\u201d chain in a 51% attack:\n\n51attack701\u00d7295 10.3 KB\n\nOur model will be simple: each block B has a self-declared timestamp t (in real protocols, the timestamp would often be implicit, eg. in the slot number). There is a commonly agreed synchrony bound \\delta. The simplest possible timeliness detector is: if you receive B before time t + \\delta, then you see the block as timely, and if you receive it after t + \\delta, then you do not. But this fails to have agreement:\n\nWe solve the problem as follows. For each block, we randomly select a sample of N \u201cattesters\u201d, v_1 ... v_n. Each attester follows the rule: if they see a block B with a timestamp t along with signatures from k attesters before time t + (2k+1)\\delta, they re-broadcast it along with their own signature. And the rule that a client follows is: if they see a block B with a timestamp t along with signatures from k attesters before time t + 2k\\delta, they accept it as timely. If they see B but it never satisfies this condition, they see B as not timely.\nLet us see what happens when even one client sees some block B as timely, though others may not see it as timely at first because of latency discrepancies. We will at first assume a single, honest, attester.\n\nThis diagram shows the basic principle behind what is going on. If a client sees a block before for deadline T, then (at least because they themselves can rebroadcast it) that block will get into the hands of an attester before the attester deadline T + \\delta, and the attester will add their signature, and they will rebroadcast it before time T + \\delta, guaranteeing that other nodes will see the block with the signature before time T + 2\\delta. The key mechanic is this ability for one additional signature to delay the deadline.\nNow, consider the case of n-1 dishonest attesters and one honest attester. If a client sees a timely block with k signatures, then there are two possibilities:\n\nOne of those k signatures is honest.\nNone of those k signatures are honest (so one attester who has not yet signed still remains)\n\nIn case (1), we know that the attester is honest, and so the attester broadcasted B with j \\le k signatures before time T + (2j-1)\\delta, which means that (by the synchrony assumption) every client saw that bundle before time T + 2j\\delta, so every client accepted B as current.\nIn case (2), we know that the honest attester will see the bundle before time T + (2k+1)\\delta, so they will rebroadcast it with their own signature, and every other client will see that expanded bundle before the k+1 signature deadline T + (2k+2)\\delta.\nSo now we have a \u201ctimeliness detector\u201d which a client can use to keep track of which blocks are on time and which blocks are not, and where all clients with latency \\le \\delta to attesters will agree on which blocks are timely.\nThe Simplest Blockchain Architecture\nCome up with any rule which determines who can propose and who attests to blocks at any slot. We can define a \u201c99% fault tolerant blockchain\u201d as follows: to determine the current state, just process all timely blocks in order of their self-declared timestamp.\nThis actually works (and provides resistance to both finality-reversion and censorship 51% attacks), and under its own assumptions gives a quite simple blockchain architecture! The only catch: it rests everything on the assumption that all clients will be online and the network will never be disrupted. Hence, for it to work safely, it would need to have a block time of perhaps a week or longer. This could actually be a reasonable architecture for an \u201cauxiliary chain\u201d that keeps track of validator deposits and withdrawals and slashings, for example, preventing long-run 51% attacks from censoring new validators coming in or censoring themselves getting slashed for misbehavior. But we don\u2019t want this architecture for the main chain that all the activity is happening on.\nA more reasonable alternative\nIn this post, however, we will focus on architectures that satisfy a somewhat weaker set of security assumptions: they are fine if either one of two assumptions is true: (i) network latency is low, including network latency between validators and clients, and (ii) the majority of validators is honest. First, let us get back to the model where we have a blockchain with some fork choice rule, instead of just discrete blocks. We will go through examples for our two favorite finality-bearing fork choice rules, (i) FFG and (ii) LMD GHOST.\nFor FFG, we extend the fork choice rule as follows. Start from the genesis, and whenever you see a block with two child chains which are both finalized, pick the chain with the lower-epoch timely finalized block. From there, proceed as before. In general, there will only ever be two conflicting finalized chains in two cases: (i) a 33% attack, and (ii) many nodes going offline (or censoring) leading to a long-running inactivity leak.\nCase (i):\n\nCase (ii), option 1 (offline minority finalizing later):\n\n51attack5726\u00d7291 9.95 KB\n\nCase (ii), option 2 (offline majority, later reappearing with finalized chain):\n\n51attack6761\u00d7291 10.2 KB\n\nHence, in all cases, we can prevent 51% attacks from breaking finality, at least past a certain point in time (T + 2k\\delta, the time bound after which if a client has not accepted a block as timely then we know that it will never accept it as timely). Note also that the above diagram is slightly misleading; what we care about is not the timelines of the finalized block, but rather the timeliness of a block that includes evidence that proves that the block is finalized.\nFor clients that are offline sometimes, this does not change anything as long as there is no 51% attack: if the chain is not under attack, then blocks in the canonical chain will be timely, and so finalized blocks will always be timely.\nThe main case where this may lead to added risk is the case of clients that have high latency but are unaware that they have high latency; they could see timely blocks as non-timely or non-timely blocks as timely. The goal of this mechanism is that if the non-timeliness-dependent fork choice and the timeliness-dependent fork choice disagree, the user should be notified of this, so they would socially verify what is going on; they should not be instructed to blindly accept the timeliness-dependent fork choice as canonical.\nDealing with censorship\nWe can also use timeliness detectors to automatically detect and block censorship. This is easy: if a block B with self-declared time t is timely, then any chain that does not include that block (either as an ancestor or as an uncle) before time t + (2k+2)\\delta is automatically ruled non-canonical. This ensures that a chain that censors blocks for longer than (2k+2)\\delta will automatically be rejected by clients.\n\nThe main benefit of using timeliness detectors here is that it creates consensus on when there is \u201ctoo much\u201d censorship, avoiding the risk of \u201cedge attacks\u201d that are deliberately designed to appear \u201csufficiently bad\u201d to some users but not others, thereby causing the community to waste time and energy with arguments about whether or not to fork away the censoring chain (instead, most users would in all cases agree on the correct course of action).\nNote that this requires an uncle inclusion mechanism, which eg. eth2 does not have. Additionally, it requires a mechanism by which transactions inside of uncles get executed, so that the censorship resistance extends to transactions and not just the raw bodies of blocks. This requires care to work well with stateless clients.\nOne additional nitpick is that care is required to handle the possibility of many blocks being published and gaining timeliness status and needing to be included as uncles at the same time. This could happen either due to delayed publication or due to a single proposer maliciously publishing many blocks in the same slot. The former can be dealt with via a modified rule that blocks must include either all timely blocks that are older than (2k+2)\\delta or the maximum allowed number (eg. 4) of uncles. The latter can be dealt with with a rule that if one block from a particular slot is included, all other blocks from that slot can be validly ignored.\nNote that in the Casper CBC framework, censorship prevention and de-prioritization of chains containing non-timely or censoring blocks by itself suffices to provide the same finality guarantees as we saw for the FFG framework above.\nChallenges / todos\n\n(Non-technical) Come up with the best way to explain to users what happened in the event that timeliness-aware and non-timeliness-aware fork choice rules disagree, and how they should respond to the situation.\nAnalyze the behavior of the system in cases where latency is sometimes above \\delta, or latency is always potentially above \\delta but we have assumptions that eg. some fixed fraction of attesters is honest, or other hybrid assumptions. See if there are ways to modify the rules to improve performance in those scenarios.\nAnalyze ways to achieve these properties without including a new class of attestation; instead, reuse existing attestations (eg. the attestations that validators make every epoch in FFG)\nDetermine if there are small modifications to \u201csimple\u201d longest-chain-based fork choice rules that allow them to benefit from timeliness detectors to gain a kind of finality.\n\n", "\nProposal: Delay stateRoot Reference to Increase Throughput and Reduce Latency\nBy: Charlie Noyes, Max Resnick\nIntroduction\nRight now, each block header includes a stateRoot that represents the state after executing all transactions within that block. This design requires block builders and intermediaries (like MEV-Boost relays) to compute the stateRoot, which is computationally intensive and adds significant latency during block production.\nThis proposal suggests modifying the Ethereum block structure so that the stateRoot in block n references the state at the beginning of the block (i.e., after executing the transactions in block n - 1, rather than the state at the end of the block).\nBy delaying the stateRoot reference by one block, we aim to remove the stateRoot calculation from the critical path of block verification at the chain tip, thereby reducing L1 latency and freeing up capacity to increase L1 throughput.\nTechnical Specification (High-Level)\nWhen validating block n, nodes ensure that the stateRoot matches the state resulting from executing block n-1 (i.e., the pre-state root of block n).\nTo be clear, there is no change to exeuction ordering. Transactions in block n are still applied to the state resulting from block n-1.\nMotivation\nstateRoot calculation and verification is unnecessary work on the critical path of block production. A builder cannot propose a block on MEV boost without first calculating the stateRoot and the attestation committee cannot verify a block without computing the stateRoot to compare with the proposed stateRoot. stateRoot calculation itself accounts for approximately half of time spent by all consensus participants working at the tip. Moreover, whatever latency implications the stateRoot calculation imposes are paid twice on the critical path: once at the block building stage and then again during verification.\n\n\n\nWhen block builders submit blocks to relays, they are required to provide the calculated stateRoot. From surveying three of the four largest builders, each spends on average only 40%-50% of their time actually building each block, and the rest on stateRoot calculation.\n\n\n\n\nWhen MEV-Boost relays recieve blocks from builders, they are supposed to verify their correctness. In Flashbots\u2019 relay, also approximately half of the ~100ms (p90) verification time is spent on the stateRoot calculation.\n\n\n\n\nWhen validators receive a new block, or when non-MEV-Boost validators (\u201chome builders\u201d) produce a block, they are also required to re-verify its execution and its stateRoot. Commodity hardware Reth nodes spend approximately 70% of its time in live-sync on the stateRoot (remainder on execution).\n\n\n\nRETH benchmarks2130\u00d71118 161 KB\n  ~70% of RETH's block processing time is spent on `stateRoot` calculation.\n\nThese participants - builders, relays, and validators - are highly latency sensitive. They operate under tight timing constraints around slot boundaries (particularly with the incresaing prevalence of timing games).\nThe latency introduced by stateRoot verification at the tip is unnecessary and removing it could allow us to improve the health of the block production pipeline, and network stability.\nBenefits of Delaying the stateRoot\n\nHigher L1 throughput, because the time currently spent verifying the stateRoot can be re-allocated to execution. stateRoot verification would be pipelined to occur in parallel with the next slot (i.e. during time that nodes are currently idle). Bandwidth requirement increases and state growth would also need to be acceptable before activating a throughput increase.\nTime saved by pipelining the stateRoot could also be allocated towards lowering slot times - improving L1 Ethereum UX, and likely resulting in tighter spreads for users of decentralized exchanges.\nBuilders and relays avoid an unnecessary latency speedbump. Both are highly latency-sensitive actors. We want to minimize the sophistication it takes to be a relay or validator. Removing stateRoot latency from the critical path of block verification means they will no longer have to worry about optimizing it, improving the health and efficiency of the block production pipeline.\n\nPotential Downsides and Concerns\nImpacted Applications\n\nLight Clients and SPV Clients\n\n\nImpact: These clients rely on the latest stateRoot to verify transactions and account balances without downloading the entire blockchain. A one-block delay introduces a latency in accessing up-to-date state information. Cross-chain communication protocols (like bridges that utilize light clients) would also experience this delay.\n\n\nConsideration: We do not see an obvious issue with light clients being delayed by a single block.\n\n\n\n\nStateless Client Protocols\n\n\nImpact: Stateless clients rely on the latest stateRoot to verify transaction witnesses. A one-block delay could affect immediate transaction validation.\n\n\nConsideration: If these clients can tolerate a one-block delay, the impact may be minimal. This aligns with ongoing discussions in the statelessness roadmap.\n\n\n\nRationale\nWhy This Approach?\n\nEfficiency: Removing stateRoot computation from the critical path significantly reduces block verification time.\nSimplicity: The change is straightforward in terms of protocol modification, affecting only the placement of the stateRoot reference. This is backwards-compatible with the existing block production pipeline (i.e., native building and MEV-Boost). Other proposals which include execution pipelining, like ePBS, are significantly broader in scope and complexity. Delaying the stateRoot is a simpler change we can make with immediate benefit and little risk.\nMinimal Disruption: While some applications may be affected, we think most (all?) can tolerate a one-block delay without significant issues. We should collect feedback from application developers to validate this.\n\nBackwards Compatibility and Transition\n\nHard Fork Requirement: This change is not backwards compatible and would require a network hard fork.\nApplication Adaptations: Affected applications (light clients, Layer 2 solutions, stateless clients) may need to adjust their protocols or implementations.\n\nRequest for Feedback\nWe invite the community to provide feedback on this proposal, particularly:\n\nFeasibility: Are there technical challenges that might impede the implementation of this change?\nUpside: How much throughput will we be able to eke out from pipelining stateRoot calculation, and reallocating the time to exeuction?\nAffected Applications: We don\u2019t obviously see a class of widely used applications which would be affected. We hope any developers whose applications do depend on same-block stateRoot will let us know.\n\nNext Steps\nWe plan to formalize this proposal into an EIP for potential inclusion in Pectra B.\nAcknowledgements\nThanks to Dan Robinson, Frankie, Robert Miller, and Roman Krasiuk for feedback and input on this proposal.\n", "\nSo you wanna Post-Quantum Ethereum transaction signature\nThanks to Vitalik Buterin, Justin Drake, Renaud Dubois, Marius Van Der Wijden and Zhenfei Zhang for fruitfull discussions.\nIntroduction\n2024 will probably be remembered as one of the years marking the acceleration of the quantum computer menace. Google, under its CEO Sundar Pichai, finally unveiled its quantum chip, Willow, via a loud tweet!\nScott Aaronson, one of the most famous quantum experts in the world, has changed his message to people asking whether they should be worried about quantum computers. He shifted from saying\n\n\u2026 Maybe, eventually, someone will need to start thinking about migrating from RSA, Diffie-Hellman, and elliptic curve cryptography to lattice-based crypto or other systems that could plausibly withstand quantum attacks,\u2026\n\nto\n\nYes, unequivocally, worry about this now. Have a plan.\u2019\n\nVitalik has already written about how to hard-fork to save most users\u2019 funds in a quantum emergency. Also, few days ago, he highlighted in a podcast the four main Ethereum components potentially vulnerable to quantum attacks. They are:\n\nEthereum transaction signatures (notably using ECDSA)\nBLS signatures in consensus\nData Availability Sampling (leveraging KZG commitments)\nVerkle trees (if shipped with Bandersnatch)\n\nAn attentive reader might have noticed that these four points have something in common\u2014yes, it\u2019s my beloved elliptic curves. Unfortunately, the discrete logarithm problem for elliptic curves (ECDLP) is broken by Shor\u2019s Algorithm, a famous quantum algorithm.\nIn this short note, we are going to analyze a possible post-quantum replacement for the first point, namely a potential post-quantum Ethereum transaction signature.\nWhich PQ signature?\nNow, a legitimate question is: which post-quantum (PQ) signatures should we use? Fortunately, we don\u2019t need to overthink this too much if we had to choose right now. Zhenfei Zhang, a former Ethereum Foundation cryptographer, has already written about the NIST Post-Quantum Cryptography Standardization Process. If we analyze the three possible signature choices (two of which leverage lattice-based cryptography), it\u2019s clear (at least for now) that Falcon appears to be the most promising candidate. The computation for the verifier should be roughly the same as other lattice-based signature schemes (like Dilithium), i.e., bounded by an FFT. However, Falcon does have a smaller signature size.\nShip it!!!\nNow that we\u2019ve \u2018settled\u2019 on the signature to use, the next question is: how are we going to ship it?  There is a big dichotomy now: one implies a hard fork, and the other doesn\u2019t. Let\u2019s dig a bit deeper.\nThe Account Abstraction way\nThe first approach we will discuss, arguably the most elegant and promising, involves Account Abstraction (AA). It has been advocated by Justin Drake and Vitalik on various occasions.\nFor people not familiar with it, AA is a proposed improvement to make the Ethereum ecosystem more flexible and user-friendly by changing how transactions and accounts are managed. It shifts certain functionalities traditionally reserved for externally owned accounts (EOAs) into smart contracts, effectively \u201cabstracting\u201d the differences between EOAs and smart contract accounts.\nEthereum developers have introduced various proposals for implementing AA, including ERC-4337. This is a practical solution that achieves AA without requiring a consensus-layer upgrade. It uses a mechanism called User Operation objects and introduces a separate Bundler layer to handle transactions.\nAdding Falcon as the Ethereum transaction signature in this scenario means coding a Falcon verifier contract that is responsible for verifying the validity of User Operation objects before they are executed by the Entry Point contract.\nNow, this may sound like all sunshine and rainbows, but there is at least one substantial underlying issue. Coding Falcon in Solidity might not be the best experience (and it\u2019s probably quite gas-costly). On top of that, there are even nastier problems, such as the fact that Falcon deals with 13-bit numbers, while Solidity only supports U256. The latter is the kind of issue that could be addressed by adding SIMD and EVMMAX to the EVM.\n\nPros: It is an elegant and flexible solution.\nCons: It is costly in terms of gas consumption.\n\nThe hard fork way\nThe method we discuss here is probably the simplest technically. It is inspired by previous work done by Marius Van Der Wijden and essentially involves introducing a new transaction type signed with Falcon signatures instead of BLS signatures. The biggest problem here is that, by doing so, we are tightly bound (through a new EIP) to a favored master signature scheme.\nSo, to recap this approach\n\nPros:  Easy to code and fast.\nCons: Not future-proof.\n\nHybrid\nA really tempting approach would be to take the best of the two methods above and combine them into a single one. In a nutshell, we could leverage AA in a similar way that RIP-7212 does, but of course, we would need a new RIP for Falcon. This might provide the time to experiment with the feature in rollups and determine if Falcon is truly the way to go. However, it is important to note that this approach does not solve the original problem of introducing a new signature scheme at the L1 level.\n\nPros: Easy to code and fast.\nCons: Temporary (does not solve the L1 use case).\n\nConclusion\nThe rise of quantum computing demands urgent action to secure Ethereum, particularly its transaction signatures vulnerable to Shor\u2019s Algorithm. Falcon, a lattice-based signature scheme, emerges as a strong candidate due to its efficiency and compact size. Deployment strategies, including Account Abstraction, hard forks, or a hybrid approach, each offer distinct benefits and trade-offs. A careful evaluation is essential to ensure Ethereum remains robust against quantum threats while maintaining scalability and usability.\n", "\nAbstract\nThis research paper presents a novel decentralized payment network model that leverages zero-knowledge proofs (ZKPs) to ensure transaction validity and balance consistency without relying on validators or traditional consensus mechanisms. The network features a fixed token supply, airdropped to participants at inception, eliminating the need for mining and associated costs. The core design focuses on direct mutual verification between sender and receiver, with an extensive exploration of the underlying mathematical foundations, formal proofs, algorithms, and data structures underpinning this system.\nThe proposed payment network aims to address key challenges faced by existing decentralized payment systems, such as high transaction costs, scalability limitations, and privacy concerns. By employing ZKPs and a unilateral payment channel architecture, the network enables efficient, secure, and privacy-preserving transactions without the need for intermediaries or complex consensus protocols. The paper provides a comprehensive analysis of the system\u2019s security, privacy, and scalability properties, along with detailed comparisons to alternative approaches. The underlying mathematical framework and formal proofs are rigorously defined, ensuring the robustness and correctness of the proposed model.\nIntroduction\nDecentralized payment systems have garnered significant attention due to their potential to provide secure, transparent, and efficient financial transactions without intermediaries. However, existing solutions often face challenges related to high transaction costs, scalability limitations, and privacy concerns. This research introduces a novel decentralized payment network that leverages zero-knowledge proofs (ZKPs) and unilateral payment channels to address these issues.\nThe proposed network architecture is designed to address specific challenges faced by existing decentralized payment systems:\n\nThe absence of mining and associated costs solves the issue of high transaction fees and energy consumption in traditional proof-of-work-based systems.\nThe elimination of validators and consensus mechanisms tackles the scalability limitations and potential centralization risks in proof-of-stake and delegated systems.\nThe use of storage partitioning and off-chain payment channels addresses the scalability and privacy concerns associated with storing all transactions on-chain.\n\nBy distributing a fixed token supply to participants at the network\u2019s inception, the system eliminates the need for mining and its associated costs. The network focuses on enabling direct mutual verification of transactions between senders and receivers, ensuring the validity of transactions and the consistency of account balances without relying on validators or consensus mechanisms. By leveraging zk-SNARKs, the network allows for direct proof of validity between sender and receiver, as the succinct zero-knowledge proofs inherently prove the correctness of transactions.\nTo enhance efficiency and scalability, the network uses a multi-tier Merkle tree system with Merkle proofs, ensuring that only a constant succinct size (O(1)) of data is submitted to the blockchain. This design minimizes on-chain storage requirements and ensures data availability.\nAt the core of this novel payment network lies a comprehensive mathematical framework that leverages zero-knowledge proofs, particularly zk-SNARKs, to validate transactions and generate wallet state proofs. These proofs enable efficient verification of transaction validity and balance updates while preserving user privacy.\nThe network\u2019s architecture is composed of several key components, including unilateral payment channels, hierarchical smart contracts, and partitioned storage nodes. These components work together to enable scalable, secure, and privacy-preserving transactions, while minimizing on-chain storage requirements and ensuring data availability.\nTo ensure the robustness and correctness of the proposed model, the paper presents formal algorithms, theorems, and proofs for crucial aspects of the system, such as the Balance Consistency Theorem and the dispute resolution mechanism. These mathematical formalisms provide a solid foundation for the security and reliability of the payment network.\nFurthermore, the paper includes an in-depth analysis of the network\u2019s security, privacy, and scalability properties, highlighting its advantages over alternative approaches, such as traditional blockchain-based payment systems and centralized payment networks. The analysis also acknowledges potential limitations and challenges, such as the complexity of zk-SNARK implementations and the need for ongoing optimizations.\nThe main contributions of this research can be summarized as follows:\n\nA comprehensive mathematical framework for ensuring transaction validity and balance consistency using zero-knowledge proofs, particularly zk-SNARKs.\nA detailed description of the network\u2019s architecture, including unilateral payment channels, hierarchical smart contracts, and partitioned storage nodes.\nFormal algorithms, theorems, and proofs for key components of the system, such as the Balance Consistency Theorem, zk-SNARK proof generation, smart contract verification, and dispute resolution.\nAn in-depth analysis of the network\u2019s security, privacy, and scalability properties, along with detailed comparisons to alternative approaches.\nAn exploration of promising use cases that leverage the enhanced privacy features of the proposed system.\n\nThe proposed decentralized payment network presents a promising approach to enabling secure, private, and scalable transactions in a decentralized setting, paving the way for more efficient and accessible financial services on the blockchain. The extensive mathematical formalism and rigorous analysis provided in this paper contribute to the growing body of research on decentralized payment systems and demonstrate the potential of zero-knowledge proofs in enhancing the security, privacy, and scalability of blockchain-based financial applications.\nDALL\u00b7E_2024-06-05_20.49.33_-_A_high-level_overview_of_a_decentralized_payment_network_architecture._The_diagram_includes_Off-Cha1117\u00d7970 218 KB\nBackground\nThis section introduces the key concepts and technologies used in the proposed decentralized payment network, providing a solid foundation for understanding the system\u2019s design and functionality.\nZero-Knowledge Proofs (ZKPs)\nZero-knowledge proofs (ZKPs) are cryptographic protocols that enable one party (the prover) to prove to another party (the verifier) that a statement is true without revealing any information beyond the validity of the statement itself. ZKPs have numerous applications in blockchain technology, particularly in privacy-preserving transactions and scalable off-chain solutions.\nOne prominent type of ZKP is zk-SNARKs (Zero-Knowledge Succinct Non-Interactive Arguments of Knowledge). zk-SNARKs enable the generation of succinct proofs that can be verified quickly, making them well-suited for blockchain applications where proofs need to be stored on-chain and verified by multiple parties.\nDefinition: A zero-knowledge proof for a statement S is a protocol between a prover P and a verifier V such that:\n\nCompleteness: If S is true, V will be convinced by P with high probability.\nSoundness: If S is false, V will not be convinced by P except with negligible probability.\nZero-Knowledge: If S is true, V learns nothing beyond the fact that S is true.\n\nIn the proposed decentralized payment network, zk-SNARKs are employed to prove the validity of transactions and generate wallet state proofs, ensuring the privacy and security of user balances while enabling efficient verification.\nSystem Architecture\nThe proposed decentralized payment network consists of several key components: off-chain payment channels, hierarchical smart contracts, and partitioned storage nodes. This section provides a detailed description of each component and their interactions within the overall system.\nCertainly! Here is the document reformatted in Markdown with low-level LaTeX math included:\nUnilateral Payment Channels\nPayment channels are a key component of scalable blockchain solutions, enabling off-chain transactions between parties without the need to record every transaction on the main blockchain. In the proposed network, each user has a unilateral payment channel associated with their wallet contract, which holds their tokens off-chain. This design choice simplifies channel management and enables cross-partition transactions.\nDefinition: A unilateral payment channel between a user U and their wallet contract W is a tuple (B, T_1, T_2, \\ldots, T_n), where:\n\nB is the initial balance of U in the payment channel.\nT_1, T_2, \\ldots, T_n are the transactions executed within the payment channel.\n\nThe final state of the payment channel is determined by the cumulative effect of all transactions T_1, T_2, \\ldots, T_n on the initial balance B.\nTo set up a unilateral payment channel, a user creates a wallet contract on the blockchain and transfers the desired amount of tokens to the contract. The wallet contract manages the user\u2019s off-chain balance and state updates through zk-SNARK proofs. When a user wants to transfer tokens to another user, they generate a zk-SNARK proof that verifies the validity of the transaction and includes the necessary metadata for the recipient to generate the next transaction proof. This design enables instant transaction finality and eliminates the need for on-chain confirmation.\nExample 1: Unilateral Payment Channel Setup and Transactions\nSuppose Alice wants to set up a unilateral payment channel with an initial balance of 100 tokens. She creates a wallet contract W_A on the blockchain and transfers 100 tokens to it. The wallet contract initializes Alice\u2019s off-chain balance to 100 tokens.\nLater, Alice decides to send 30 tokens to Bob. She generates a zk-SNARK proof \\pi_1 that verifies the validity of the transaction, including the availability of sufficient funds and the correctness of the updated balances. Upon generating the proof \\pi_1, the wallet contract immediately locks 30 tokens, reducing Alice\u2019s available balance to 70 tokens. Alice sends the transaction details and the proof \\pi_1 to Bob.\nBob verifies the proof \\pi_1 to ensure the transaction\u2019s validity. If the proof is valid, Alice and Bob update their local off-chain balances accordingly. Alice\u2019s balance remains 70 tokens, while Bob\u2019s balance increases by 30 tokens. Both parties sign the proof \\pi_1 to authorize the future rebalancing of their respective payment channels.\nIf Bob does not accept the proof within a specified timeout period, the smart contract automatically releases the locked funds back to Alice\u2019s available balance, ensuring no funds are indefinitely locked.\nThis example demonstrates how unilateral payment channels enable secure, off-chain transactions between users while preserving privacy and scalability.\nOff-Chain Payment Channel Operations\nAs introduced in Section 2, off-chain payment channels form the foundation of the proposed network\u2019s scalability. Each user has a unilateral payment channel associated with their wallet contract, which holds their tokens off-chain. The channel setup and transaction process can be formally described as follows:\nChannel Setup\n\\begin{algorithm}[H]\n\\caption{Unilateral Payment Channel Setup}\n\\begin{algorithmic}[1]\n\\Procedure{SetupChannel}{$U, W, B$}\n\\State $U$ creates a wallet contract $W$ on the blockchain\n\\State $U$ transfers $B$ tokens to $W$\n\\State $W$ initializes the off-chain balance of $U$ to $B$\n\\State \\textbf{return} $W$\n\\EndProcedure\n\\end{algorithmic}\n\\end{algorithm}\n\nHere, U represents the user, W is the wallet contract, and B is the initial balance in the payment channel.\nOff-Chain Transactions\n\\begin{algorithm}[H]\n\\caption{Off-Chain Transaction}\n\\begin{algorithmic}[1]\n\\Procedure{OffchainTransaction}{$S, R, T$}\n\\State $S$ generates a zk-SNARK proof $\\pi$ for the transaction $T$\n\\State $S$'s wallet contract locks the transaction amount\n\\State $S$ sends $(T, \\pi)$ to $R$\n\\State $R$ verifies $\\pi$ to ensure the validity of $T$\n\\If{$\\pi$ is valid}\n\\State $S$ updates their local off-chain balance\n\\State $R$ updates their local off-chain balance\n\\State $S$ and $R$ sign $\\pi$ to authorize rebalancing\n\\Else\n\\State $S$'s wallet contract releases the locked amount after timeout\n\\EndIf\n\\State \\textbf{return} $(T, \\pi)$\n\\EndProcedure\n\\end{algorithmic}\n\\end{algorithm}\n\nHere, S represents the sender, R is the receiver, and T is the transaction. The zk-SNARK proof \\pi verifies the validity of the transaction, including the availability of sufficient funds and the correctness of the updated balances. If the proof is valid, both parties update their local off-chain balances and sign the proof to authorize the future rebalancing of their payment channels. If the proof is not accepted within a specified timeout period, the smart contract automatically releases the locked funds back to the sender\u2019s available balance.\nHierarchical Smart Contracts\nThe hierarchical smart contract structure is a key component of the proposed network, enabling efficient rebalancing of payment channels and management of cross-partition transactions. The structure consists of three layers: root contracts, intermediate contracts, and wallet contracts.\n\n\nRoot Contracts:\nResponsibilities:\n\nServe as the entry point for users and maintain a mapping of intermediate contracts.\nAggregate Merkle roots from intermediate contracts and submit a single, final aggregated Merkle root.\nSubmit the final Merkle root to the blockchain at regular intervals, ensuring the global state is verifiable on-chain with minimal frequency and cost being a constant size O(1).\n\n\n\nIntermediate Contracts:\nResponsibilities:\n\nManage liquidity pools for specific transaction types or user groups.\nMaintain a mapping of wallet contracts and are responsible for rebalancing payment channels based on the transaction proofs submitted by users.\nCollect Merkle roots from wallet contracts and aggregate them into a single Merkle tree within their partition.\nPeriodically submit the aggregated Merkle root to the root contract.\nEnsure the state within their partition is verifiable on-chain with minimal frequency and cost.\n\n\n\nWallet Contracts:\nResponsibilities:\n\nRepresent individual user payment channels and hold the users\u2019 off-chain balances.\nGenerate zk-SNARK proofs for their state and submit these proofs to the storage nodes.\nStore proofs to the storage nodes for data availability.\n\n\n\nThe hierarchical structure allows for efficient liquidity management and reduced on-chain transaction costs, as rebalancing operations are performed at the intermediate level, and the root contracts only need to process periodic updates.\nExample 3: Hierarchical Smart Contract Interaction\nContinuing with the previous examples, suppose Alice, Bob, Carol, and David belong to the same user group managed by an intermediate contract IC_1. The intermediate contract IC_1 is mapped to a root contract RC.\n\n\nWhen Alice sends 30 tokens to Bob (transaction T_1), she generates a zk-SNARK proof \\pi_1. Upon generating the proof \\pi_1, Alice\u2019s wallet contract immediately locks 30 tokens, reducing her available balance accordingly. The transaction proof \\pi_1 is then submitted to the intermediate contract IC_1. The intermediate contract verifies the proof and updates the balances of Alice\u2019s and Bob\u2019s wallet contracts accordingly.\n\n\nSimilarly, when Alice receives 50 tokens from Carol (transaction T_2), Carol generates a zk-SNARK proof \\pi_2. Upon generating the proof \\pi_2, Carol\u2019s wallet contract immediately locks 50 tokens, reducing her available balance. The transaction proof \\pi_2 is then submitted to IC_1, which verifies the proof and updates the balances of Alice\u2019s and Carol\u2019s wallet contracts.\n\n\nPeriodically, the intermediate contract IC_1 submits a summary of the balance updates to the root contract RC, which maintains a global view of the network\u2019s state by submitting a single aggregated Merkle root to the blockchain.\n\n\nThis hierarchical structure, with the immediate balance locking mechanism, ensures that all transactions are secure and funds are not double spent, even if there are delays in transaction acceptance or verification.\nStorage Nodes and Blockchain Interaction\nTo ensure data availability and scalability, the proposed network employs storage nodes that store the off-chain transaction history and wallet state proofs. Each storage node maintains a copy of the entire off-chain data, ensuring redundancy and decentralization.\nStorage Node Operations:\n\nStoring Proofs: Storage nodes store zk-SNARK proofs for individual wallet states. Each wallet maintains its own Merkle tree that includes these proofs.\nAggregating Data: At regular intervals, storage nodes aggregate the off-chain data into a single Merkle root, representing the state of all payment channels they manage. This Merkle root is then submitted to the intermediate contracts.\n\ndef store_proof(proof, user, wallet):\n    # Store the proof for user and wallet contract\n    # Update the local Merkle tree with the proof\n\ndef submit_merkle_root():\n    # Generate the Merkle root for all stored proofs\n    # Submit the Merkle root to the intermediate contract\n\nThe blockchain acts as a secure and immutable ledger, storing the Merkle roots submitted by the root contract. This allows for efficient\nverification of the network\u2019s global state, as any discrepancies between the off-chain data and the on-chain Merkle roots can be easily detected and resolved.\nThis hierarchical structure enables efficient verification of individual payment channels and the entire network state without storing the full transaction history on-chain. By leveraging the security and immutability of the blockchain while keeping the majority of the data off-chain, the proposed network achieves a balance between scalability, data availability, and security.\nExample 4: Storage Node Operation and Blockchain Interaction\nFollowing the previous examples, suppose storage node SN_1 is responsible for storing the transaction proofs and wallet state proofs for Alice, Bob, Carol, and David.\n\nWhen Alice generates a wallet state proof \\pi_s after transactions T_1, T_2, and T_3, she submits the proof to the storage node SN_1. The storage node stores the proof and updates its local Merkle tree with the new proof.\nSimilarly, when Bob, Carol, and David generate their wallet state proofs, they submit them to SN_1, which stores the proofs and updates its local Merkle tree accordingly.\nAt the end of each epoch, SN_1 generates a Merkle root R that represents the state of all payment channels it manages. The storage node then submits the Merkle root R to the intermediate contract, providing a compact and tamper-evident snapshot of the network\u2019s state.\nThe intermediate contract aggregates the Merkle roots from all storage nodes within its partition and submits a single final Merkle root to the root contract.\nThe root contract aggregates the Merkle roots from all intermediate contracts and submits a single final Merkle root to the blockchain.\nThe blockchain stores the submitted Merkle root, allowing for efficient verification of the network\u2019s global state. If any discrepancies arise between the off-chain data and the on-chain Merkle roots, they can be easily detected and resolved using the dispute resolution mechanism described in the following section.\n\nThis hierarchical structure, combined with immediate balance locking and zk-SNARK proofs, ensures secure, efficient, and scalable off-chain transactions, maintaining the integrity and security of the overall network.\nTransaction Validity and Balance Consistency\nTo ensure the validity of transactions and the consistency of account balances, the proposed payment network employs a combination of zero-knowledge proofs and formal mathematical proofs. This section presents the core theorems and algorithms that underpin the system\u2019s security and correctness. (as stated in the abstract, we have a fixed supply released in full at genesis)\nTransaction Validity\nEach transaction in the proposed network is accompanied by a zk-SNARK proof that verifies the following conditions:\n\nThe sender has sufficient balance to cover the transaction amount.\nThe sender\u2019s updated balance is correctly computed.\nThe receiver\u2019s updated balance is correctly computed.\n\nLet T_i be a transaction in which sender S transfers \\Delta_i tokens to receiver R. The accompanying zk-SNARK proof \\pi_i ensures the following conditions:\n\n\\begin{align*}\nB_S &\\geq \\Delta_i \\\\\nB'_S &= B_S - \\Delta_i \\\\\nB'_R &= B_R + \\Delta_i\n\\end{align*}\n\nwhere B_S and B_R are the initial balances of S and R, respectively, and B'_S and B'_R are the updated balances after the transaction.\nBalance Consistency\nTo prove the consistency of account balances in the presence of valid transactions, we present the following theorem:\nTheorem (Balance Consistency): Given a series of valid transactions T_1, T_2, \\ldots, T_n between two parties S and R, the final balances B'_S and B'_R satisfy:\n\nB'_S + B'_R = B_S + B_R\n\nwhere B_S and B_R are the initial balances of S and R, respectively.\nProof: We prove the theorem by induction on the number of transactions n.\nBase case: For n = 1, we have a single transaction T_1 with amount \\Delta_1. The updated balances after the transaction are:\n\n\\begin{align*}\nB'_S &= B_S - \\Delta_1 \\\\\nB'_R &= B_R + \\Delta_1\n\\end{align*}\n\nAdding the above equations yields:\n\nB'_S + B'_R = B_S + B_R\n\nInductive step: Assume the theorem holds for n = k transactions. We prove that it also holds for n = k + 1 transactions.\nLet B^{(k)}_S and B^{(k)}_R be the balances after the first k transactions. By the induction hypothesis, we have:\n\nB^{(k)}_S + B^{(k)}_R = B_S + B_R\n\nNow, consider the (k+1)-th transaction T_{k+1} with amount \\Delta_{k+1}. The updated balances after this transaction are:\n\n\\begin{align*}\nB^{(k+1)}_S &= B^{(k)}_S - \\Delta_{k+1} \\\\\nB^{(k+1)}_R &= B^{(k)}_R + \\Delta_{k+1}\n\\end{align*}\n\nAdding the above equations and substituting the induction hypothesis yields:\n\n\\begin{align*}\nB^{(k+1)}_S + B^{(k+1)}_R &= (B^{(k)}_S - \\Delta_{k+1}) + (B^{(k)}_R + \\Delta_{k+1}) \\\\\n&= B^{(k)}_S + B^{(k)}_R \\\\\n&= B_S + B_R\n\\end{align*}\n\nTherefore, the theorem holds for n = k + 1 transactions.\nBy the principle of mathematical induction, the theorem holds for any number of valid transactions\nn \\geq 1. \\blacksquare\nThe Balance Consistency theorem ensures that the total balance of the system remains constant throughout a series of valid transactions, providing a fundamental property for the security and correctness of the proposed payment network.\nFraud Prevention Mechanisms\nThe proposed decentralized payment network integrates multiple layers of fraud prevention mechanisms through its hierarchical smart contract system and the use of zk-SNARKs. These measures ensure the integrity and consistency of transaction states, inherently preventing the submission of outdated or fraudulent states. This section outlines how these mechanisms work in detail.\nZK-SNARK Proofs and State Updates\nThe network leverages zk-SNARKs to validate each transaction. The key elements include:\n\n\nProof of Validity: Each transaction within the network must be accompanied by a zk-SNARK proof. This proof verifies several critical aspects:\n\nThe sender has sufficient balance to cover the transaction.\nThe sender\u2019s updated balance after the transaction is correctly computed.\nThe receiver\u2019s updated balance after the transaction is correctly computed.\n\n\n\nConsistent State Management: Each user\u2019s wallet contract maintains a Merkle tree of state proofs. Each state update (i.e., each transaction) is validated through zk-SNARKs, ensuring it is consistent with the previously recorded state. This cryptographic validation prevents unauthorized or incorrect state changes.\n\n\nPrevention of Old State Submission\nThe design of the proposed network inherently prevents the submission of outdated or fraudulent states through the following mechanisms:\n\n\nProof Consistency: Each zk-SNARK proof submitted for a state update must be consistent with the latest recorded state. Intermediate contracts aggregate data from wallet contracts, and root contracts submit these aggregated roots to the blockchain. Any attempt to submit an old state would be detected as it would not match the current aggregated Merkle root.\n\n\nOn-Chain Verification: The final aggregated Merkle root submitted by the root contract is stored on the blockchain, providing a tamper-evident record of the global state. During dispute resolution, the submitted state proofs are verified against this on-chain Merkle root to ensure only the most recent valid state is considered.\n\n\nMitigated Need for Watchtowers\nDue to the robust fraud prevention mechanisms built into the proposed system, the traditional need for watchtowers entities that monitor the blockchain for malicious activities and act on behalf of users is significantly reduced. The hierarchical structure and the use of zk-SNARKs ensure that:\n\nEach state update is cryptographically verified, preventing unauthorized changes.\nThe aggregated Merkle roots provide a consistent and tamper-evident record of the network\u2019s state.\nDispute resolution is handled efficiently and fairly based on the most recent valid state proofs.\n\nThe comprehensive fraud prevention mechanisms of the proposed decentralized payment network ensure high levels of security and integrity without the need for external monitoring entities like watchtowers. The hierarchical smart contract system and zk-SNARKs work together to maintain consistent and verifiable transaction states, providing a secure and efficient framework for decentralized financial transactions.\nRole of the DAO\nWhile the built-in mechanisms provide robust security and minimize the need for watchtowers, there are scenarios where manual involvement might be necessary. To address these situations, a Decentralized Autonomous Organization (DAO) can be implemented to manage and oversee the network\u2019s operations. The DAO would play a crucial role in:\n\nHandling Exceptional Cases: Situations that require manual intervention beyond the automated fraud prevention and dispute resolution mechanisms.\nBalancing Automation and Trust: Ensuring the right mix of automated processes, cryptographic proofs, and trust mechanisms to maintain network integrity.\nDemocratic Decision-Making: Leveraging community governance to make decisions on critical issues, such as protocol upgrades, handling disputes that the automated system cannot resolve, and other governance matters.\n\nDAO Functions\n\nManual Dispute Resolution: For disputes that cannot be resolved through automated proofs, the DAO can step in to review and make a final decision based on community consensus.\nProtocol Upgrades: The DAO can propose and vote on protocol upgrades to enhance the system\u2019s functionality and security.\nNetwork Oversight: Providing ongoing oversight and making strategic decisions to ensure the network remains secure and efficient.\n\nThe combination of zk-SNARKs, hierarchical smart contracts, and the DAO creates a robust framework for fraud prevention and network governance. The minimized need for watchtowers is achieved through advanced cryptographic verification and efficient dispute resolution mechanisms. However, the DAO ensures that any issues requiring manual involvement are handled with a balance of automation, trust, rigorous mathematical verification, and democratic decision-making. This comprehensive approach provides a secure, scalable, and trustworthy decentralized payment network.\nDispute Resolution\nIn the event of a dispute between parties, the proposed network employs a dispute resolution mechanism based on the submitted zk-SNARK proofs and Merkle roots. The dispute resolution process can be formally described as follows:\n\\begin{algorithm}[H]\n\\caption{Dispute Resolution}\n\\begin{algorithmic}[1]\n\\Procedure{ResolveDispute}{$S, R, \\pi_S, \\pi_R$}\n\\State $S$ submits their final state proof $\\pi_S$\n\\State $R$ submits their final state proof $\\pi_R$\n\\State Verify $\\pi_S$ and $\\pi_R$ against the submitted Merkle roots\n\\If{$\\pi_S$ is valid and $\\pi_R$ is invalid}\n\\State Resolve the dispute in favor of $S$\n\\ElsIf{$\\pi_R$ is valid and $\\pi_S$ is invalid}\n\\State Resolve the dispute in favor of $R$\n\\Else\n\\State Resolve the dispute based on the most recent valid state proof\n\\EndIf\n\\EndProcedure\n\\end{algorithmic}\n\\end{algorithm}\n\nHere, S and R represent the disputing parties, and \\pi_S and \\pi_R are their respective final state proofs. The dispute resolution mechanism verifies the submitted proofs against the Merkle roots stored on-chain and resolves the dispute based on the validity of the proofs. This ensures that the resolution is based on the most recent valid state of the payment channel, preventing fraud and maintaining the integrity of the system.\nThe dispute resolution process follows these steps:\n\nDispute Initiation: Either party can initiate a dispute by submitting a dispute request to the relevant smart contract (e.g., the intermediate contract managing their user group).\nEvidence Submission: Both parties are required to submit their final state proofs (\\pi_S \\text{ and } \\pi_R) within a predefined timeframe (e.g., 24 hours). These proofs represent the latest state of their respective payment channels and include the relevant transaction history.\nProof Verification: The dispute resolution mechanism verifies the submitted proofs against the Merkle roots stored on-chain. This verification process ensures that the proofs are valid and consistent with the global state of the network.\nResolution: The dispute is resolved based on the validity of the submitted proofs:\n\nIf \\pi_S is valid and \\pi_R is invalid, the dispute is resolved in favor of party S.\nIf \\pi_R is valid and \\pi_S is invalid, the dispute is resolved in favor of party R.\nIf both proofs are valid, the dispute is resolved based on the most recent valid state proof, determined by the timestamp or sequence number associated with the proofs.\nIf neither proof is valid or if one party fails to submit their proof within the required timeframe, the dispute can be escalated to a higher-level contract (e.g., the root contract) or a trusted third party for manual review and resolution.\n\n\nOutcome Enforcement: Once the dispute is resolved, the smart contracts automatically enforce the outcome by updating the balances of the involved parties according to the resolution decision. This may involve redistributing tokens between the parties\u2019 payment channels or applying penalties for fraudulent behavior.\n\nTo incentivize honest behavior and discourage frivolous disputes, the network can implement additional mechanisms:\n\nDispute Bond: Parties initiating a dispute may be required to post a bond (in the form of tokens) that is forfeited if their submitted proof is found to be invalid or if they fail to submit their proof within the required timeframe. This bond serves as a deterrent against malicious actors and ensures that disputing parties have a stake in the resolution process.\nReputation System: The network can maintain a reputation score for each user based on their history of successful transactions and dispute resolutions. Users with a high reputation score may be given preference in case of ambiguous disputes or may enjoy reduced dispute bond requirements. Conversely, users with a history of fraudulent behavior or frivolous disputes may face higher bond requirements or even temporary suspension from the network.\n\nBy combining cryptographic proofs, smart contract automation, and economic incentives, the proposed dispute resolution mechanism ensures that conflicts are resolved fairly and efficiently while maintaining the integrity of the payment network.\nExample 5: Dispute Resolution\nSuppose a dispute arises between Alice and Bob regarding the final state of their payment channel. Alice claims that her final balance is 100 tokens, while Bob claims that Alice\u2019s final balance is 80 tokens.\n\nDispute Initiation: Alice initiates a dispute by submitting a dispute request to the intermediate contract IC_1 that manages their user group. She deposits the required dispute bond of 10 tokens.\nEvidence Submission: Alice and Bob submit their respective final state proofs, \\pi_A and \\pi_B, to the dispute resolution mechanism within the 24-hour timeframe. Alice\u2019s proof \\pi_A shows her balance as 100 tokens, while Bob\u2019s proof \\pi_B shows Alice\u2019s balance as 80 tokens.\nProof Verification: The dispute resolution mechanism verifies the submitted proofs against the Merkle roots stored on-chain. It finds that Alice\u2019s proof \\pi_A is consistent with the on-chain state, while Bob\u2019s proof \\pi_B is invalid.\nResolution: As Alice\u2019s proof \\pi_A is valid and Bob\u2019s proof \\pi_B is invalid, the dispute is resolved in favor of Alice. The resolution confirms that Alice\u2019s final balance is indeed 100 tokens.\nOutcome Enforcement: The intermediate contract IC_1 automatically updates the balances of Alice and Bob\u2019s payment channels according to the resolution decision. Alice\u2019s balance remains at 100 tokens, while Bob\u2019s balance is adjusted based on the discrepancy. Additionally, Bob\u2019s dispute bond of 10 tokens is forfeited and distributed as a reward to Alice for submitting a valid proof.\n\nThis example demonstrates how the dispute resolution mechanism ensures the integrity of the payment network by resolving conflicts based on the validity of the submitted zk-SNARK proofs and the Merkle roots stored on-chain, while also incentivizing honest behavior through the use of dispute bonds.\nComparison to Alternative Approaches\nThe proposed decentralized payment network offers several advantages over alternative approaches, such as traditional blockchain-based payment systems and centralized payment networks.\nCompared to traditional blockchain-based payment systems, the proposed network provides higher scalability and privacy. The use of off-chain payment channels and zk-SNARKs enables faster and more private transactions, while the hierarchical smart contract structure and partitioned storage nodes enable more efficient processing and storage of transaction data.\nCompared to centralized payment networks, the proposed system offers greater security, transparency, and censorship resistance. By leveraging the security and immutability of blockchain technology and the privacy-preserving properties of zk-SNARKs, the network can provide a more secure and transparent payment infrastructure that is resistant to censorship and control by central authorities.\nExample 6: Comparison to Centralized Payment Networks\nSuppose a centralized payment network relies on a single trusted entity to process transactions and manage user balances. While this approach may offer high transaction throughput, it also presents several risks and limitations:\n\nSingle point of failure: If the central entity experiences technical issues or becomes compromised, the entire payment network may become unavailable or vulnerable to fraud.\nLack of transparency: Users must trust the central entity to manage their funds honestly and securely, without the ability to independently verify the state of their balances or the validity of transactions.\nCensorship risk: The central entity may choose to block or reverse transactions based on their own criteria, censoring users or restricting access to the payment network.\n\nIn contrast, the proposed decentralized payment network addresses these issues through its use of blockchain technology, zk-SNARKs, and a decentralized architecture:\n\nDecentralization: The network is maintained by a distributed network of storage nodes and smart contracts, eliminating the single point of failure and ensuring the availability and resilience of the system.\nTransparency and verifiability: Users can independently verify the state of their balances and the validity of transactions using the zk-SNARK proofs and the Merkle roots stored on-chain, providing a high level of transparency and trust in the system.\nCensorship resistance: The decentralized nature of the network and the use of zk-SNARKs ensure that transactions cannot be easily censored or reversed by any single entity, preserving the freedom and autonomy of users.\n\nThis example highlights the significant advantages of the proposed decentralized payment network over centralized alternatives, providing a more secure, transparent, and censorship-resistant payment infrastructure for users.\nAnalysis\nThis section provides a comprehensive analysis of the security, privacy, and scalability properties of the proposed decentralized payment network, and compares it to alternative approaches.\nWe delve into the technical details of the zk-SNARK implementation, discuss potential challenges and trade-offs, explore additional privacy-enhancing techniques, and consider the governance aspects of the system.\nSecurity Analysis\nThe security of the proposed network relies on the soundness and completeness of the zk-SNARK proofs, as well as the integrity of the hierarchical smart contract structure. We employ the state-of-the-art zk-SNARK construction proposed by Groth, which offers succinct proofs and efficient verification. The zk-SNARK scheme is built upon the q-Power Knowledge of Exponent (q-PKE) assumption and the q-Decisional Diffie-Hellman (q-DDH) assumption in bilinear groups.\nLet \\mathcal{G}_1, \\mathcal{G}_2, and \\mathcal{G}_T be cyclic groups of prime order p, and let e : \\mathcal{G}_1 \\times \\mathcal{G}_2 \\rightarrow \\mathcal{G}_T be a bilinear map. The q-PKE assumption states that for any polynomial-size adversary \\mathcal{A}, the following probability is negligible in the security parameter \\lambda:\n\n\\Pr\\left[\n\\begin{array}{c}\ng \\xleftarrow{\\$} \\mathcal{G}_1, \\quad \\alpha, s \\xleftarrow{\\$} \\mathbb{Z}_p, \\quad g_2 \\leftarrow g^{\\alpha},\\\\\n(c_1, \\ldots, c_q) \\xleftarrow{\\$} \\mathbb{Z}_p^q, \\quad t_i \\leftarrow g^{c_i} \\cdot g_2^{s \\cdot c_i},\\\\\n(h, \\hat{h}) \\leftarrow \\mathcal{A}(g, g_2, \\{t_i\\}_{i=1}^q) : \\\\\nh = g^s \\wedge \\hat{h} \\neq \\prod_{i=1}^q t_i^{c_i}\n\\end{array}\n\\right]\n\nThe q-DDH assumption states that for any polynomial-size adversary \\mathcal{A}, the following probability is negligible in the security parameter \\lambda:\n\n\\Pr\\left[\n\\begin{array}{c}\ng \\xleftarrow{\\$} \\mathcal{G}_1, \\quad \\alpha, s, r \\xleftarrow{\\$} \\mathbb{Z}_p, \\quad g_2 \\leftarrow g^{\\alpha},\\\\\n(c_1, \\ldots, c_q) \\xleftarrow{\\$} \\mathbb{Z}_p^q, \\quad t_i \\leftarrow \\begin{cases}\ng_2^{c_i}, & \\text{if } b=0\\\\\ng_2^{c_i + s}, & \\text{if } b=1\n\\end{cases},\\\\\nb \\xleftarrow{\\$} \\{0, 1\\}, \\quad b' \\leftarrow \\mathcal{A}(g, g_2, \\{t_i\\}_{i=1}^q) : \\\\\nb = b'\n\\end{array}\n\\right]\n\nUnder these assumptions, the zk-SNARK construction ensures that the proofs are sound and complete, meaning that a prover cannot create a valid proof for a false statement (soundness) and that a valid proof always verifies successfully (completeness). Consequently, transactions are guaranteed to be valid, and balances are correctly updated, preventing double-spending and other fraudulent activities.\nAttack Vector Prevention\nThe hierarchical smart contract structure, combined with the storage nodes, ensures that the network\u2019s global state remains consistent and verifiable, even in the presence of malicious actors. The smart contracts are implemented using the Solidity language and are formally verified using the Oyente and Zeus tools to ensure their correctness and security.\n1. Collusion during the trusted setup ceremony:\n\nMitigated by the use of secure multi-party computation (MPC) protocols like ZEXE, ensuring a distributed setup process.\nInvolvement of diverse participants reduces the likelihood of successful collusion.\n\n2. Collusion among users:\n\nPrevented by the use of unforgeable and computationally binding zk-SNARK proofs (PLONK), making it infeasible for users to create valid proofs for fraudulent transactions.\nSmart contracts verify proofs before executing transactions, ensuring only legitimate transactions are processed.\n\n3. Collusion among storage nodes:\n\nMitigated by the distributed storage architecture with multiple nodes maintaining data copies, making it difficult for nodes to collude and provide false data without detection.\nThe use of Merkle trees and hash-based commitments allows smart contracts to verify data authenticity.\n\n4. Smart contract vulnerabilities:\n\nAddressed by formal verification tools, independent security audits, secure coding practices, access controls, and error handling mechanisms.\nUpgradability and emergency stop mechanisms allow for deploying security patches and freezing contracts in case of severe vulnerabilities.\n\n5. Privacy leaks:\n\nMitigated by the use of zk-SNARKs, ensuring transaction privacy.\nMixing techniques, anonymity networks, metadata obfuscation, and regular security assessments further enhance privacy protection.\n\n6. Sybil attacks:\n\nInherently resistant due to the use of zk-SNARK proofs, smart contract verification, and the underlying blockchain\u2019s consensus mechanism.\nThe system\u2019s design, including proof validity and economic disincentives, makes it infeasible for attackers to create and manipulate multiple identities or payment channels.\nThe requirement of fees to set up payment channels and execute transactions further discourages Sybil attacks by making them financially costly for attackers.\n\n7. Denial-of-Service (DoS) attacks:\n\nInherently mitigated by the computational cost of generating zk-SNARK proofs for each transaction, making it impractical for attackers to flood the network with a large number of transactions.\nThe decentralized architecture and the resilience of the underlying Ethereum blockchain provide additional protection against DoS attacks.\n\nScalability Analysis\nThe proposed decentralized payment network exhibits significant scalability potential due to its innovative use of zero-knowledge proofs (ZKPs), particularly zk-SNARKs, and the absence of traditional consensus mechanisms, which together enable instant finality. In this section, we will provide a detailed mathematical assessment and real-world benchmarks to validate the network\u2019s scalability potential.\nMathematical Formalism of TPS Scalability\nLet us define the total time per transaction T_{tx} as the sum of the time for proof generation T_{pg}, network latency T_{nl}, and the overhead for contract execution and state updates T_{oh}. Given that we aim for high scalability, we will leverage parallel processing capabilities of nodes to handle multiple channels efficiently.\n\nT_{tx} = T_{pg} + T_{nl} + T_{oh}\n\nAssuming average values for these times, such as:\n\n\\begin{align*}\nT_{pg} & \\approx 50 \\text{ ms} \\\\\nT_{nl} & \\approx 50 \\text{ ms} \\\\\nT_{oh} & \\approx 50 \\text{ ms} \\\\\n\\end{align*}\n\nThe total time per transaction can be approximated as:\n\nT_{tx} = 50 \\text{ ms} + 50 \\text{ ms} + 50 \\text{ ms} = 150 \\text{ ms}\n\nThus, the transactions per second (TPS) per node can be calculated as:\n\nTPS_{\\text{per node}} = \\frac{1}{T_{tx}} = \\frac{1}{150 \\text{ ms} / 1000 \\text{ ms/s}} \\approx 6.67 \\text{ TPS}\n\nIf we consider the network scaling linearly with the number of nodes, the total TPS for n nodes can be expressed as:\n\nTPS_{\\text{total}} = TPS_{\\text{per node}} \\times n = 6.67 \\times n\n\nFor example, with 100 nodes, the network could achieve:\n\nTPS_{\\text{total}} = 6.67 \\times 100 = 667 \\text{ TPS}\n\nTPS Within Channels\nTo further detail the TPS within individual payment channels, consider that each node can manage multiple channels. Let c denote the number of channels a node can handle, and let T_{channel} represent the time to process a transaction within a channel.\n\nT_{channel} = T_{pg} + T_{nl} + T_{oh} = 70 \\text{ ms} \\quad (\\text{considering optimized conditions})\n\nThus, the TPS per channel:\n\nTPS_{\\text{per channel}} = \\frac{1}{T_{channel}} = \\frac{1}{70 \\text{ ms} / 1000 \\text{ ms/s}} \\approx 14.29 \\text{ TPS}\n\nIf each node can handle c channels, the total TPS per node considering channels would be:\n\nTPS_{\\text{node, channels}} = TPS_{\\text{per channel}} \\times c\n\nAssuming a node can handle 10,000 channels:\n\nTPS_{\\text{node, channels}} = 14.29 \\times 10,000 = 142,900 \\text{ TPS}\n\nFor a network with n nodes, the total TPS could be:\n\nTPS_{\\text{total, channels}} = 142,900 \\times n\n\nReal-World Micro Benchmarks\nTo validate these theoretical calculations, we consider benchmarks from existing state channel implementations:\n\nCeler Network: Claims to handle up to 15,000 TPS per node.\nRaiden Network: Aims for several thousand TPS per node.\nLightning Network: Estimates around 1,000 TPS per node in practical scenarios.\n\nGiven these benchmarks, our assumption of handling 10,000 channels per node with approximately 14.29 TPS per channel, resulting in 142,900 TPS per node, is ambitious but within a reasonable range for a highly optimized implementation leveraging zk-SNARKs and efficient contract management.\nPotential Bottlenecks\nDespite the promising scalability, several bottlenecks could impact performance:\n\nProof Generation and Verification: While zk-SNARKs are efficient, the complexity of proofs can increase with advanced use cases.\nNetwork Latency: Global transactions can introduce delays that affect overall throughput.\nSmart Contract Efficiency: Inefficiencies in smart contracts can create processing delays.\nStorage and Data Management: Managing large numbers of channels and associated data could become challenging.\nNode Reliability and Security: Ensuring the reliability and security of each node is critical.\n\nAddressing these bottlenecks through ongoing optimization and robust infrastructure will be crucial to achieving the theoretical TPS and ensuring the network\u2019s scalability and robustness.\nSummary\nThe proposed decentralized payment network, leveraging zk-SNARKs and instant finality mechanisms, exhibits significant scalability potential. The mathematical formalism and real-world benchmarks indicate that the network can achieve high TPS by efficiently managing multiple channels per node. Continuous optimization and addressing potential bottlenecks will be essential to realizing this potential in practice.\nScalability Comparison with Existing Layer 2 Solutions\nKey Features of the Proposed Network\n\nUnilateral Payment Channels: Enables high transaction throughput by facilitating off-chain transactions.\nZero-Knowledge Proofs (zk-SNARKs): Ensures privacy and efficient transaction validity.\nInstant Finality: Transactions achieve instant finality without on-chain confirmations.\nPartitioned Storage Nodes: Manages off-chain data efficiently, reducing on-chain storage requirements.\n\nExisting Layer 2 Solutions\nState Channels (e.g., Lightning Network, Raiden Network):\n\nScalability: High throughput off-chain.\nFinality: Near-instant off-chain finality.\nChallenges: Requires channel monitoring and on-chain closures.\n\nPlasma:\n\nScalability: High throughput with off-chain child chains.\nFinality: Periodic on-chain commitments.\nChallenges: Complex exit management and data availability.\n\nOptimistic Rollups:\n\nScalability: Batches transactions off-chain.\nFinality: Delayed due to fraud proof periods.\nChallenges: Requires fraud proof monitoring.\n\nZK-Rollups:\n\nScalability: High throughput with off-chain transaction bundling.\nFinality: Near-instant with zk-SNARKs.\nChallenges: Complex proof generation.\n\nComparative Analysis\nThroughput and Finality:\n\nThe proposed network achieves high throughput and instant finality, comparable to state channels and ZK-Rollups and superior to Optimistic Rollups.\n\nEfficiency and Cost:\n\nMore cost-efficient by reducing on-chain transactions and eliminating mining, outperforming state channels and Plasma.\n\nData Management:\n\nEfficient off-chain data management through partitioned storage nodes, similar to Plasma and rollups.\n\nSecurity and Privacy:\n\nRobust security and privacy with zk-SNARKs, comparable to ZK-Rollups and superior to solutions relying on fraud proofs.\n\nImplementation Details\nThe proposed decentralized payment network is implemented using a combination of Rust, TypeScript, and Solidity. The core components, such as the zk-SNARK proof generation and verification, are written in Rust for performance and security reasons. The smart contracts are developed using Solidity, while the frontend and client-side interactions are built with TypeScript.\nSpecific zk-SNARK Construction\nThe system employs the PLONK zk-SNARK construction, which offers universality, updatability, and efficient proof generation and verification. PLONK allows for the creation of a universal and updateable structured reference string (SRS) that can be reused across multiple circuits or applications, reducing the complexity and coordination overhead associated with repeated trusted setups.\nThe PLONK circuits are designed using the arkworks library in Rust, which provides a set of tools and primitives for building zk-SNARK circuits compatible with the PLONK proving system. The library supports efficient constraint generation, witness computation, and proof generation, making it well-suited for the development of the decentralized payment network.\nChallenges and Optimizations\nOne of the main challenges in implementing PLONK is the complexity of designing and optimizing the circuits to take advantage of the universal SRS. This requires a deep understanding of the PLONK framework and the techniques for constructing efficient and secure circuits.\nTo address this challenge, the implementation leverages various optimization techniques, such as:\n\nConstraint system optimization: Minimizing the number of constraints in the circuit by using efficient gate design and layout techniques, such as gate aggregation and constant folding.\nWitness compression: Reducing the size of the witness by using compact data representations and eliminating redundant information.\nProof aggregation: Batching multiple proofs together to reduce the overall proof size and verification cost.\n\nThese optimizations help to improve the performance and scalability of the PLONK-based zk-SNARK circuits, ensuring that the decentralized payment network can handle a high volume of transactions efficiently.\nIntegration with Ethereum\nThe smart contracts for the payment network are implemented using Solidity and deployed on the Ethereum blockchain. The contracts interact with the PLONK proofs generated by the Rust components through a verification contract that is optimized for the PLONK proving system.\nThe verification contract is designed to be gas-efficient and supports batch verification of PLONK proofs, allowing multiple proofs to be verified in a single transaction. This helps to reduce the overall cost and improve the throughput of the system.\nTrusted Setup Ceremony\nAs PLONK requires a trusted setup for the universal SRS, a multi-party computation (MPC) ceremony is conducted to generate the SRS. The ceremony involves multiple participants from different organizations and backgrounds, ensuring that no single party has control over the entire setup process.\nThe MPC ceremony is organized and facilitated using secure computation frameworks, such as the ZEXE library, which provides a set of tools and protocols for conducting distributed key generation and parameter setup.\nConcise Example: Private Asset Transfer\nIn a private asset transfer, Alice can transfer assets to Bob without revealing the transaction details to the public. Using PLONK, Alice generates a proof \u03c0 that verifies the validity of the transfer and her sufficient balance without disclosing the transaction amount \u0394.\nTransfer T = (A, B, \\pi) where \\pi is the PLONK proof\n\n\\pi \\leftarrow \\text{generateProof}(A, B, \\Delta)\nSubmit (A, B, \\pi) to the transfer contract\nContract verifies \\pi\n\nIf \\pi is valid, execute transfer from A to B\nElse, reject transfer\n\n\n\nThe proof \\pi ensures the following conditions:\n\nB_A \\geq \\Delta (Alice\u2019s balance is sufficient)\nB_A' = B_A - \\Delta (Alice\u2019s updated balance)\nB_B' = B_B + \\Delta (Bob\u2019s updated balance)\n\nThe smart contract executes the transfer only if the proof is valid, ensuring the transfer\u2019s legitimacy without revealing the transaction details.\nBy leveraging PLONK, the proposed decentralized payment network achieves a balance between privacy, scalability, and ease of implementation. The universal and updateable nature of PLONK, combined with the optimization techniques and secure trusted setup ceremony, provides a solid foundation for building a privacy-focused and efficient payment system.\nUse Cases for Privacy\nConfidential Voting Systems\nConfidential voting systems are a critical use case for enhanced privacy in decentralized networks. Voting systems must ensure that each vote is anonymous and secure while maintaining the integrity and transparency of the election process. By leveraging zk-SNARKs, our network can provide a solution that guarantees the confidentiality of votes while allowing for public verification of election results.\nIn a confidential voting system built on the proposed network, voters would cast their votes through private transactions, with zk-SNARKs proving that each vote is valid and belongs to an eligible voter without revealing the voter\u2019s identity. The votes would be tallied through a series of confidential transactions, with the final result verifiable through the aggregated Merkle roots stored on-chain. This approach ensures that the voting process is transparent and auditable while preserving the privacy of individual voters.\nPrivate Asset Transfers\nPrivate asset transfers are another significant use case for enhanced privacy in a decentralized network. These transfers require confidentiality to protect the financial privacy of users, ensuring that transaction details remain private while the integrity of the transfer is verifiable.\nWith the proposed network, users can transfer assets through confidential payment channels, with zk-SNARKs proving the validity of the transactions without revealing the amounts or the identities of the parties involved. This feature is particularly valuable for businesses and individuals who wish to keep their financial transactions private, such as in the case of sensitive business deals, wealth management, or personal remittances.\nSecure Health Records Management\nSecure health records management is an essential use case for enhanced privacy, where sensitive health information must be kept confidential while ensuring that authorized parties can verify the records. Using zk-SNARKs, the proposed network can enable the secure storage and sharing of health records while maintaining patient privacy.\nIn this use case, health records would be stored off-chain, with zk-SNARKs proving the authenticity and integrity of the records without revealing their contents. Patients can grant access to their records to authorized parties, such as healthcare providers or insurance companies, through private transactions. The authorized parties can then verify the records\u2019 authenticity using the zk-SNARK proofs, ensuring that the records have not been tampered with while preserving patient confidentiality.\nGlobal Payment System\nA global payment system is perhaps the most scalable and impactful use case for a decentralized network with enhanced privacy. Such a system must provide sufficient privacy to protect user transactions while ensuring transparency and scalability to facilitate mass adoption. By leveraging zk-SNARKs, the proposed network can achieve a balanced privacy level that ensures transaction confidentiality without hindering scalability or regulatory compliance.\nIn a global payment system built on the proposed network, users can transact through confidential payment channels, with zk-SNARKs proving the validity of transactions without revealing the amounts or the identities of the parties involved. This privacy level can be customized based on the specific requirements of different jurisdictions, ensuring compliance with local regulations while still preserving user privacy.\nTo facilitate cross-border transactions and enable seamless interoperability with existing payment systems, the network can integrate with traditional financial institutions and payment processors through secure off-chain communication channels. These channels can leverage zk-SNARKs to prove the authenticity of transactions and balances without revealing sensitive information, enabling a hybrid approach that combines the benefits of decentralized privacy with the reach and stability of established financial networks.\nBy leveraging zk-SNARKs in these use cases, the proposed decentralized payment network can provide enhanced privacy and scalability, making it suitable for a wide range of applications. These examples illustrate how the network can achieve a balance between privacy and transparency, facilitating mass adoption while maintaining the necessary confidentiality.\nConclusion\nThe proposed decentralized payment network offers:\n\nHigher Throughput: Comparable to or exceeding state channels and rollups.\nInstant Finality: Superior to Optimistic Rollups.\nCost Efficiency: Reduces on-chain interactions and eliminates mining.\nEnhanced Privacy: Matches or surpasses ZK-Rollups.\n\nThe unique combination of features in the proposed network makes it a potentially more scalable and private solution compared to existing Layer 2 systems.\nBy leveraging zk-SNARKs in these use cases, we can provide enhanced privacy and scalability, making our decentralized network suitable for a wide range of applications. These examples illustrate how the network can achieve a balance between privacy and transparency, facilitating mass adoption while maintaining the necessary confidentiality.\nReferences\n\nPoon, J., & Dryja, T. (2016). The Bitcoin Lightning Network: Scalable Off-Chain Instant Payments. Lightning Network Whitepaper. https://lightning.network/lightning-network-paper.pdf\nButerin, V., & Poon, J. (2017). Plasma: Scalable Autonomous Smart Contracts. Plasma Whitepaper. https://plasma.io/plasma.pdf\nRaiden Network Team. (2017). Raiden Network: Fast, Cheap, Scalable Token Transfers for Ethereum. Raiden Network\nCeler Network. (2019). Celer Network: Bring Internet Scale to Every Blockchain. Celer Network Whitepaper. https://www.celer.network/doc/CelerNetwork-Whitepaper.pdf\nPLONK Documentation. (n.d.). ZK-SNARKs: PLONK. Retrieved from https://docs.plonk.cafe/\nBen-Sasson, E., Chiesa, A., Tromer, E., & Virza, M. (2014). Scalable Zero-Knowledge via Cycles of Elliptic Curves. In International Cryptology Conference (pp. 276-294). Springer, Berlin, Heidelberg.\nGroth, J. (2016). On the Size of Pairing-based Non-interactive Arguments. In Annual International Conference on the Theory and Applications of Cryptographic Techniques (pp. 305-326). Springer, Berlin, Heidelberg.\nZhang, F., Cecchetti, E., Croman, K., Juels, A., & Shi, E. (2016). Town Crier: An Authenticated Data Feed for Smart Contracts. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (pp. 270-282).\nBen-Sasson, E., Chiesa, A., Genkin, D., Tromer, E., & Virza, M. (2015). SNARKs for C: Verifying Program Executions Succinctly and in Zero Knowledge. In Annual Cryptology Conference (pp. 90-108). Springer, Berlin, Heidelberg.\nHioki, L., Dompeldorius, A., & Hashimoto, Y. (2024). Plasma Next: Plasma without Online Requirements. Ethereum Research. Retrieved from Plasma Next: Plasma without Online Requirements\n\n", "\nOn the gas efficiency of the WHIR polynomial commitment scheme\nJoint post with @WizardOfMenlo\nTLDR; We developed an open-sourced and MIT licensed prototype EVM verifier for the WHIR polynomial commitment scheme (PCS). For a multivariate polynomial of 22 variables and 100 bits of security, verification costs are 1.9m gas. With a more aggressive parameter setting, we reach even lower costs, below the 1.5m gas mark. This makes WHIR a serious post-quantum PCS candidate for teams using or looking to leverage STARKs in production.\nWHIR\nWHIR is a multilinear PCS requiring only a transparent setup and guaranteeing post-quantum security, while achieving high verification speeds. We wanted to estimate how high verification speed would translate in terms of gas usage, when verifying WHIR proofs on the EVM. To this end, we developed a prototype WHIR verifier and benchmarked it at various parameters levels.\nOur prototype implementation supports various settings, with a folding factor of at most 4 - this is an implementation idiosyncrasy stemming from gas optimizations and can be easily modified. While we present results for a specific set of parameters, our verifier is open-sourced and MIT-licensed, letting anyone test it with different configuration requirements.\nGas saving strategies\nWHIR is equipped with a variety of parameters and lets users choose how much work the prover should carry out in the benefit of the verifier. We quickly review here four important ones:\n\nThe chosen code rate \\rho will impact the argument size and hence verifier time, at the expense of prover time. In concrete terms, with WHIR, the number of queries (i.e. requests for evaluations of a polynomial) a verifier makes tends to 0 as \\rho approaches 0. This means that each saved query translates into saved gas costs.\nThe folding factor k determines the number of rounds between the prover and verifier. A higher k means fewer rounds, but additional prover work.\nThe amount of proof of work grinding[1] lets the verifier decrease the number of authentication paths it will demand from the prover. Higher grinding levels translates into reducing both calldata and verifier execution costs.\nWe can also tune WHIR\u2019s security parameter \\lambda directly. In practice, this means that higher \\lambda values will result in increased verifier queries and gas costs.\n\nIndependently from WHIR, one final technique we can leverage is masking. It allows to save calldata by masking the output of the last bytes of the merkle tree hash function[2]. For the sake of completeness, we included an implementation of this verifier working with a (naive) version of this industry-standard technique. We recall that masking is specifically beneficial when achieving \\lambda \\le 128, as to get to a \\lambda security level, we need our hash function digest size to be at most 2*\\lambda.\nResults\nIn this work, we show how the WHIR verifier performs with different starting rates \\rho, folding factors k, and \\lambda bits of security, for randomly sampled polynomials of 16, 20 and 22 variables and instantiated with the capacity bound conjecture. We will plot how each of those parameters impact gas costs relative to the chosen level of pow grinding and show how to reach low gas costs when using WHIR.\nWe chose our experiment parameters to be in line with what we consider to be useful levels of practicality and security. Our gas results are obtained from computing an average transaction gas cost using foundry forge and anvil at version 0.2.0 - commit e10ab3d.\nSetting \\rho\nWe start with adjusting for the code rate and plot the impact of the chosen rate on the verifier\u2019s gas usage. We can see the impact of choosing the correct starting code rate \\rho is quite large. For 20 variables and pow grinding at 30 bits, we can cut gas costs in half when going from \\rho=2^{-1} to \\rho=2^{-6}. Hence, for the rest of our experiment, we set a code rate \\rho=2^{-6}. Also, we omitted for the sake of clarity results for polynomials with 22 variables. We will now start to plot results for it as well.\n\nSetting k\nOur next parameter of interest will be the folding factor k. In the case where we have a polynomial composed of 22 variables and pow grinding set to 30 bits, we end up with a x1.75 increase in gas costs when using a lower folding factor k=2 instead of k=4. Hence, we will now set k=4.\n\nSetting \\lambda\nNext, we tune our \\lambda parameter to a more aggressive level of security. We plot how setting \\lambda=80 further decreases our gas costs. Clearly, removing 20 bits of security results in large additional savings. In the case of 22 variables and 30 bits of grinding, each removed bit of security translates into more than 10k of saved gas.\n\nMasking\nFinally, we apply a naive masking strategy for our calldata. Since we set \\lambda = 80, we mask commitments to 160 bits - a similar level to what starkware runs (or has been running) its verifier. Combined with \\rho=6, k=4, we get to our final and rather competitive gas costs for our WHIR verifier:\n\nFor 16 variables, our naive implementation of this strategy saves us an average additional 25k gas. We end with an average total gas cost of 1091720 gas, just above the 1m mark.\nIn the case where our multivariate  polynomials are of size 20 and 22, we can save an additional ~35k gas. We end up with average total gas costs of respectively 1388230 and 1493552 gas, below the 1.5m gas mark.\n\nFor comparison, verifying groth16 and fflonk proofs costs around 300k gas today. However, WHIR is transparent and post-quantum. Such gas costs illustrate why we think WHIR is a pretty good option when not only compared to protocols with similar assumptions like FRI, but also to trusted setup based ones.\nFuture directions\nFirst, we developed a prototype verifier. We are confident that additional optimizations lie ahead. One of the low hanging fruit being that there still are parts of our code that would benefit from assembly re-writes.\nFor 22 variables, with \\lambda=80, \\rho=1/2^6, k=4, 30 bits of grinding and masking, our average gas cost is almost 1.5m gas. However, we expect a total optimized gas cost averaging around 1.2m-1.3m gas. Indeed, our current (average) costs breakdown consists into roughly: 700k gas for calldata, 250k gas for merkle proving, 80k gas for computing STIR challenges, 65k gas for the sumcheck iterations, 60k gas for fiat shamir utilities (pop from transcript, pow checking, \u2026) and 25k gas for various uni/multivariate evaluations. Hence, we estimate remaining glue code (paid in memory expansions, overheads from using abstractions, \u2026) to take around 300k - 250k gas, which an assembly rewrite would help reduce.\nAlso, our implementation of masking has been somewhat naive, as it simply consisted into zeroing the last 12 bytes of our  merkle decommitments. We would be happy to integrate an improved version of this masking strategy to not pay anything for masked values. On a related front, we carried out a bespoke multi merkle proof assembly implementation hailing from Recmo\u2019s initial solidity version. Multi-merkle proofs make up for ~20% of gas costs of the verifier. We would be happy to see further algorithmic improvements here as we suspect our version to be far from perfect.\nFinally, WHIR isn\u2019t limited to a particular field. We would be curious to see how those numbers fare in the context of different, smaller fields. In particular, we expect our field operations to bear less gas costs compared to when working with the BN254 scalar field.\nAcknowledgements\nThanks to @alxkzmn for helping with the merkle and pow modules in an early version of this work.\n\n\nFor a primer on grinding, see section 3.11.3 of the ethSTARK paper. \u21a9\ufe0e\n\nGas savings obtained from this approach might be subject to change in the future, see EIP 7623. \u21a9\ufe0e\n\n\n", "\nCurrently at max, Ethereum TPS is maxed out 12-15 TPS  compared to VISA(Visa does 25k up to 65k per seconds btw) that\u2019s like way too low, now of course one of the main reasons this is happening is due to lack of parallelization on Ethereum but what if there was a way we could parallize Ethereum, in my current research which is still in work, I propose two ways to parallelize Ethereum transactions using two methods:\n1. Historical data\n2. Transactional data\nHistorical data: when you think about historical data, think about branch prediction your compiler uses. This would involve analysing past transactions from popular contracts that are not proxies or dynamic in nature. For example, think about Uniswap v3 contracts, non-upgradeable and highly predictable due to the number of times it has been executed there\u2019s no reason we can\u2019t predict possible states a popular or highly executed non-upgradeable smart contract can interact with, this is how it works: based on how highly executed contracts have behaved previously (which states they accessed), the system could predict which states future transactions will likely interact with.\nTransactional data: 3 methods and 3 ways, ever heard of static analysis, using a transaction method, the calldata and the contract bytecode there is way we can predict the expected states a transaction will cause effect\nHow the parallelization will work:\n\n\nBlock Builders can perform this analysis before proposing blocks. Once the transactional or historical data is analysed, the block builder can organize the transactions into groups or clusters that can be parallelized. Each cluster represents transactions that do not conflict with each other.\n\n\nBlock Proposal: After grouping transactions into parallelizable clusters, the block builder can propose a block in a way that minimizes execution time, potentially allowing multiple processors or threads to handle different clusters in parallel.\n\n\nChallenges\n\n\nDependent Transactions:  let\u2019s assume we have two transactions (y1, y2) that depend on each other, parallelization of this will be extremely difficult, a popular example of this would be a transaction to approve a number of tokens for spending(y1) and another one to actually transfer the token by a smart contract, trying to parallelize this will throw an error. A common way to solve this is not to parallelize transactions or use a single processor to process these transactions, how do we identify these transactions? quite simple, we can use Tx.origin and msg.sender to identify these types of transactions\n\n\nInternal transactions that are inter and intra dependent: The only way to solve these types is to implement a type of algorithm I call TGA (Transactional Graph Analysis). What\u2019s TGA,\n\n\nHow TGA works:\n\nUnderstanding Transaction Dependencies\n\n\nEach Ethereum transaction can modify or read certain parts of the state (e.g., account balances, contract storage).\nSome transactions depend on the results of previous transactions. For example, a transaction to transfer tokens depends on a prior approval transaction. If the approval hasn\u2019t happened yet, the transfer will fail.\nTGA helps map out these dependencies by constructing a graph where each node is a transaction, and edges between nodes represent dependencies.\n\n\nBuilding the Transaction Graph\n\n\nNodes: Each transaction in a block is represented as a node in the graph.\nEdges: An edge between two nodes (transactions) indicates that one transaction depends on the other. For example, if Tx1 modifies a state that Tx2 will read, an edge would be drawn from Tx1 to Tx2, showing that Tx2 depends on Tx1.\n\n\nAnalyzing the Graph\n\n\nOnce the graph is constructed, TGA analyzes the dependencies to identify independent transactions (i.e., nodes without incoming or outgoing edges).\nParallelizable clusters: Transactions that are not connected by edges can be grouped into clusters that can be processed in parallel. These clusters are independent and won\u2019t conflict with each other.\nSequential clusters: If transactions are connected by edges, TGA ensures that they are processed sequentially to preserve the correct execution order.\n\n\nHandling Cycles (Mutual Dependencies)\n\n\nSometimes, transactions may form a cycle where Tx1 depends on Tx2, and Tx2 depends on Tx1. This situation is called a cyclic dependency.\nTGA would detect these cycles and flag the transactions involved as requiring sequential processing, meaning they cannot be parallelized.\n\n\nPractical Implementation\n\n\nIn a block, the Block Builder could run TGA to analyse all transactions before proposing the block. This analysis would result in two sets:\n\n\nIndependent transactions: These can be executed in parallel across different processors.\n\n\nDependent transactions: These must be executed sequentially, based on their dependencies.\n\n\n\n\nExample:\nLet\u2019s say a block has 5 transactions:\n\nTx1: Transfers ETH from Alice to Bob.\nTx2: Approves a DAI transfer for a DeFi contract.\nTx3: Transfers DAI from Alice to the contract (depends on Tx2).\nTx4: Changes a parameter in a governance contract.\nTx5: Reads a balance from the governance contract (depends on Tx4).\n\nUsing TGA:\n\nTx1, Tx2, and Tx4 are independent (no edges between them), so they can be parallelized.\nTx3 depends on Tx2 and must be executed after it.\nTx5 depends on Tx4, so it must be executed after Tx4.\n\nOf course, a better method will be to implement timestamps for each transaction \nI also think this is more suitable to a L2 \n", "\nBackground\nCryptographically relevant quantum computer, if built, could enable Shor\u2019s algorithm and Grover\u2019s algorithm. These completely break ECDSA / ECDH, and reduce hash function (& cipher) strength from 2^n to 2^\\frac{n}{2}\nThere are some non-obvious parts of ETH which would need to be upgraded.\nWhat is still OK\n\nbip39 (pbkdf2-sha512) seems just fine\neip2333 validator withdrawal keys are also fine!\n\nWhat needs upgrading\n\nbip32 hdkey derivation\n\nShould be replaced by something post-quantum and better (no \u201cnon-hardened\u201d keys)\nNew scheme could be based on HKDF (like EIP-2333), but not HKDF-SHA256\nAlternative KDF is Blake3 in context mode (pq security is unclear)\nThe proposed scheme should support both ECC and new pq mode\n\n\nTransaction signing\n\nShould be replaced by lattice-based Falcon (FN-DSA / FIPS-206),\nor hash-based Sphincs-plus (SLH-DSA / FIPS-205)\nNew keys and signatures will consume more space\nFalcon-1024 has 1.75KB keys and 1.25KB sigs\nSLH-DSA-256 has 48-128B keys and 17-51KB sigs\n\n\nSender address recovery\n\nIt was feature of ECDSA (not available in Schnorr, for example)\nPerhaps txs (and not sigs) should encode sender addresses\n\n\nAddress format\n\nCurrently it\u2019s 40 hex characters, keccak256(pubkey)\nkeccak256 should be replaced by keccak512 / sha3-512 / sha512 / blake3-512\nHow does Grover algo affect brute-forcing of addresses? Should 40 characters be upped to 80-128?\nLonger address formats should probably use something like bech32 for checksumming & human-friendliness\nHow would new addresses interop with old addresses / EVM?\n\n\nEncrypted wallets\n\nShould upgrade from AES-128 to AES-256 or chacha20\nhmac-sha256 should upgrade to hmac-sha512 / kmac / blake3-512 (keyed mode)\n\n\nKZG EIP-4844 verification\n\nShould be replaced by a post-quantum scheme\nAlgorithms are unclear for now, any suggestions?\n\n\nEVM 0x20 opcode (KECCAK256)\n\nShould be replaced by keccak512 / sha3-512 / sha512 / blake3-512 (new opcode)\n\n\nEVM precompile for ECRECOVER\n\nSee address recovery above\n\n\nEVM precompiles for BN / BLS / KZG\n\nShould be replaced by newer schemes (unclear which ones?)\nNo more Groth16, vanilla PLONK, Marlin, BulletProof\n\n\nConsensus layer signature aggregation\n\nCurrently aggregates signatures of all validators once per epoch (6 mins)\nMore than 1M signatures right now?\nAlgorithms are unclear for now, any suggestions?\n\n\nZK-rollups (non STARK)\nAnything else?\n\nFinal thoughts\nI\u2019m confident all of these problems can be solved even in limited time. Let\u2019s start solving them.\nIf such computer appears soon, it\u2019s possible to do Vitalik\u2019s trick (How to hard-fork to save most users\u2019 funds in a quantum emergency): freeze all accounts and leverage BIP39 with ZK-proofs to recover funds into new pq scheme.\n", "\nOn Increasing the Block Gas Limit: Technical Considerations & Path Forward\nAuthored by: Toni, Marek, Pari, Jacek, Paul, Tim and Alex.\nAuthors\u2019 Note:\nThe core development community is committed to continuous improvement of the network\u2019s scalability and user experience. With recent community-driven initiatives, such as pumpthegas.org, there has been a growing call to increase Ethereum\u2019s block gas limit with some proposals approaching 60 million. While this enthusiasm reflects the shared goal of expanding Ethereum\u2019s capacity, it is important to proceed deliberately and in harmony with the technical realities of the protocol and its clients. Before encouraging the community to actively signal for limits beyond 36 million, we may want to deepen our understanding of the potential consequences\u2014conducting more analysis, collecting empirical data, and examining results of upcoming protocol changes in the greatest detail possible\u2014so that adjustments are made with both confidence and caution.\n\nContext\nThe consensus-layer (CL) clients currently implement certain constraints, as specified by the formal specifications. These constraints include a maximum acceptable uncompressed block size for gossip propagation, currently set to 10 MiB. In practice, this indirectly influences the maximum feasible block gas limit. Today, raising the gas limit to 60 million gas, as proposed by some community members, would generate blocks that exceed this gossip constraint\u2014leading to missed slots and overall network instability.\nUntil these client-level assumptions can be revisited and improved, the network should move forward with caution when considering increases beyond certain thresholds.\n\nRationale for Limits (Security Considerations):\nThese constraints are not arbitrary; they are in place to safeguard the network. Extremely large blocks can facilitate potential DoS vectors by forcing nodes to handle unwieldy amounts of data. Without practical use cases for such large blocks\u2014and with the risk of malicious actors exploiting them\u2014the core developers have designed limits to mitigate negative effects and protect the network\u2019s health.\n\n\nWhat This Means in Practice\n\n\nFunctionality up to ~40M gas:\nBlocks at or below this level remain within the acceptable size range, allowing clients to propagate them and maintain consensus stability. This ensures that validators do not see unexpected missed slots due to overly large blocks which would be prevented from being propagated because of gossip limits.\n\n\nBeyond ~40M gas:\nValid blocks larger than 10 MiB could fail to propagate as expected. This results in some validators missing their slots despite producing otherwise valid blocks. The gossip limits, which cannot be easily circumvented today, create a bottleneck. In addition, without further empirical data, the initial analyses guiding the blob count increases may not fully reflect the increased complexities of operating under a significantly higher gas limit.\n\n\n\nWhy Wait for Pectra?\nThe core developers have been planning the Pectra network upgrade that reduces worst-case block sizes and create the headroom needed to safely increase capacity. Two notable upcoming changes are:\n\n\nEIP-7623 (Included in Pectra):\nThis proposal aims to reduce worst-case block sizes. By increasing the cost of calldata for calldata-heavy transactions, it opens pathways to safely handle more capacity\u2014be that additional blobs or a higher gas limit. Reducing worst-case scenarios mitigates potential DoS vectors and helps ensure that the network remains stable and resilient under heavier loads.\n\n\nEIP-7691 (Included in Pectra):\nThis proposal will increase the target/maximum number of blobs per block from 4/6 to 6/9. By observing the network\u2019s performance under increased blob counts, we can gather data on propagation behavior, storage demands, and client resource usage. This empirical evidence will guide safer adjustments in block composition and size.\n\n\nBy first deploying the Pectra hardfork and analyzing the outcomes of EIP-7623 and EIP-7691 in a production environment, we stand to gain critical empirical evidence. This data will inform both core developers and the broader Ethereum community on how the network responds to changes in block composition and size. Armed with this understanding, the community can make more informed decisions on how to increase the gas limit while maintaining Ethereum\u2019s robustness and security.\nFuture upgrades, such as PeerDAS, will build on these insights, further refining parameters and scaling capabilities as the network evolves.\n\nA Call for Patience and Collaboration\nThe Ethereum community\u2019s proactive approach and passion for scaling solutions is commendable. Core developers are keenly aware of this momentum and, in general, are supportive of finding a responsible path to increasing the gas limit. However, moving too quickly\u2014especially beyond 36M gas\u2014risks unintended consequences and network instability.\nWe encourage all stakeholders\u2014users, validators, researchers, and client developers\u2014to remain patient and work together through this transition.\nBy deferring significant capacity increases until after the Pectra hardfork, monitoring the real-world effects of EIP-7623 and EIP-7691, and carefully reviewing the results, we can ensure that these increases are implemented responsibly and sustainably.\nWhile many sympathize with the desire to see Ethereum\u2019s gas limit significantly increase over a short period, a more incremental approach might be sounder. For instance, starting with a moderate increase to around 36M gas would allow us to carefully monitor the network\u2019s response, assess client performance, and ensure that no unforeseen issues arise. If the data supports further increases, we could then proceed more confidently to higher limits while maintaining the network\u2019s stability and security.\nFinally, we may also anticipate further updates and guidance from core developers in the coming days/weeks as they work towards resolving these issues.\n\nIn Summary\n\nThe current CL client constraints make immediately raising the gas limit to 60M gas impractical due to block size and gossip propagation issues.\nIncreasing the gas limit beyond 36M requires careful, data-driven planning and consideration of DoS resilience.\nThe upcoming Pectra hardfork, which includes EIP-7623 and EIP-7691, will provide the groundwork and data needed for safe throughput increases.\nCore developers support scaling the network, but emphasize a measured, evidence-based approach. This is in alignment with the motivation of the pumpthegas.org.\n\n", "\nCo-authored by @The-CTra1n , @linoscope , @AnshuJalan and @smartprogrammer , all Nethermind. Thanks to @Brecht and Daniel Wang from Taiko for their feedback on this solution. Feedback is not necessarily an endorsement.\nSummary\nWe introduce a new fast path for L2 withdrawals to L1 within the same L1 slot, enabled by solvers. To mitigate risks for solvers and maximize participation, solvers can observe preconfirmed L2 batches containing fast withdrawal requests and condition solutions for these requests on the batch containing the requests being successfully proposed to L1. This ensures no risk for solvers beyond the need to wait for the L2 state root containing the request to be proven on L1 to receive payment for facilitating the withdrawal.\nOverview\nThis post introduces a protocol which we describe as Fast & Slow (FnS) withdrawals to enable rollup users to atomically withdraw tokens from L2 to L1. When we say atomically withdraw, we mean that the first time the L2 withdrawal transaction appears on L1, tokens can be made available for the L1 user in the same slot without introducing reorg risk to the L2 bridge. This is achieved through solvers who facilitate these atomic withdrawals.\nAny L1 user can act as a solver. Prospective solvers observe L2 withdrawal requests on the L2 and solve the request on behalf of the L2 users on L1. This repurposes the entire L1 network as a network of solvers for fast L2 withdrawals. Leveraging that rollups post their transaction batches to the L1, FnS allows solvers to condition their withdrawal solutions on the sequence in which the L2 withdrawal exists. This allows the L1 solver to guarantee that the tokens being withdrawn by the L2 user will be made available to the solver when the state root containing the withdrawal request is proved on L1.\nWhen the state root containing a request is proven on L1, the L2 tokens requested for withdrawal become available on L1. This is the \u201cslow withdrawal\u201d, which always happens. The solver of a request gains unique access to the slow withdrawal tokens corresponding to the request. As such, L1 solvers must wait some time before the L2 tokens become available on L1. As mentioned though, opting in to a fast FnS withdrawal is fully optional for solvers with no token lock-up for solvers except when they must wait for a slow withdrawal after providing a valid solution. Therefore, it is up to the L2 user to provide a fee covering:\n\nthe cost of waiting to receive the L2 tokens on L1\nthe L1 fees for submitting the solution and collecting the slow withdrawal tokens from the bridge.\n\nHow does it work?\nIn the following, we capitalize the Request, the Solution of the Request, the Users submitting the Request, and the Solvers providing the Solution, to ensure the objects are clearly differentiated from the verbs.\nThe diagram below illustrates how the fast path withdrawal facilitated by Solvers works:\n1188\u00d7883 102 KB\n\n(A) L2 Users wanting fast withdrawals send a Request to the L2 FnS contract, including the tokens the User wishes to withdraw and the L1 address to which the tokens should be sent.\n(B) L1 Solvers (being a Solver is permissionless with no registration requirement) observing the FnS Request in an L2 batch through preconfirmations (or L1 mempool) can attempt to back-run the L2 batch with a Solution to satisfy the withdrawal Request. Solutions send the specified tokens to the specified destination address.\n(C) Solutions can be conditioned on the L2 batch containing the Request, ensuring no reorg risk for Solvers who execute the L2 block: if the L2 block is recorded, the Solution doesn\u2019t execute.\n(D) At any time before a proven L2 state root containing the Request is provided to L1, a fast withdrawal via a Solution can take place.\n(E) After a proven state root containing the Request is executed on L1, the slow withdrawal path is triggered. At that point, the L2 tokens in the FnS contract are available for withdrawal from the L1 FnS contract.\n\n(E-1) If a valid Solution was provided for a valid Request (validity checked in state root proof), the Solver who submitted the Solution (only one Solution per Request gets accepted on L1) can withdraw the tokens from the L1 FnS contract.\n(E-2) Otherwise, the User can withdraw their tokens from the L1 FnS contract.\n\n\n\nTechnical Specification\nThis section specifies the algorithm for FnS withdrawals.\nGlossary of Smart Contracts and Transactions Used\nWe introduce the following terms, concepts, data structures, and smart contracts.\n\nL1 rollup contract. The smart contract on L1 where the rollup/L2 state transitions are recorded and proven.\nL2 transaction batch. The data structure through which L2 transactions are posted to the L1 smart contract representing the L2. Each L2 transaction batch can be accompanied by a signature verifying the signer proposed the batch.\nL2 proposer. With respect to a given L2 batch of transactions, the entity with permission to post the batch to the L1 rollup contract. This proposer role can either be:\n\npermissioned. In the case of centralized sequencer, or preconfirmer, the L2 proposer is the respective centralized sequencer or preconfirmer.\npermissionless. Anyone can sign for and submit the batch.\n\n\nL2 Withdrawal Request: An L2 transaction from an L2 user specifying\n\nRequest ID. A unique identifier for each Request which is a collision resistant commitment to the other variables in this Request, e.g. a hash of the encodings of these variables.\nL2 input tokens. Sent to by the user (token address and amount)\nOutput Condition 1. L1 output tokens (token address and amount)\nOutput Condition 2. L1 output address. Address where the L1 output tokens must be received.\nNonce. Nonce used to generate a unique Request ID.\n\n\nSolver: Entities that \u201csolve\u201d user Requests by satisfying the L1 output conditions of Requests.\nSolution: An L1 transaction from a Solver which satisfies the L1 output conditions of a Request. Although Solvers can solve Requests by any means producing the output conditions, only one Solution is allowed per Request ID. Solutions have the following inputs:\n\n( L2 input tokens, Output Condition 1, Output Condition 2, Nonce)\nRequest ID. Must correspond to the commitment function output results from the previous variables in the Solution. Note, the L1 smart contract does not need to read L2 state to match Solution \u201cRequest IDs\u201d with actual Request IDs. This is the job of the Solver.\nChained L2 batch commitment. A commitment of (previous chained L2 transaction batch commitment, current L2 transaction batch commitment) pair. Needed to prevent double spends of Request input tokens.\nL1 Solver output address. Address where the L2 input tokens of Request ID are to be sent if Request ID was correctly solved.\nCalldata (where the actual solving is done, can be any instruction e.g. send the tokens to the L1 Solver contract, which then sends tokens according to the output conditions).\n\n\nL1 Solver Contract: An L1 smart contract which takes as input Solutions. For each solution, it should record the Request ID to prevent double solving (a bad thing). For each Solution, the L1 Solver Contract must verify that the L2 output conditions specified in the Solution were satisfied.\n(L1-L2) Bridge Contract: A contract that lives on L1. L2 tokens sent to specific burn contracts on L2 can be withdrawn on L1 from the Bridge contract when a proven L2 state root is provided to the bridge which contains those token burns on L2. For this document, we assume this is the only way to withdraw tokens from L2 to L1.\nL2 Solver Contract: An L2 smart contract where Requests are sent. Tokens sent to the L2 Solver Contract are burned and signalled for withdrawal on L1 from the Bridge Contract.\nL1 Solver Withdrawal Transaction: Contains:\n\nL2 State Root ID: ID of a proven L2 state root posted to L1.\nRequest ID.\nMerkle Proof: Proof of existence of Request ID in the state root corresponding to L2 State Root ID.\n\n\nL1 User Withdrawal Transaction: Contains:\n\nL2 State Root ID: ID of a proven L2 state root posted to L1.\nA full L2 Withdrawal Request transaction: (L2 input tokens, L1 output token address, L1 output address, Nonce, Request ID)\nMerkle Proof: Proof of existence of Request ID in the state root corresponding to L2 State Root ID.\n\n\n\nProtocol Description\nWe will now step through the protocol.\n\nL2 User submits a valid Request to L2 Solver Contract.\nL2 proposer includes the Request in an L2 transaction batch, signs the batch.\nSolver reacts to Request in a valid signed batch:\n\nObserve a valid Request in a signed batch.\nConstruct a Solution to the Request (how to satisfy Request output conditions), according to the format specified in the Glossary.\nEither:\n\nSolver creates a bundle of L1 transactions, with the first transaction being the L2 batch containing the Request, and the second being the Solution to the Request\nSolver submits the Solution to a third party, e.g. the L2 proposer, another Solver solving another Request(s), to be bundled together with the L2 batch, with this bundle posted to L1.\n\n\n\n\nL2 transaction batch is received on L1 to the L1 rollup contract.\n\nIf the L2 proposer is permissioned, verify the accompanying signature was from the L2 proposer (NOT the same as verifying msg.sender).\n\n\nSolution received on L1 to L1 Solver contract. For given Solution of the form,\n(L2 input tokens,\nL1 output token address,\nL1 output address,\nNonce,\nRequest ID,\nChained L2 batch commitment,\nL2 solver output address,\nCalldata ),\ndo the following, or revert:\n\nVerify no other Solution has been submitted corresponding to Request ID.\nVerify Request ID matches the commitment of (L2 input tokens, L1 output token address, L1 output address, Nonce).\nVerify there exists L2 batches corresponding to chained L2 batch commitment.\nExecute Calldata.\nVerify Request\u2019s Output Conditions were satisfied.\n\n\nState Root containing Request is proven and submitted with proof to the L1 rollup contract.\nSolver of a Request submits an L1 Solver Withdrawal Transaction to the Bridge Contract. Complete all of the following, or revert:\n\nVerify Withdrawal\u2019s Merkle Proof is valid.\nVerify Solver address matches the L1 Solver Output Address corresponding to the Solution which corresponds to Request ID of the L1 Solver Withdrawal Transaction.\nSend L2 Input Tokens corresponding of the Request corresponding to Request ID to the Solver.\n\n\nUser who submitted a Request sends an L1 User Withdrawal Transaction to the Bridge Contract. Complete all of the following, or revert:\n\nVerify Withdrawal\u2019s Merkle Proof is valid.\nVerify no Solution exists corresponding to Request ID.\nSend L2 Input Tokens of the Request corresponding to the User*.*\n\n\n\nComparisons\nAcross\nThe proposed solution can be viewed as a variant of an intent-based solver solution, similar to protocols like Across. However, a key distinction lies in enabling Solvers to condition their solutions on batches from the source chain, thereby eliminating the reorg risk of the source chain. This is possible because the source chain is a rollup, while the destination is the L1 it settles to\u2014i.e., the source chain batch is submitted directly to the destination chain.\nAggregated Settlement\nAggregated settlement enables synchronous composability between L2s by conditioning one rollup\u2019s settlement on the state of others. This allows a rollup to import messages from another, execute L2 blocks based on them, and later revert execution if the messages are found invalid by verifying the other rollup\u2019s state root at the shared settlement time. This protocol effectively enables synchronous composability between L2s by utilizing the fact that the L2s share a settlement layer. However, aggregated solutions do not address L2\u2192L1 composability, as the L1 cannot reorg based on the settled state of an L2.\n", "\nBlock Arrivals, Home Stakers & Bumping the blob count\n\nThanks to all the community members that are selflessly sharing their data that allowed this analysis to be possible, to MigaLabs for their validator entity data and to ProbeLab for an early version of their Mainnet bandwidth report.\nSpecial thanks to Parithosh, Toni, Mikel, Andrew, and Matty for their feedback and help throughout this analysis.\n\nTL;DR\nSummary\n\nTiming games have historically made it hard to predict a safe blob count bump\nethPandaOps recently started receiving block arrival timing data from the community\nAnalysis of the most bandwidth sensitive scenario shows a healthy block & blobs arrival time\n\nOutcomes\n\nWhen naively extrapolating this data and combining with EIP7623, this analysis supports increasing the blob count in EIP7691 to either 4/8 or 6/9.\n\nThis is only one data point, but it leads to a more optimistic outlook on a potential blob count bump than previous analysis.\n\n\n\nIntro\nThe ethPandaOps team has recently started receiving data from members of the community. Home stakers are one of the Ethereum network\u2019s most valuable assets, and this scheme is starting to shine a light on how they see the network. A sidecar binary is run alongside a beacon node, and records the events happening on the node via the beacon API event stream. These events are sent to the ethPandaOps team, who then publishes the data (with a small delay & some data scrubbing).\nFor more info:\n\nData collection\nAccessing the data\n\nThe team has been collecting community data for around 6 weeks, and we now have enough data to make some interesting observations that were previously not possible.\nBackground\nWith the arrival of EIP4844, a block is only considered valid once the block & the blobs referenced in the block are received by a node. This block/blob bundle has until 4s in the slot to be received by the majority of the network otherwise it runs the risk of being re-orged.\nSophisticated operators (and now MEV-Relays) play block proposal timing games. These operators submit their blocks as late as possible to maximise their profit while minimizing their risk of being re-orged. These timing games have historically obfuscated block arrival timing data.\nUnreleased, upcoming research from ProbeLab indicates that the 50th percentile of non-cloud nodes on the network have 20Mbps of upload bandwidth.\nBlob usage on Mainnet continues to grow.\nProblem statement\nEthereum\u2019s decentralization is paramount to it\u2019s success. EIP7691 aims to increase the blob count, and runs the risk of unintentionally excluding some node operators from it\u2019s set if the parameters are too high.\nWe need to:\n\nEnsure that a blob count increase is safe for home stakers, as this group of actors is the \u201cworst case\u201d for a blob count increase as they have the lowest available bandwidth.\nEnsure that the network has enough data throughput to support Layer 2 growth.\n\nGiven the existing landscape, we can make some assumptions with regards to a potential blob count increase:\nLeast at risk:\nCounterintuitively, if you looked at block arrival data, operators playing timing games would appear to be the most at risk of being impacted by a blob count increase. Being reorged out for proposing late is bad for business, and we can assume they\u2019ll adjust their block proposal timings accordingly. A blob count increase is unlikely to be problematic here.\nMost at risk:\nA solo staker building a block locally (no mev-relay) and being attested to by other home stakers. In this scenario:\n\nThe proposer:\n\nneeds to publish their block and all blobs to the network. This node needs to publish the block (~100KB), and then all of the blobs (128KB each) to all of its mesh peers as fast as possible.\n\nwhen building blocks locally, the proposer does not have the help of the MEV Relay gossiping the block/blobs bundle to its own peers.\n\n\n\n\nThe attesters:\n\nneed to download the block/blobs bundle from the p2p network before 4s in to the slot.\n\n\n\nThis analysis will ask the following questions:\n\nQuestion 1: How is 3/6 performing on Mainnet?\nQuestion 2: Does arrival time scale with block/blob bundle size?\nQuestion 3: How much more can we support on Mainnet today?\n\nWe\u2019ll answer these questions from the perspective of a home staker as this is our most at-risk group of operators.\nAnalysis\nStart: 2024-10-04T22:00:00Z\nEnd: 2024-11-25T02:00:00Z\nBlocks: 366,384\nArrival events: 75,945,392\nCountries: 9 (Australia, Bulgaria, Czechia, Germany, Italy, Spain, The Netherlands, United Kingdom, United States)\n\nCheck out the juypter notebook here.\nThe timing data was captured by Xatu Sentry, a sidecar binary that runs alongside a beacon node and records many events via the beacon API event stream. These events can be considered worst case for block arrival times as there is additional processing time and network overhead for the event to be recorded. From analysis, this overhead ranges between 50-300ms but we have kept the timing data as-is in the interest of safety.\nEach beacon node implementation emits block, head, and blob_sidecar events in different orders. To address this, we define a block/blob bundle as \u201carrived\u201d only after all associated events for the slot have been recorded from each beacon node. This is once again a worst case scenario.\nQuestion 1\nHow is 3/6 performing on Mainnet?\n\nTL;DR: Pretty well!\n\n1166\u00d7804 115 KB\nThis chart shows block/blob bundle seen arrival times against the combined size of the bundle. When looking at locally built blocks proposed by solo stakers and seen by home users, a lot of block/blob bundles are seen before 4s.\n1166\u00d7804 118 KB\nWe should also look at blocks provided by MEV Relay as the reality is that a lot of blocks are proposed via this route. We can see an increase in the min, as there are additional round trips involved in this process, but things still look healthy!\nOutcome: Block/blob bundles are arriving well within the 4s deadline for our home users.\nQuestion 2\nDoes arrival time scale with block/blob bundle size?\n\nTL;DR: Yes\n\n3530\u00d72411 650 KB\nThe trend lines show the 99th, 95th, and 50th percentiles of arrival times - meaning what percentage of blocks arrive were seen faster than that line.\nThese percentile trend lines answer our question: as bundle sizes increase, arrival times also increase. This suggests bandwidth is the primary bottleneck for these actors.\n3530\u00d72411 651 KB\nAgain looking at blocks provided via MEV Relay, we see a similar story.\nOutcome: Yes, arrival times scale with block/blob bundles size\nQuestion 3:\nHow much more can we support on Mainnet today?\n\nTLDR: 4/8 and 6/9 are both achievable\n\nTo answer this question we need to check how big the block is. The 99th percentile of compressed beacon block size through our time period is 101KB. Our blobs are fixed at a size of 128KB.\nUsing these parameters, we can overlay the block/blob count:\n3530\u00d72411 918 KB\nWe can very naively plot a trend line to see when would cross the 4s attestation deadline.\n3530\u00d72411 1 MB\nThis trend line assumes a linear relationship between blob count and arrival time. Under this assumption, we can support up to 14 blobs per block while maintaining 95% of block/blob bundles arriving within the deadline. The 95% target provides margin for the 50-300ms processing overhead in our measurements, while also accounting for outliers.\n3530\u00d72411 1020 KB\nWhen looking at MEV Relay blocks specifically, the data shows an even more optimistic picture - the 95th percentile trend line indicates support for up to 20 blobs per block. This improved performance can be attributed to MEV Relays being high-bandwidth nodes that help distribute blocks and blobs across the network in parallel with the proposer.\nEIP7623\nEIP7623 improves the worst case compressed block size to ~720KB - about 7x larger than our average historical block. Let\u2019s analyze if we can still support more blobs with this increased block size.\n3530\u00d72411 967 KB\nEven with an absolute worst case block size with EIP7623 we still support a blob increase. Note that the current maximum compressed block size on Mainnet is 1.79MB (and we\u2019re seemingly going ok!), so take this data point with a grain of salt.\n1175\u00d7804 208 KB\nThe trend for MEV Relay blocks again supports a much higher blob count compared to locally built blocks.\nOutcome: The data supports a blob count increase, especially if EIP7623 is included at the same time. 4/8 or 6/9 are both safe to apply. There is potential for a higher blob count, but we\u2019ll need to see how the network performs with an initial bump first.\nConclusion\nEthereum\u2019s decentralization is fundamental, and home stakers play a crucial role in this picture. The network is a delicate and intricate system that demands thoughtful and deliberate consideration.\nOur analysis indicates that block arrival performance surpasses initial expectations for nodes with limited bandwidth. The community-contributed data offers valuable real-world insights into the network\u2019s capabilities, and we would like to once again thank those who are contributing their data.\nWhile we naively assumed a linear relationship between blob count and arrival time, this is a simplified view of a highly complex distributed system. Additionally, there are ongoing work streams that could either improve or worsen bandwidth requirements over time. Our analysis is focused on the data available to us now, based on observations from the past six weeks of network performance.\nBased on block/blob arrival metrics alone, increasing the blob count from (target 3/max 6) to (target 4/max 8) or (target 6/max 9) appears to be viable. However, this is just one of many factors to evaluate when deciding on blob count adjustments.\n", "\nOn solo staking, local block building and blobs\n791\u00d7785 121 KB\nIn recent weeks, discussions about a potential increase in blob throughput in Pectra have intensified, with two distinct groups emerging. One advocates for the increase, while the other is hesitant, preferring to wait for clear data supporting such a change.\nFrom my perspective, one sentiment is overwhelmingly clear within the community:\n\n\\textbf{Solo stakers are at the heart of Ethereum.}\n\nWhile there hasn\u2019t been a consensus on the minimum requirements for validators (see sassal.eth\u2019s tweet on that), the Ethereum community has made one thing clear:\n\n\\textbf{We do not want to sacrifice solo/home stakers for additional linear scaling.}\n\nIn my view, this reflects a healthy direction for Ethereum and underscores the community\u2019s view on the importance of viable solo staking.\nHowever, it raises an important question: \u201cWhere is the line?\u201d\nSpecifically, at what point does the contribution of a weaker, lower-bandwidth staker to decentralization no longer justify the limitations it imposes on Ethereum\u2019s ability to scale?\nIn this piece, I aim to provide additional data points to help the community make an informed decision on whether we want to pursue a blob throughput increase in Pectra.\n\nAs Potuz, a core developer from Prysm, aptly stated, the real question is not \u201cDo we want to scale, and how?\u201d but rather, \u201cAre we ready to do so now?\u201d\n\nWho is being reorged today?  (Oct 2023 - Oct 2024)\n1000\u00d7500 33.8 KB\n\nOn average ~0.2% of blocks are reorged (=reorged \u2286 missed).\nProfessional node operators (NOs) such as Lido, Kiln, Figment, and EtherFi are reorged less often than the average.\nLess professional NOs such as solo stakers, Rocketpool operators, or the unidentified category which likely includes many solo stakers that couldn\u2019t be identified, are more frequently reorged.\n\n\nAs shown in an earlier analysis, the reorg rate has been trending down since the Dencun hardfork.\nIn the following chart, we can see that this effect was different for different entities:\nreorgs_entites_over_time (3)1000\u00d7500 53.4 KB\n\nThe reorg rate decreased for solo stakers and unidentified since Dencun.\nThe same applies to Rocketpool operators, as well as larger operators such as Lido, Coinbase, Figment, and OKX.\n\nWhat about local block building? (Oct 2023 - Oct 2024)\n1000\u00d7500 17 KB\n\nLocal builders have a reorg rate of approximately 1.02%.\nMEV-Boost builders have a reorg rate of approximately 0.20%.\nLocal builders are approximately 5 times more likely to be reorged than MEV-Boost builders.\n\n\n1000\u00d7500 32.9 KB\n\nThe reorg share for local block builders seems to have remained constant or even increased after the Dencun hardfork.\nFor MEV-Boost users, reorgs have been trending down since Dencun.\nNotably, previous analysis showed that local builders included on average more blobs into their blocks. Furthermore we have seen that right after the Dencun hardfork blocks with 6 blobs struggled a bit, but this eventually stabilized again. This might explain why the reorg rate didn\u2019t decrease for local builders.\n\nWho are the local builders? (Oct 2023 - Oct 2024)\n1200\u00d7300 13.7 KB\n\nSolo stakers (here labeled as \u201csolo staker\u201d but with many solo stakers in the unidentified category) are the largest entity within the \u201clocal builder\u201d category.\nFurthermore there are Lido NOs that are not using MEV-Boost at all or use the min-bid flag.\n\nKey Insights\n\nSolo stakers tend to miss more slots compared to professional validators.\nSolo stakers often build their blocks locally rather than using MEV-Boost.\nLocal block builders don\u2019t benefit from the fast propagation offered by MEV-Boost relays.\nRelays engage in timing strategies (e.g., relay delays, allowing time to wait for even more profitable blocks).\nEpoch boundaries contribute to an increase in reorgs.\n\nMultiple factors can lead to reorgs, making it challenging to pinpoint exactly why certain validators, like solo stakers, experience them more frequently than others.\n", "\nZero-knowledge proofs of identity using electronic passports\n282155110-514ae671-3c02-434f-ac6a-31ce20eec24d1792\u00d7401 181 KB\nMany applications need to verify their user\u2019s identity online, whether it is nationality, age, or simply uniqueness. Today, this is hard. They are stuck between shady heuristics like tracking IP addresses and technologies like Worldcoin that need to deploy their infrastructure widely.\nFortunately, UN countries in association with the International Civil Aviation Organization have built a great tool for us to piggyback on: electronic passports. They are issued by more than 172 countries and include an NFC chip with a signature of the person\u2019s information, including name, date of birth, nationality and gender. Issuing countries make their public keys accessible in online registries, enabling the verification of signatures.\nA circuit for passport verification\nFor someone to prove their identity using a passport, they will have to do two things. First, read the content of their passport\u2019s chip. This can be done easily with any NFC-enabled phone. Then, show a verifier that their passport has been correctly signed. Instead of sending all of their personal data for the verification to happen, they can generate a zero-knowledge proof that redacts some of their inputs.\nOur circuit will have to checks two things:\n\nThe disclosed attributes have been signed correctly\nThe corresponding public key is part of the public key registry of UN countries\n\nA simple circuit compliant with the electronic passport specs would look something like this:\n\nB1_4A2ZxC.png2483\u00d71669 96.6 KB\n\nHere is roughly what happens:\n\nEach datagroup stored in the passport contains some of the person\u2019s information. The datagroups we are most interested in are the first one (nationality, age, etc) and the second one (photo). The circuit takes them as inputs along with the signing public key.\nDatagroups are hashed, concatenated and hashed again.\nThe final result is formatted, hashed and signed by the country authority. We can use the public key to check this signature.\n\nThis makes the following attributes disclosable: name, passport number, nationality, issuing state, date of birth, gender, expiry date, photo.\nSome countries also provide additional data like place of birth, address, phone number, profession and a person to notify. Biometrics like fingerprint and iris are sometimes included but can\u2019t be retrieved, as they require a special access key.\nIn practice, we want our circuit to have a few other features:\n\nInstead of passing the country\u2019s public key directly, we want the user to prove that the public key that signed their passport is part of the registry published by the ICAO. This can be done by passing a merkle proof of inclusion and having only the merkle root as a public input.\nTo allow for selective disclosure of any attribute, we pass a bitmap as a public input that will redact some of the attributes.\nWe want specific modules for age disclosure and nationality list inclusion. A range check can guarantee someone is above a certain age without disclosing the precise age, and an inclusion check can be done over a set of countries to prove someone is or is not a citizen of any country in a list.\nFor applications like minting an SBT or voting, we want to check that the passport is not expired. This can be done by passing the current date and doing a range check over the date in the circuit. We can then check that the current date is correct using the block timestamp in a smart contract or server-side in offchain verification.\nFor applications that need sybil-resistance, we want to store a nullifier that prevents using the same passport twice. The simplest approach involves storing a hash of the government\u2019s signature, though this does not render the individual anonymous from the government\u2019s perspective. There are other approaches, see here for a discussion of the tradeoffs.\n\nA map of a more complete circuit can be found here.\nOne of the challenges is the number of signature algorithms used. Most countries use common ones like RSA with SHA256, but the ICAO specifications are quite permissive and some countries chose to use hash functions like SHA512 or unusual padding formats. We currently support the most common one and we are working on adding support for more.\nApplications\nApplications roughly fall into three categories: proof of humanity, selective disclosure and authentication.\nProof of humanity can be used in general for sybil resistance. This includes voting, fair airdrops, quadratic funding and helping social media fight bots. If passports can\u2019t be construed as a general solution today, they can be integrated into wider systems like Gitcoin Passport or Zupass.\nSelective disclosure has applications like privacy preserving age check. Some countries restrict buying alcohol, drugs or entering casinos for minors, and zk could help bringing better privacy to those controls.\nAnother example of selective disclosure is proving one is not a citizen of any country in a set of forbidden countries. This could help creating an intermediate level of compliance between KYC-gated traditional finance and fully permissionless DeFi.\nUsing passport signatures for authentication, one can build a ERC-4337 recovery module that asks for a proof from a specific passport as one of the conditions for recovery. Some passports also support Active Authentication, meaning they have their own private key and the ability to sign data. This would make them suitable for direct transaction signing, either for small transactions or in a multisig setup with other signers.\nLimitations\nThe most obvious limitations of using passport signatures are the following:\n\nThe passport does not do any kind of biometric check when the chip is read. Therefore there is no straightforward way to know if the passport has not been borrowed or stolen.\nMost of the world population does not have a passport. Even in the US, only around 50% of the population owns a passport.\nIssuing authorities can create an arbitrary number of passports and cheat in systems that require passports for sybil resistance.\nPassports can be lost or revoked. Some countries allow citizen to keep their previous passport when they are issued a new one. Some people have dual citizenship. All those cases are hard to mitigate, as the signatures stay valid.\n\nThose limitations are all quite fundamental to the way passports work today. They can be addressed by aggregating attestations from multiple sources, which will be covered in a future post.\nCurrent state\nProof of Passport is fully open source, from mobile app to circuits. If you are interested in contributing, please check open issues.\nWhile performance would have been a bottleneck a few years ago, work from teams like Polygon ID, arkworks and mopro have made client-side proving on smartphones quite fast. Generating a proof with the current circuit takes ~4 seconds on a recent iPhone.\nWe are currently focused on shipping the mobile app for the first integrations. It allows users to mint an Soulbound Token disclosing only specific attributes they chose, or none at all other than the validity of their passport. Contact us to try out the beta release.\nThanks to R\u00e9mi, Andy, Aayush, Youssef and Vivek for contributing ideas and helping build this technology!\n", "\nBy Cairo & Diego\nMany thanks to Vitalik Buterin, Alexander Herrmann, Philogy, William X, Thomas Thiery, and Julian Ma for feedback and discussions.\n1024\u00d71024 106 KB\nSolo validators are vital to Ethereum\u2019s security, decentralization, and censorship resistance. They operate independent nodes distributed around the world that cannot be easily coerced. However, they face key challenges:\n\n\nHigh entry barriers: The 32 ETH staking requirement is prohibitively high for many potential participants.\n\n\nOpportunity costs: Significantly more capital is locked up than what\u2019s likely to be lost in cases of validator misbehavior.\n\n\nLimited liquidity: Solo validators can\u2019t easily borrow against their stake and obtain leverage while still operating the validator.\n\n\nTo address these issues, we propose SOLO\u2014a protocol that enables a permissionless liquid staking token (LST) minted against the portion of a validator\u2019s stake that\u2019s unlikely to be lost, even in cases of penalties and slashing. Following the Pectra update, this could be as much as 96.1% of the stake, barring significant concurrent mass slashing events and long inactivity leaks. This would reduce entry costs for solo validators by a factor of 25.6, to just 1.25 ETH. The protocol would also enable solo validators to borrow efficiently against their otherwise illiquid stake.\nOur mechanism achieves a single, fungible LST across all validators without relying on governance, trusted hardware (e.g., SGX), or permissioned operator sets. The protocol leverages EIP-7002 (Execution Layer Triggerable Withdrawals) and EIP-7251 (\u201cMaxEB\u201d) from Pectra. Validators need only point their withdrawal credentials to the protocol.\nTo counter a potential decrease in the cost of a 51% attack on Ethereum and discourage dominance by single validators, the protocol dynamically limits the achievable leverage using an economic anti-concentration mechanism that disproportionately discourages larger validators. This aims to prevent any single entity from gaining excessive control over Ethereum through the protocol.\nBackground\nEthereum\u2019s protocol uses two types of negative incentives for validators: penalties and slashing.\nPenalties are applied for missed attestations and sync committee duties. They\u2019re relatively mild\u2014a validator can recover from one day of downtime with about one day of uptime.\nSlashing, however, is more severe. It\u2019s applied for serious protocol violations, such as proposing or attesting to multiple blocks for the same slot. This punishment consists of four parts:\n\n\nAn initial penalty: Post-Pectra, this would be 1 ETH per 4,096 ETH of effective balance. For a 32-ETH validator, the loss would be 0.00781 ETH.\n\n\nA correlation penalty for mass slashing events: Typically zero, this penalty could potentially wipe out a validator\u2019s entire balance if a third or more of the total stake is slashed within the 18 days before and after the validator\u2019s slashing. In a scenario where 1% of the total stake is slashed, the correlation penalty would be at most 0.960 ETH for a 32-ETH validator.\n\n\nPenalties for missed attestations: Ethereum automatically invalidates all attestations from the slashed validator until they become withdrawable, which occurs after 8,192 epochs. This penalty would amount to 0.0564 ETH for a 32-ETH validator today.\n\n\nAn inactivity leak penalty: If the chain is not finalizing, slashed validators incur additional costs. A 32-ETH validator would suffer a loss of 0.0157 ETH for a leak spanning 128 epochs.\n\n\nIf up to 1% of the total stake is slashed and inactivity leaks amount to no more than 128 epochs, slashing a 32-ETH validator would result in a loss of at most approximately 1.04 ETH\u2014just 3.25% of the balance.\n2482\u00d71104 131 KB\nThis means that, outside of extraordinary mass slashing events, most of the ETH held by a single independent validator is not at risk even if that validator can misbehave arbitrarily.\nPrior Work\nSeveral approaches have explored LSTs in this context:\n\n\nJustin Drake proposed an LST mechanism for solo validators that relies on trusted hardware (SGX) to prevent slashing penalties.\n\n\nDankrad Feist proposed a two-tiered staking system with separate slashable and non-slashable capital tiers.\n\n\nLido allows ETH holders to \u201clend\u201d their ETH to node operators without giving those node operators the ability to steal their funds. Their Community Staking Module supports permissionless participation by solo stakers who put down a bond.\n\n\nRocket Pool enables solo validators to \u201cborrow\u201d up to 24 ETH for each 8 ETH they provide. However, they must also stake RPL tokens\u2014at least 10% of the borrowed amount\u2014and share validator rewards proportionally.\n\n\nfrxETH also allows solo anonymous validators to borrow up to 24 ETH for each 8 ETH they provide, but the borrowed ETH can only be used to create new validators.\n\n\nSOLO, however, offers solo validators unique advantages:\n\n\nIt allows them to create validators with as little as 1.25 ETH.\n\n\nIt enables minting of a single, fungible LST for any validator without relying on trusted hardware, governance, a permissioned operator set, or staking of an additional token.\n\n\nIt compensates LST holders for the time value of their ETH and the risk of bad debt through a market-based funding rate.\n\n\nIt requires no changes to Ethereum, except for EIP-7002 and EIP-7251, both in Pectra.\n\n\nIt dynamically counterbalances the potential risk it poses to Ethereum of a 51% attack, using a mechanism that disproportionately discourages larger validators from accumulating too much stake.\n\n\nProtocol Overview\nThe mechanism draws inspiration from synthetic stablecoin systems like RAI. Node operators act as borrowers, minting an LST called SOLO against validators\u2019 staked ETH, while SOLO holders serve as lenders. If a validator\u2019s SOLO \u201cdebt\u201d becomes too high relative to their stake, they\u2019re liquidated and force-withdrawn. A dynamic funding rate compensates SOLO holders for the time value of the underlying ETH and the risk of bad debt, making the token trade close to 1 ETH.\nThe protocol defines the loan-to-value (LTV) of active validator i, whose withdrawal credentials point to the protocol, as follows:\n\nLTV_i = \\frac{debt_i}{collateral_i}\n\ndebt_i is the amount of the SOLO minted against validator i, including any unpaid funding, and collateral_i is validator i's effective balance in Ethereum\u2019s consensus layer.\nA validator\u2019s LTV can increase due to SOLO minting, funding, penalties, or partial withdrawals, raising their liquidation risk. Conversely, their LTV can decrease due to validator rewards and additional deposits, lowering this risk.\nLTV_{max} is the maximum allowed LTV after SOLO minting and partial withdrawals. It is strictly less than 100% and also caps the system\u2019s achievable leverage by limiting how little operators can put forward relative to the validator\u2019s stake size.\nLTV_{liq} is the liquidation threshold\u2014the highest LTV before a position becomes eligible for liquidation and forced withdrawal\u2014and must be at least LTV_{max}. Its value should account for potential losses during delays between liquidation eligibility and execution (e.g., if the validator is a proposer and censors the liquidation) and during the exit queue following the forced withdrawal.\nH_i is the health factor of validator i, indicating how close it is to liquidation eligibility, if it has any debt:\n\nH_i= \\frac{LTV_{liq}}{LTV_i}\n\nA value of H_i below 1 makes the validator eligible for liquidation.\n1788\u00d71368 84.9 KB\nFollowing the analysis in the Appendix, we could estimate values of 96.1% for LTV_{max} and 96.4% for LTV_{liq}.\nMinting\nTo mint SOLO, the withdrawal credentials of the validator must point at a protocol-controlled surrogate contract. Surrogate contracts are used to attribute withdrawals from the consensus layer. If the validator\u2019s withdrawal credentials haven\u2019t been migrated to 0x01 yet, the operator should migrate them and set them to the surrogate contract.\nIf its withdrawal credentials point at the contract, a validator can be registered by the operator by calling register. The operator can then call mint to mint SOLO against the validator.\nThe protocol should also offer a method deploy that atomically transfers 32 ETH from the caller, deploys a new validator via Ethereum\u2019s deposit contract, with withdrawal credentials set to the surrogate contract, registers it, and mints SOLO.\nWithdrawing\nOperators initiate the full withdrawal of the validator by triggering a voluntary exit in the consensus layer or by calling a function withdraw, which triggers the exit of the validator. Partial withdrawals can only be triggered through withdraw, which ensures that the resulting LTV would be less than LTV_{max}. These functionalities, as with liquidation, depend on EIP-7002.\nOnce the withdrawal completes, the operator calls a method claim to receive the ETH from the stake. If this were a full withdrawal, the operator must have first paid for any outstanding debt against the validator using method repay.\nLiquidating\nA validator becomes eligible for liquidation when its health factor drops below 1, which occurs when its LTV exceeds the liquidation threshold. Once eligible, anyone can trigger the liquidation process by calling method liquidate on the validator. This call then triggers the withdrawal of the validator.\nAfter the withdrawal completes, the contract auctions the received ETH for SOLO to pay back the validator\u2019s debt. Any excess ETH can be returned to the validator (or kept as a liquidation fee by the protocol). As LTV_{max} accounts for slashing penalties, the received ETH should suffice to cover incurred losses. In the event that this is not enough, SOLO holders would incur the costs from the bad debt.\nSlashing\nAt the end of the slashing process, the protocol receives any remaining stake of the validator after penalties have been applied. The protocol then auctions this ETH for SOLO in order to cover the slashed validator\u2019s debt, following a similar process as a liquidation.\nFunding Rates\nA mechanism is needed to compensate SOLO holders for the time value of their ETH, as well as for the risk of any bad debt (e.g., from a significant mass slashing event or prolonged inactivity leak). Otherwise, SOLO wouldn\u2019t trade close to 1:1 with ETH, which is essential for the protocol.\nWe propose using a dynamic, market-based funding rate. SOLO minters (debtors) pay this rate to SOLO holders (lenders). It works by proportionally increasing debts, with SOLO holders benefiting through continuous token rebasing (akin to Lido\u2019s stETH daily rebasing). If the SOLO price falls below 1 ETH, the funding rate rises, making SOLO holding more appealing. If the SOLO price rises above 1 ETH, the funding rate falls, making SOLO holding less attractive. In extreme cases where the SOLO price persistently remains below 1 ETH, the funding rate would eventually rise so high that all positions would be liquidated.\nThis funding rate can be implemented in various ways. For real-world examples, see the funding rate used in RAI and Squeeth. However, unlike with those mechanisms, the SOLO funding rate can be floored at 0%, avoiding the need for negative rebasing. To prevent the possibility of a sustained depeg in which SOLO trades at higher than 1 ETH even when rates are 0%, the protocol can allow unlimited minting of SOLO against native ETH 1:1 whenever the funding rate is at 0%. Anyone can then redeem SOLO for this ETH at a 1:1 ratio. This reserve of ETH must be emptied before the funding rate can rise above 0% again.\nIn practice, any increase in protocol returns would likely trigger a surge in SOLO demand, prompting a rise in the funding rate. The reverse holds as well. This self-balancing mechanism becomes even more efficient due to the low entry costs. Nevertheless, SOLO enables solo validators to deploy their borrowed funds into higher-yielding ventures while still operating the validator\u2014a potentially profitable strategy if those returns exceed the funding cost when combined with the validator rewards.\nLowering Capital Requirements\nThe protocol leverages flash loans to significantly lower the capital requirements for solo validators.\nCreating a validator\nSolo validators would call a special function, flashDeploy, with an amount of ETH, S, as little as  32 (1-LTV_{max}), which does the following:\n\n\nBorrows (32 - S) ETH from a fee-free flash-loan provider to reach 32 ETH with S;\n\n\nCalls deploy with 32 ETH, along with validator information, to mint (32 - S) of SOLO;\n\n\nExchanges (32 - S) of SOLO for (32 - S) ETH in a decentralized exchange (assuming no price impact);\n\n\nRepays the (32 - S) ETH debt with the flash-loan provider.\n\n\n2042\u00d71302 88.7 KB\nSolo validators must then maintain a health factor of at least 1 to avoid liquidation, as this validator would accumulate funding while it\u2019s active.\nFor an LTV_{max} of 96.1%, the minimum required amount would be approximately 1.25 ETH (i.e., 32  (1 - LTV_{max})), reducing the entry cost for solo validators by a factor of 25.6.\nWithdrawing a validator\nTo withdraw the initial amount deposited, S, and rewards, solo validators only need to cover any additional debt accrued, \\Delta debt \u2014not the full debt of the validator for (32 - S) SOLO. They\u2019d initiate this process by triggering the full withdrawal of the validator in the consensus layer or through function withdraw. After the withdrawal is completed, they\u2019d call function flashClaim, which does the following:\n\n\nTransfers \\Delta debt SOLO from the solo validator to the protocol.\n\n\nBorrows (32 - S) ETH from a fee-free flash-loan provider;\n\n\nExchanges (32 - S) ETH for (32 - S) SOLO in a decentralized exchange (assuming no price impact);\n\n\nCalls claim to settle the validator\u2019s debt of (32 - S + \\Delta debt) SOLO;\n\n\nRepays (32 - S) ETH debt with the flash-loan provider;\n\n\nReturns any ETH left, (S + rewards - penalties), to the solo validator.\n\n\n2042\u00d71640 105 KB\nIf the withdrawn amount is less than the validator\u2019s debt, the protocol incurs bad debt. This scenario, resulting from potential significant validator losses during mass slashing events or prolonged inactivity leaks, would render flashClaim inoperable as there wouldn\u2019t be enough ETH to repay the flash-loan. However, since solo validators only receive the ETH left after debt repayment, their expected return in this scenario would be zero, meaning that no ETH remains locked for them.\nAnti-Concentration: Discouraging Large Validators\nBy reducing the cost of creating a validator, the protocol fosters decentralization and the proliferation of small validators, but it also may reduce the costs of accumulating very large amounts of stake and even potentially executing a successful 51% attack on Ethereum. While the protocol\u2019s benefits are theoretically available to both attackers and honest users, coordinated attackers may be more likely to exploit this advantage effectively.\nTo counterbalance this risk, we propose an anti-concentration mechanism: a dynamic leverage adjustment that automatically decreases LTV_{max} as the protocol\u2019s share of the total ETH staked grows. One possible formula for this adjustment would be the following:\n\nLTV_{max}=LTV_{max}^{t_0}(1-\\frac{total\\,ETH\\,staked\\,through\\,SOLO}{total\\,ETH\\,staked\\,on\\,Ethereum})\n\nwhere LTV_{max}^{t_0} is the initial maximum LTV (e.g., 96.1%).\nThis mechanism serves two purposes.\nFirst, it helps ensure that SOLO does not dominate Ethereum staking. If SOLO ETH becomes too large a share of all ETH staked, then SOLO will stop providing significant leverage.\nBut an additional benefit of the mechanism is that it helps ensure that SOLO itself is not dominated by a few large stakers. It does this by disproportionately discouraging large validators from adding more stake.\nConsider the \u201cmarginal LTV \u201d for an individual validator\u2014the ratio of the amount of SOLO that they can mint for each new unit of ETH they put in as collateral. For anyone who is not yet using the protocol, their marginal LTV is equal to LTV_{max}. But for anyone who is already a staker using the protocol\u2014particularly a large one\u2014the calculus is different. Because of the anti-concentration mechanism, each additional unit of collateral they deposit increases the LTV for all of their stake, requiring them to deposit additional collateral to support all of their existing SOLO debt. This means the marginal LTV for this large validator is actually lower than the marginal LTV of a small validator.\nTo calculate the marginal LTV, we can start with the formula for the maximum amount of debt that a user, Alice, can take out, debt_{max}, as a function of the amount she has staked through SOLO, C_{Alice}. This is expressed partly in terms of the total amount of ETH that other stakers are staking through SOLO:\n\ndebt_{max}=LTV_{max}^{t_0} (1-\\frac{other\\,ETH\\,staked\\,through\\,SOLO\\,+\\,C_{Alice}}{total\\,ETH\\,staked\\,outside\\,SOLO\\,+\\,other\\,ETH\\,staked\\,through\\,SOLO\\,+\\,C_{Alice}}))\\,C_{Alice}\n\nThe user\u2019s marginal LTV is just the derivative of that formula with respect to the user\u2019s collateral, which works out to:\n\nLTV_{marginal}=LTV_{max}-LTV_{max}^{t_0} (\\frac{total\\,ETH\\,staked\\,outside\\,SOLO}{total\\,ETH\\,staked\\,on\\,Ethereum})(\\frac{C_{Alice}}{total\\,ETH\\,staked\\,on\\,Ethereum}))\n\nThe upshot of this formula is that users who already have a large amount of collateral being used in the system face a lower marginal LTV than smaller users.\nThe chart below shows both effects of the anti-concentration mechanism. The red line shows the marginal LTV for a small staker considering using SOLO, as a function of the percent of total stake that is using SOLO. The blue line shows the marginal LTV for a staker who already controls half of the stake in SOLO.\n1208\u00d71130 75.8 KB\nImportantly, this mechanism does not depend on any kind of identity-based Sybil resistance. Attackers can\u2019t avoid it by splitting their holdings across multiple validators or by consolidating large amounts of ETH into single validators. This anti-concentration mechanism provides an economic disincentive for large stakers to accumulate stake through SOLO.\nConclusion\nSOLO addresses key challenges faced by solo validators: high entry barriers and limited liquidity for their stake. All in all, we believe this mechanism could make solo validating significantly more attractive.\nAppendix\nA 32-ETH validator would incur a maximum loss of 0.00867 ETH if offline for an entire exit queue lasting up to 5.6 days as of today. If the validator\u2019s liquidation was delayed by a day, the maximum loss would increase to 0.0102 ETH. Should inactivity leaks occur during this process for up to 128 epochs, the loss would rise to 0.0259 ETH. Moreover, if the validator is slashed during the withdrawability delay period following the exit queue, incurring at most penalties for 1.04 ETH, the total loss would reach 1.07 ETH.\nTo prevent immediate liquidation when funding accrues, we introduce a buffer between LTV_{max} and LTV_{liq}. This buffer, set at 0.3% of the borrowed amount, would mean that LTV_{liq}=1.003\\cdot  LTV_{max}.\nThe maximum loss LTV_{liq} can tolerate without causing bad debt, as a function of LTV_{max} is 32 (1-LTV_{max}). Thus,\n\nLTV_{liq} \\leq 1-\\frac{potential\\,loss}{32}\n\nNow, potential\\,loss=penalties + buffer= 1.07 ETH +\\,0.003 \\cdot borrowed\\, amount. Since borrowed\\, amount is 32\\cdot LTV_{max} ETH, the buffer is 0.003( 32\\cdot LTV_{max}).\nConsequently, substituting back into the equation gives the following:\n\nLTV_{liq} \\leq 1- \\frac{ 1.07 + 0.003  (32\\cdot LTV_{max})}{32}\n\nSolving this inequality yields maximum values of \t\\approx 96.1% for LTV_{max} and \\approx 96.4% for LTV_{liq}.\n", "\nAn Introduction to Blockchain Mechanism Math, Terminology, and Hieroglyphics for Deeply Casual People who want to Sound Smart when Discussing White Papers with their Peers\nBy Alex Watts et al\n\nAbstract: Mechanism Designers working in DeFi really like writing white papers. They like it a lot. Perhaps too much. They also like writing these papers in an ancient, hieroglyphics-based language that is difficult for a normal, socially-adjusted person to decipher. This is unfortunate because most of the math and game theory is actually quite simple once you understand their alien terminology. In this paper, I\u2019ll give you the tools you\u2019ll need to sound smart when discussing these white papers with your peers, which is a very important skill set because in all likelihood your peers are also trying really hard to sound smart.\nLet\u2019s start with this section - the \u201cabstract.\u201d  This section is really just a summary of the white paper. In this section, the Mechanism Designers will summarize a problem, then they\u2019ll summarize their solution, and then - if they\u2019re good - they\u2019ll also summarize the shortcomings of the paper. We may never know why the Mechanism Designers like to call this section an \u201cabstract\u201d rather than a \u201csummary\u201d - in fact, we\u2019ll probably never know why they do most of the things that they do. But by reading this paper, hopefully you\u2019ll be better able to understand what they\u2019re doing - and, if you\u2019re into this kinda thing, who they\u2019re doing it to.\nIntuitively, this paper doesn\u2019t have any shortcomings. But if I had to pick only one, it\u2019d be that a lot of the information in this paper is deliberately wrong. Very wrong. I\u2019m way off on a lot of these explanations. But I\u2019m not being byzantine - which means \u201cdishonest\u201d or \u201cadversarial\u201d, by the way, and two thousand years ago it probably would\u2019ve been flagged by HR departments as racially offensive against the citizens of the Byzantine empire. So yeah, I\u2019m not being byzantine - I\u2019m just oversimplifying things to make the concepts easier to understand in expectation.\n\nSets\nA set is a group of things.\nBy the way, if your first thought after reading that last sentence was \u201cthat definition left out a lot!\u201d then be warned: it\u2019s all downhill from here.\n\nSets are important because Mechanism Designers like subjecting entire groups of people or things to their nefarious designs.\nSets are represented with an uppercase letter (such as X or Y).\nWhen sets are defined, they\u2019re often surrounded with brackets. For example, a set of three numbers could be written as X = \\{1,2,3\\}.\nMembers of the set are represented with a lowercase letter (such as x or y).\n\nThere are a handful of set-related funny-looking symbols that these Mechanism Designers like to use in their white papers that you should probably learn:\n\n\n\\in means \u201cin\u201d and is used to show membership in a set\n\u2013 When you see x \u2208 X, that means that x is a member of the set X.\n\u2013 Example: if X = \\{2, 4, 6, 8, 10, 12, 14, 16, 18, 20\\}, then x \u2208 X means x is one of those 10 values.\n\n\n\\cup means \u201cunion\u201d and is meant to take two sets and combine all of their members together (removing duplicates).\n\u2013 When you see X \u222a Y, that means you\u2019re combining sets X and Y.\n\u2013 Example: if X = \\{1, 2, 3, 4\\}, and Y = \\{3, 4, 5\\} then X \u222a Y = \\{1,2,3,4,5\\}.\n\n\n\\cap means \u201cintersection\u201d and is meant to take two sets and make a new set only out of their common members.\n\u2013 When you see X \u2229 Y, that means you\u2019re taking the intersection of sets X and Y.\n\u2013 Example: if X = \\{1, 2, 3, 4\\}, and Y = \\{3, 4, 5\\} then X \u2229 Y = \\{3,4\\}.\n\n\n\\subset means \u201csub set\u201d - all of the elements of the first set exist in the second set.\n\n\n\\supset means \u201csuper set\u201d - all of the elements of the second set exist in the first set. Note that if you see this then you\u2019re dealing with a particularly troublesome brand of Mechanism Designer and you should be on your guard for more trickery.\n\n\n: (the colon) typically means \u201cif\u201d and is often seen together with a funny looking upside-down A. Speaking of which\u2026\n\n\n\\forall means \u201cfor all\u201d and is meant to iterate through a set, often times in the creation of another one.\n\u2013If you\u2019re a programmer, whenever you see \\forall think \u201cfor loop.\u201d\n\u2013 Often times \\forall and : are used to iterate through a set to create a new one.\n\u2013 Example: Consider this monstrous expression: Y = \\{2x, \\ \\forall x\\in X : x>3 \\}. The english translation is something along the lines of \u201cY is a set of numbers that was created by taking each of the x's in X that were greater than 3 and then multiplying that x by 2.\u201d For posterity\u2019s sake, if  X = \\{1, 2, 3, 4, 5 \\} then it follows that (aka \\Rightarrow, as we\u2019ll discuss later) Y = \\{8, 10 \\}.\n\n\nWarning: When it comes to defining sets, Mechanism Designers really like to move variables around or remove them entirely. They do this not to confuse you but to confuse each other - presumably to increase their job security. Take, for example, Y = \\{2x, \\ \\forall x\\in X : x>3 \\}. That would be the same as Y = \\{2 \\cdot x\\in X : x>3 \\} or Y = \\{z \\in \\{2x, \\forall x\\in X \\}  : z>6 \\}. Watch out for anyone using linear algebra to define sets (look for the brackets, sets multiplied by sets, or sets of multiple things in parentheses known as \u201ctuples\u201d). They\u2019re one of the least understandable species of Mechanism Designers and should be avoided by all but the most skilled larpers. In fact, if one of them is reading this right now, they\u2019re probably day dreaming about correcting the author - presumably using made-up words like \u201cvector\u201d or \u201cscalar.\u201d\n\n\n\\mathbf{card}(X) or |X| is the \u201ccardinality\u201d of the set X.  This is a fancy way of saying \u201cthe number of things in X.\u201d If X = \\{ 1, 17, 31, 5\\} then |X| = \\mathbf{card}(X) = 4. Keep in mind that |X| is the size of X while |x| is the absolute value of x \\in X; be vigilant, and remember that the Mechanism Designers will only win if you let them.\n\n\nThere are a handful of prestigious sets that you should know because Mechanism Designers really like letting you know that they know them, too:\n\n\\mathbb{Z} is the set of all integers.\n\\mathbb{R} is the set of all real numbers.\n\\mathbb{E} isn\u2019t a set of numbers at all - it means \u201cthe expectation of\u201d and is used in probability. But it looks similar to these fancy sets, so be careful not to get it mixed up. That\u2019s how they get you.\n\\mathbb{C} is the set of endless agony. If you see it, run.\n\nThe Mechanism Designers will typically use these fancy sets when defining a new set. For example, they might say X \\subset \\mathbb{Z}, which means X is a subset of \\mathbb{Z}, which means that all of the possible x's in X must be integers. If you\u2019re wondering \u201cWhy doesn\u2019t the Mechanism Designer just say that the set is made up only of integers?\u201d the answer is because they hate you.\nStochasticisms and Probabalisticalities\nTwo things that Mechanism Designers simply can\u2019t get enough of are intuitions and expectations.\n\u201cIntuitively\u2026\u201d or \u201cThe intuition is\u2026\u201d means that the Mechanism Designer is about to tell you something that they think is so obvious that they won\u2019t explain it because only a moron would disagree. Unfortunately, for a Mechanism Designer i in the set of all people P, the set of morons M = \\{ p, \\forall p \\in P : p \\not = i \\}, which, in English, means \u201cThe set of morons is all of the people in the set of people who aren\u2019t also the Mechanism Designer\u201d. If you want to understand a Mechanism Designer\u2019s intuition, your best chance is their culture\u2019s traditional approach of asking for an explanation after you beat them in duel with flint-lock pistols. Intuitively, you should be sure to ask quickly.\n\u201cThe expectation is\u2026\u201d or \u201c\u2026 in expectation.\u201d means that the Mechanism Designer is about to do some math, and we can expect that the math will involve probabilities. Probably.\n\n\nP(y) is the probability of event y occuring. Example: for a coinflip, P(\\text{heads}) = 0.50\n\n\nP(y \\cap z) is the probability of event y and event z both occuring. This is also abbreviated as P(y, z).\n\n\nP(y \\cup z) is the probability of event y or event z occuring.\n\n\nP(y | z) =  the probability of event y occuring if we assume event z has already occured. For example, assume a coinflip, but this time there is a cheater using a weighted coin that can change the odds from 50-50 to 80-20. In that case, P(\\text{heads} | \\text{cheater bet on heads} | \\text{cheater's flip}) = 0.80 and P(\\text{heads} | \\text{cheater bet on tails} | \\text{cheater's flip}) = 0.20.\n\n\n\\mathbb{E}[X] = The expected (or probabalistic, if you want to sound smart while actually being wrong) value of X. For example, if there\u2019s a game with a coinflip and you get $0 for tails and $1 for heads then \\mathbb{E}[\\text{game}] = $0.50. Note that if we want to treat the game\u2019s outcomes as a set, {E}[X] = \\{ {E}[\\text{heads}], \\ {E}[\\text{tails}]\\} = \\{ \\$0, \\$1 \\}, and \\mathbb{E}[X] is just the average of the expected values (aka \u201cthe expectation of\u201d) the possible outcomes in X.\n\n\n\\mathbb{E}[X|y] = The value of X in expectation if we assume that y happened. For example, if there\u2019s a game with a coin flip and you get $0 for tails and $1 for heads, but there\u2019s a cheater who is using a weighted coin that can change the odds from 50-50 to 80-20 (against you) then \\mathbb{E}[\\text{game} | \\text{cheater's flip}] = $0.20.\n\u2013Now imagine that the cheater gets to flip some of the time and you get to flip the coin the rest of the time:\n \\mathbb{E}[\\text{game}] = \\left( \\mathbb{E}[\\text{game} | \\text{cheater's flip}] \\times P(\\text{cheater's flip}) \\right) \\ + \\  \\left( \\mathbb{E}[\\text{game} | \\text{your flip}] \\times P(\\text{your flip}) \\right) \n\n\n\\mathbb{P}(x) is just a fancy way of saying P(x). Some Mechanism Designers insist that if P(x) is the probability of x, \\mathbb{P}(x) is the probability of x in expectation, but nobody knows what that means and so we just ignore them and move on with our lives.\n\n\nFancy Math\n\u2211 (Sum) \nThis sigma may have been an key part of your fraternity or sorority\u2019s identity during college, but it has another, less-important use case: math.\n \\sum^n_{x=1}f(x) = The sum of f(x) for all x values from 1 to n.\nIn other words, it\u2019s a \u201cfor loop\u201d that sums the different values of f(x), with x ranging between 1 (the bottom) and n( the top).\nMath Example:\n\\sum^4_{x=1}2x = 2+4+6+8 = 20\nNote that you can replace the range notation of \\sum with a set.  If X = \\{1, 2, 3, 4 \\} then\n \\sum_{x \\in X} 2x \\ = \\ 2 + 4 + 6 + 8 \\ = \\ 20 \\ = \\ 2\\sum_{ X} \nMechanism Designers really like talking about whether x should start at 1 or 0, although nobody knows why. Leading experts have hypothesized that it\u2019s a core part of their mating ritual, but the results are still inconclusive.\n\u220f (Product) \nThis *\"pi from the uncanny valley\"* is actually a product:\n\\prod^n_{x=1}f(x) = the product of f(x) for all x values from 1 to n.\nThe best way to explain it is through a comparison:\n \\sum : \\prod :: \\text{addition}: \\text{multiplication} \nIf you don\u2019t remember the : :: : comparison format from the SATs then you are beyond saving.\nThe \"Big U\" and the Binomial Coefficient\n\\bigcup^x_y  \\text{ or } \\binom{n}{k}\nUnless x and y are both small, normal-looking numbers, you\u2019re about to have a really bad time. The math isn\u2019t hard, it\u2019s just a real pain in the ass to write out. The one on the left is an iterator for the union of sets and the one on the right is the binomial coefficient.\nd/dx  (Derivatives)\nGet excited because it's finally time for everyone's favorite subject: calculus!\nf(x)\\frac{d}{dx} = f'(x) =  \\text{the derivative of} f(x) \nDerivatives measure the rate that one thing is changing (probably x) relative to the rate another thing is changing (probably y, but maybe t if the Mechanism Designer is fully domesticated). If f(x) is a line, then its derivative is the slope of the line. In other words, it\u2019s the rate of change of the line. If there is a line that graphed \u201ctime\u201d on the x-axis and a car\u2019s distance from the starting point on the y-axis, then the derivative of that line would be the car\u2019s velocity (the rate that the position is changing relative to the rate that time is changing). If the velocity of a car is on the y-axis, then the derivative of that line would be the car\u2019s rate of acceleration. This is what they teach in calc 1, but that you haven\u2019t needed to use since highschool because it only recently became cool to discuss \u201cnovel mechanisms.\u201d It looks like your teacher was right all along.\n\u222b (Integral)\nThis squigly line is an \"integral.\" Fun fact - there are no integrals in the bible.\n \\int^x_yf(x) = the integral, aka \u201cantiderivative.\u201d\nIf a function f(x) creates a line on a graph, its integral is the area underneath it. Even your highschool teacher would\u2019ve admitted that it\u2019s unlikely that you\u2019ll need to use integrals in your day-to-day job. After all, integrals are pretty useless unless you\u2019re either a math teacher or having to deal with the probabilities of expected probabilities in a Mechanism Designer\u2019s white paper. Speaking of which\u2026\nBack to Probabilities\nCumulative Distribution Function\nF_X(x) is a cumulative distribution function, aka CDF. If you have a distribution X (which is a set of all possible values that x could be) then F_X(x) = \\mathbb{E}[P(x > X)], which is the probability that x is greater than a randomly selected value from the set X.\nExample: If X = \\{2, 4, 6, 8, 10, 12, 14, 16, 18, 20 \\}   and x = 8, then F_X(8) = 0.30 because when you draw a random number from X there is only a 30% chance that you\u2019ll get one of the three that are less than 8 (2, 4, and 6).\nProbability Density Function\nf_X(x)  is a probability density function, aka PDF. It is basically saying the probability that a randomly chosen value from X will equal x.\nExample: If X = \\{2, 4, 6, 8, 10, 12, 14, 16, 18, 20 \\}  then f_X(6) = 0.10 because there\u2019s a 10% chance that we\u2019ll draw a 6 from the set. In other words, f_X(x) = \\mathbb{E}[P(x = X)] . Don\u2019t think about that equation too much - if the thought of x = X seems contradictory to you, that\u2019s a good sign that you\u2019re still a healthy, normal person.\nExample: X = \\{2, \\ 2, \\ 6, 8, 10, 12, 14, 16, 18, 20 \\}  (note that we replaced the 4 with another 2) then f_X(2) = 0.20 and f_X(4) = 0.0.\nNote that F_X(x) is very useful in analyzing auctions because if X is the set of all bids, F_X(x) is the probability that our bid x is greater than a randomly selected bid, and F_X(x)^n is the probability that our bid x is greater than n number of bids.\nThe calculus from the previous section comes into play because the probability density function f_X(x) is the derivative of the cumulative distribution function F_X(x), and the cumulative distribution function is the integral of the probability density function:\nf_X(x) = F_X(x)\\frac{d}{dx} \nF_X(x) = \\int^{\\infty}_{- \\infty} f_X(x)\nWe\u2019ll often have to use calculus to get back and forth between how likely someone is to bid something, and how likely a bid is to be higher than another bid.\nAuction Hieroglyphics\nPeople typically refer to the set of players (so-called) in an auction as P, and a player in the set of players as i. Nobody knows why i was chosen over p, but it was probably so that Mechanism Designers could go around saying \u201ci player\u201d to each other and laughing at their clever inside joke. This has been going on for decades.\nIf the bids are referred to as b then b_i would be i's bid.  If you want to compare two players, j is typically a stand-in for \u201cthe other player,\u201d whereas -i is the stand-in for \u201call players other than i.\u201d\nSymbols Over (or Under) Letters \nSometimes a Mechanism Designer might want to share a new formula with you that is similar to an existing one, but that\u2019s just slightly different. If you see weird symbols over or under letters, it probably means the equation or variable has had something added to it to make it extra special.\nHere are some examples:\n\nIf i is a player (bidder) in an auction, i' might be his sworn nemesis.\nIf b_i is the bid of player i, b^*_i might be the optimal bid.\n\u2013Warning: You might be wondering, \u201cAren\u2019t all bids optimal bids? Why would player i bid an amount that isn\u2019t optimal?\u201d but you should never ask this question out loud - it\u2019s considered deeply inappropriate and you\u2019ll end up on a list.\nIf g(x) is a function that works for everyone, g_i(x) is a function that only works for special players like i.\nIf t_i^2 isn\u2019t meant to be t_i squared, it may mean that it\u2019s the second t belong to i in a sequence of t_i's. Maybe the 2 is on the bottom, but then where would we put the i to mark the t as special? This is a perfect example of the kind of difficult question that Mechanism Designers spend most of their time on.\n\n\u03f5 (Epsilon)\nMechanism designers use the \\epsilon (epsilon) symbol a non-trivial amount, which is ironic because it represents a trivial amount. Trivial, by the way, is just a cooler-sounding way of saying \u201cteeny-tiny.\u201d A Mechanism Designer might say something along the lines of \u201cthe optimal bid value was market price less epsilon\u201d: b^*_i = v_i - \\epsilon.\n \u22c5 (The Dot Thingy)\nWhile this may mean multiplication, if you see it by itself inside of a function then it probably means the Mechanism Designer is being lazy and didn\u2019t want to copy and paste their math. You\u2019ll typically see this only after you\u2019ve already seen the full version. For example, if you\u2019re unlucky enough to see something like y = z + g(x^2+\\mathbb{E}[Z] - \\epsilon ), then later on you might see a = 2z + g(\\cdot), where the \\cdot is a stand-in for x^2+\\mathbb{E}[Z] - \\epsilon.\n\u21d2 (Therefore)\nIf the Mechanism Designer wants to prove why something is the way that it is, they might use this big arrow thingy. It means \u201ctherefore\u201d or \u201cit follows that\u2026\u201d. For example, if a Mechanism Designer wants to show that size of his mechanism proves that he\u2019s really smart, it might look like:\n\\mathbf{card}(mechanism_i) > \\mathbf{card}(mechanism_{j}) \\forall j\\in P : j \\not = i \\Rightarrow F_{IQ}(iq_i) = 1 - \\epsilon \nA good exercise is to assess whether or not you actually understood that equation. If you did, it means you\u2019ve been paying attention to the paper! Unfortunately, it also means you\u2019re less cool in expectation. The actual english translation is \u201cIf the number of things in the mechanism made by player i is greater than the number of things in the mechanism made by player j, for all possible player j\u2019s in the world who aren\u2019t player i, then it follows that the IQ of player i has a 100% chance (minus a teeny-tiny percent) of being greater than a randomly selected IQ from the distribution of all the IQs in the world.\u201d\n\u2192 (Approaches)\nThis small arrow thingy (\\rightarrow) means \u201capproaches.\u201d You\u2019ll often see \\rightarrow right next to \\Rightarrow due to how much the Mechanism Designers like tricking people.\nAn example, where M_i is the set of mechanisms made by player i and F_i is the set of player i's friends:\n\n|M_i| \\rightarrow \\infty \\Rightarrow |F_i| \\rightarrow 0\n\nWhich, in English, means \u201cIf the number of mechanisms made by player i approaches infinity, it follows that the number of friends that player i has approaches zero.\u201d\n \u03d5,\u03b8,\u03b3,\u03b4,\u03c3,\u03c8,\u03c4 or Other Greek Letters\nMechanism Designers love defining variables. Unlike their sworn enemies the mathematicians, Mechanism Designers really like for their variables to be exotic, and so they\u2019ll often use lower-case greek letters. It\u2019s a best practice to always have a Greek alphabet available when reading a mechanism\u2019s white paper so that you can quickly check to see if the designer is referring to a variable - which is pretty normal - or using some sort of ritualistic-sacrifice-based summoning math - which is a red flag. Intuitively.\nSmart-Sounding Auction Terms\nex ante and ex post\nWhen auction players have to figure out what their bid for an item is before they know what the value of the item is, the auction is said to be ex ante.  If the value of the item is known at bidding time, the auction is said to be ex post. This is one of the rare cases in which the obscure auction language is actually less wordy than what its describing. Way to go, Mechanism Designers! You did it!\nFirst-Price and Second-Price Auctions\nA first-price auction is one in which the highest bidder pays what they bid. A second-price auction is one in which the highest bidder pays what the second-highest bidder bid. Mechanism Designers really like second-price auctions because they pay the beneficiary more than a first-price would, but they\u2019re also easier to cheat and therefore require a more robust mechanism.\nWarning: Never ask a Mechanism Designer why second-price auctions are better than first-price auctions. It\u2019s like asking your wife if she\u2019s had any interesting dreams recently, or if that girl at work she doesn\u2019t like has caused any more problems. If a Mechanism Designer ever brings up the subject of first-price vs second-price, just look them directly in the eyes and then tell them with a firm voice, \u201cWith a properly designed mechanism and sealed bids, a second-price auction at equilibrium leads to more auction revenue in expectation, obviously!\u201d This response is a strictly-dominant strategy.\nSealed Bid means \"private\"\nThat's what the intuition is, anyway.\nUtility Function\nA utility function, often called a payoff function, is how valuable someone thinks something is. It\u2019s typically measured in expectation, but this was developed by the French so the \\mathbb{E} is silent.\nExample: If U(x) is a utility function, then U_i(b_i, b_{-i}) is the payoff to player i considering their bid ( b_i) and their competitors\u2019 bids  (b_{-i}).\nBid Shading\nBid shading occurs when bidders bid less than their true value - in other words, when v_i > b^*_i. This typically happens because the bidder can make more money by bidding below their true value. Or, in the native tongue of Mechanism Designers, \u201cThe utility function of bidders is optimized when their private valuations exceed their equilibrium bid.\u201d. We may never know why Mechanism Designers find it so fascinating that people make more money by paying less for something than they think it\u2019s worth.\nIncentive Compatibility\nSomething is considered incentive compatible (IC) if the participants in an auction are willing to bid their true value. Every bidder i assigns a value v_i to the item.  The mechanism is incentive compatible when v_i = b_i. Mechanisms can be strongly incentive compatible or weakly incentive compatible, but nobody really knows what that means. It probably has something to do with how big the mechanism is.\nBayesian-Nash Equilibrium\nA Bayesian-Nash Equilibrium (or, as the kids say these days, a \u201cBNE\u201d) exists when there are multiple rounds and each bidders\u2019 optimal strategy (AKA their \u201cBest Response\u201d or br_i or s^*_i) stays the same for each round. In other words, they won\u2019t benefit from using any \u201ctrick plays\u201d or \u201cbamboozles.\u201d\nNote that a mechanism with incentive compatibility is considered significantly better than a mechanism with just a bayesian-nash equilibrium. If two Mechanism Designers meet in the wild and one of them has an IC mechanism and the other has a BNE mechanism, then the wife and children of the BNE designer will instinctively join the tribe of the IC designer and the BNE designer will have to start rebuilding his mechanism and his family from scratch. Mother nature is cruel, but efficient.\nCredible Neutrality\nNobody actually knows what credible neutrality means. This has led to many Mechanism Designers making up their own definitions, presumably using their intuition. Unfortunately, these different definitions of credible neutrality are not credibly neutral, which is why the term remains undefined.\nFair Exchange Problem\nThe fair exchange problem refers to the difficulties of getting the parties of a trade to actually do what they promised they\u2019d do, such as paying a bid or selling an asset at a certain price.  Mechanism Designers love making people do things, so this is one of their favorite problems to solve.\nWhen someone refers to the free look problem, know that you\u2019re talking to an actual human being - a Mechanism Designer will always out themselves by calling it a fair exchange problem. Usage of the term optionality is a red flag but still inconclusive.\nConclusion\nThe conclusion section of a Mechanism Designer\u2019s white paper is the result of feeding the rest of their paper into a LLM model and then asking it to generate a summary. This is why nobody ever actually reads it, and neither should you. I\u2019d also like to thank my co-author, Et Al.\n", "\nJoint work with Kubi. Thanks to Drew, Justin, Ladislaus, Conor, Lin for their review and feedback. Feedback is not necessarily an endorsement.\nCustard  (Constraining User State To Avoid Reneging Downstream) is a technique for enabling atomically composable super transactions on based rollups through careful state management. The key insight is that by constraining specific parts of L1 state that a super transaction depends on, we can enable validators to safely issue L1 execution preconfirmations ahead-of-time without requiring control over the whole L1 block. This reduces the validator coordination needed to bootstrap preconf protocols and allows based rollups to offer super transactions sooner with fewer participating validators. This post explores three ways to implement Custard: through EIP-7702, smart contracts, and exclusion preconfirmations.\nWhy does this matter?\nA primary benefit of based sequencing is that it enables synchrony and atomicity between the L1 and based rollups (BRU). This means operations on both layers can be combined into what we call a \u201csuper transaction\u201d - a bundle of transactions that works across both layers as if they were one.\nA Real-World Example\nLet\u2019s consider a practical example: L1\u2192L2\u2192L1 arbitrage. Imagine Alice wants to atomically:\n\nMove her tokens from L1 to a BRU\nMake a trade on the BRU\nMove her tokens and profit back to L1\n\nCurrently, these would be separate steps with delays between them (e.g., normally 7 days between steps 2-3 for optimistic rollups). Applying Custard, Alice can bundle all of these actions into one super transaction that completes within a single Ethereum block.\nTo make super transactions widely adoptable, we need to address three key challenges:\n\nReal-Time Proving: We need a way to settle the BRU\u2019s state quickly enough to withdraw within a single L1 block\nGuaranteeing Atomicity: We need mechanisms to ensure either all parts of the super transaction complete successfully or none of them happen\nValidator Availability: We need sufficient validator participation to make super transactions reliably available for users\n\nThe rest of this post examines each of these challenges and existing approaches to solving them, before introducing Custard as a way to bring them together.\nChallenge #1: Real-Time Proving\nTraditional Approach and Its Limitations\nUntil now, it was commonly believed that (trustless) instant withdrawals from rollups required \u201creal-time proving\u201d - essentially, the rollup\u2019s state must first be settled via validity proof. For Alice\u2019s super transaction to complete in one L1 block (12 seconds), the validity proof would need to be generated even faster. However, the technology to generate these proofs so quickly (real-time SNARKs) isn\u2019t deployed in the market yet but there many amazing efforts in progress ().\nCurrent Workarounds\nSome projects (UniFi, Gwyneth, T1) have turned to Trusted Execution Environments (TEEs) as an alternative to SNARKs for proving the rollup\u2019s state transition function. While TEEs can generate proofs much faster than SNARKs, making real-time proving possible, they come with a significant drawback: they require trusting the hardware manufacturer and prover. This additional trust assumption introduces new risks to based rollups that traditional SNARK-based systems don\u2019t have.\nA New Solution: The Solver Approach\nNethermind recently proposed a solution that achieves the UX of instant withdrawals without needing real-time proving. Their approach:\n\nUses solvers to immediately provide users withdrawal liquidity on the L1 before the BRU state is settled\nMaintains atomicity (solvers and BRU bridge are protected from reorgs)\nMaintains trustlessness (no need to trust the solver or TEEs)\nProvides a practical path forward while we wait for real-time proving technology to mature (at the cost of capital efficiency)\n\nChallenge #2: Guaranteeing Atomicity\nFor Alice\u2019s super transaction to work seamlessly, we need to ensure that either all sub-transactions complete successfully or none of them happen at all. This is where execution preconfirmations (EPs) come in.\nThe Role of Execution Preconfs\nTo guarantee the super transaction\u2019s success, we need Ethereum validators to provide four specific guarantees:\n\nL1 Deposit Guarantee: Confirms Alice\u2019s funds will successfully move from L1 \u2192 BRU\nL2 Swap Guarantee: Ensures Alice\u2019s trade on the rollup will execute as expected\nL2 Withdrawal Guarantee: Confirms Alice\u2019s request to move funds back from BRU \u2192 L1 will be processed\nL1 Solver Guarantee: Ensures a solver transfers Alice funds on the L1\n\nWhy Ethereum Validators Matter\nA crucial insight is that these guarantees must come from Ethereum validators themselves. They are uniquely positioned to make these guarantees because they can be proposers for both layers:\n\nOn Ethereum, they have a write-lock over the L1 since they have sole authority to propose the next block\nOn the BRU, they can be configured to be the only ones with permission to sequence transactions during their slot\n\nThis dual write-lock is what makes based sequencing special - only Ethereum validators can credibly commit that a super transaction will execute exactly as planned. Based preconfs are the mechanism through which validators make these promises binding - by staking capital, validators become preconfers and face economic penalties if they fail to honor their commitments.\nChallenge #3: Validator Availability\nL1 EP constraints\nA critical limitation of L1 EPs is their \u201cjust-in-time\u201d nature. Validators can only safely issue these L1 EPs when they\u2019re the current block proposer. Why? Because a future validator doesn\u2019t have a write-lock on the L1 and earlier validators could change L1 state in ways that break their L1 EPs.\nThis is different from L2 EPs on the rollup, where validators can safely make commitments \u201cahead-of-time\u201d because the rollup\u2019s smart contracts ensure only the designated preconfer can write to the state prior and during their turn.\nimage1920\u00d7738 80.5 KB\nThe Bootstrap Challenge\nThis just-in-time constraint creates two significant problems:\n\nLimited Availability: Super transactions can only happen during blocks where the\nL1 validator has opted in as a preconfer\nUX Issues: Unless every single Ethereum validator participates in the system, there will be slots where super transactions aren\u2019t available\n\nGetting 100% of Ethereum validators to participate is an enormous BD challenge. Therefore, we need an alternative solution: finding a way for validators to safely issue L1 EPs ahead-of-time.\nWhat is Custard?\nCustard offers a solution to our timing problem by making a key observation: we don\u2019t always need to control the entire L1 block to make safe guarantees. Instead, we can selectively lock just the specific pieces of L1 state that our super transaction needs to work with.\nThis insight is powerful because it means we can issue some types of L1 EPs ahead-of-time, as long as we can guarantee that the specific state we care about won\u2019t change. By only locking what we need, rather than requiring control over everything, we can significantly reduce the number of validators needed as preconfers.\nNote: The implementations we\u2019ll describe next are intentionally simplified to clearly illustrate the mechanisms. In practice, these can be optimized for better capital efficiency and generality.\nCustard with EIP-7702\nEIP-7702 enables user accounts (EOAs) to set their own custom code based on any smart contract, effectively turning it into a smart account. We can use this to create time-locked guarantees about a user\u2019s account state.\nHow It Works\nLet\u2019s walk through how Alice could execute her super transaction using EIP-7702:\n\nInitial Lock (Slot S)\n\nAlice locks her account\u2019s nonce until a future slot S'\nThis prevents any changes to her account until the designated slot\n\n\nSetup (Slot S + 1)\n\nAlice requests her super transaction from a preconfer who will propose at a future slot S'\nThe transaction includes:\n\nDepositing B ETH to the rollup\nExecuting the arbitrage trade\nWithdrawing B + \u03b5 - f  ETH (original amount plus profit minus fee)\nHaving a solver complete the withdrawal on L1\n\n\n\n\nVerification (Slot S + 1)\n\nThe preconfer checks that Alice\u2019s account is properly locked\nIf verified, the preconfer issues all necessary preconfs\n\n\nExecution (Slot S')\n\nThe preconfer executes the entire transaction:\n\nDeposits Alice\u2019s B ETH to the BRU\nCommits the BRU blob to L1 containing Alice\u2019s trade\nCompletes the solver\u2019s transfer of B + \u03b5 - f ETH to Alice on L1\n\n\n\n\nSettlement (Slot S' + \u0394)\n\nAfter \u0394 blocks, the BRU state is proven\nThe solver can recover B + \u03b5 + f ETH by withdrawing from BRU\n\n\n\nKey Insight\nBy locking her account, Alice guarantees she\u2019ll have sufficient funds (B ETH) when the super transaction executes. Preconfers can safely issue L1 EPs ahead-of-time if they require accounts to be first locked, solving our timing problem.\nCustard with Smart Contracts\nWhile waiting for EIP-7702 to be released, we can achieve similar results using smart contracts. The key difference is that instead of modifying account behavior directly, users must first deposit their assets into an escrow contract that enforces the same guarantees:\n\nAssets are locked until the target slot\nThe funds can only be deposited into the rollup\nNo decreases in asset balance are allowed until then\n\nimage1920\u00d7916 86.9 KB\nThe execution flow mirrors the EIP-7702 approach, but with one notable advantage: the escrow contract naturally accumulates a pool of locked assets, enabling potential capital efficiency optimizations in protocol design.\nCustard with Exclusion Preconfs\nExclusion preconfs represent a different kind of validator promise: instead of guaranteeing what they will do, validators commit to what they won\u2019t do. Specifically, they promise to prevent certain state changes by disallowing specific account actions to take place. While exclusion typically goes against Ethereum\u2019s values, when used carefully in this context, it serves a constructive purpose: locking specific account states to preserve ahead-of-time L1 EP validity. Importantly, these types of preconfs would only be permitted if explicitly authorized by the account owner to avoid censorship.\nHow It Works\nLet\u2019s walk through how Alice could execute her super transaction using exclusion preconfs:\n\nIssuing Execution Preconfs\n\nAlice gets exclusion preconfs from the validators ahead of her target super transaction slot\nEach exclusion preconf promises not to:\n\nInclude transactions that would increase Alice\u2019s nonce\nInclude transactions that would decrease Alice\u2019s ETH balance\n\n\n\n\nExecution\n\nWhen the target slot arrives, Alice\u2019s EOA is guaranteed to have the required ETH\nThe super transaction can proceed safely\n\n\n\nimage1920\u00d7881 88.7 KB\nAn advantage of this approach is that all execution preconfs are issued off-chain, reducing gas costs. However, it introduces several complexities. Super transactions still require all earlier slot validators to be L1 exclusion preconfers - while easier than previous approaches, this remains a significant BD challenge. Additionally, paying for exclusion preconfs becomes tricky since nothing lands on-chain in the happy case and the collateral requirements and slashing conditions need to be carefully considered when assessing the risk of reneging.\nLimitations of this approach\nA key distinction in Nethermind\u2019s solver approach is that withdrawal requests have a simple \u201cL1 output condition\u201d - they only need to verify that tokens arrive at a specific L1 address. This simplicity is what enables atomic withdrawals without real-time proving. However, more sophisticated super transactions might require L1 output conditions to depend on complex L2 state changes, in which case we may require real-time proving of L2 state. While Custard\u2019s principles for managing L1 state dependencies still apply, implementations will either need to wait for real-time SNARKs to mature or accept the additional risks of TEE-based proving solutions.\n", "\n\nContext\nAn ECDSA signature verification circuit written in Circom results in an R1CS with approximately 1.5 million constraints, a proving key that is close to a GB large, and a proving time that is non-trivial.\nThe point of doing an ECDSA signature verification using Circom is to verify that the prover owns a certain address without revealing the address. Although it is easier to check privKey * G == pubKey, most wallets don\u2019t expose the private key of a wallet, therefore we are restricted to checking signatures. Signature verification without revealing the address has applications in, for example, zero-knowledge proof of membership. (Which is why we want to have a way to prove ownership of an address anonymously)\nIn this post, I will decribe a method that could improve the efficiency of ECDSA signature verification written in Circom.\nI focus on implementation using Circom to avoid ambiguity. However, the method is not completely dependent on Circom. You can swap \u201cCircom\u201d with \u201czkp\u201d, \u201czk-snark\u201d, or other high-level arithmetic circuit languages.\n\nOverview of the method\nThe essence of the method is, that in order to do as less computations as possible in a circuit, we offload some signature verification calculations from the circuit.\n\nThe method\nThis credit for this technique goes to the answerer of this stack exchange post. Although, the method is not formally verified. If this method lacks correctness, soundness, or zero-knowledge-ness, then the entire scheme I describe in this post will not work. That being a possibility, I\u2019m writing this post hoping it to be useful at least in some way,  as a source of ideas.\nVerify an ECDSA signature without revealing the s part of the signature leads to reducing the required calculation that needs to be kept private (i.e. needs to be SNARKified)\nFirst, you produce a signature with your private key as usual.\nR = k * G\ns = k^-1(m + da * r)\nThe signature = (r, s)\nwhere\n\n\nG: The generator point of the curve\n\nk: The signature nonce\n\nR: k * G\n\n\nQa: The public key\n\nm: The message to sign\n\nr: x-coordinate of R\n\n\nThe signature can be verified by checking the following equality\nR = s^{-1} * m * G+ s^{-1}r * Qa\nor\ns * R = m * G + r * Qa.\nThis equation can be perceived as s being the private key, R being the generator point, and m * G + r * Qa being the public key.\nNow we can prove the knowledge of s without revealing s itself, by generating a signature!\nWe calculate the signature as follows:\nR\u2019 = k\u2019 * R\ns\u2019 = k^-1(m\u2019 + s * r')\nwhere\n\n\nk\u2019: The signature nonce\n\nm\u2019: The message to sign\n\nr\u2019: x-coordinate of R'\n\n\nWe verify the signature (s\u2019, r')  by checking:\n\ns\u2019 * R\u2019 = m\u2019 * R + r' * Qa\u2019\n\nThis equation itself doesn\u2019t reveal Qa (the public key).  So it can be checked publicly, without using Circom.\nWe also need to check that Qa\u2019 actually comes from Qa. This can be done by checking:\n\nQa\u2019 = m * G + r * Qa\n\nSince we don\u2019t want to reveal Qa (the public key), this equality check is done using Circom. Moreover, it is required to keep m a secret. If m is revealed, Qa will be recoverable. That is, in zero-knowledge proof of membership, the public keys that are members of a set are publicly known; someone can just check which public key matches Qa\u2019, by filling in m and r .\n\nSummary and benchmarks\nTo sum up, the circuit will take Qa and m as the private input, and r\u2019 and Qa\u2019 as the public input. I constructed the outline of the circuit here. The circuit is not complete. The purpose of it is to demonstrate the outline.\nThe benchmarks:\n\nconstraints: \\approx 200,000\nproving key size: \\approx 134MB\nproving time (witness generation + proving): \\approx 15s on a MacBook Pro\n\nWhich is a meaningful improvement from the original circuit.\nAnd that is it.\nFeedback will be appreciated.\n", "\nLookup Singularity via MMR\nIntroduction\nWhat I will show is that if we have a public merkle mountain range where existence within it implies correctness, then this primitive can be used to achieve the lookup singularity. What I mean is you can enable complex lookup tables for sha256 hash, merklizing leaves, or potentially even EVM storage proofs.\nThere are optimistic and immediate finality variations of this scheme which balance certain trade offs.\nBackground\nLookup tables are an incredible innovation in the world of ZKPs. They allow for much faster proving for complex computation like a Sha256 hash. Barry Whitehat coined the term the lookup singularity (Lookup Singularity - General - zk research) to represent how they can drastically improve the performance of SNARKs.\nStandard lookup tables require pre-computing an entire table, and committing it during circuit compilation. Therefore these tables cannot be updated after the keys are generated. The cost of committing the table is amortized over all the lookups used during proving. There is a negligible per-lookup cost for these types of constructions (eg plonk-lookup). The obvious limitation here is that it\u2019s impractical for lookup tables that are very large, say 32-bit float arithmetic (2^64 rows).\nJustin Thaler wrote papers for a new system called Jolt/Lasso which sacrifices the low per-lookup cost in order to get more complex lookups. This is done by using multivariate polynomials to represent the lookup table as a  polynomial evaluation. This makes it so you don\u2019t need the full table in memory to use in a circuit, but increases the per-lookup cost. See Lasso/src/subtables/lt.rs at 823da601422d17f7c150631947b33a9db1ad5b98 \u00b7 a16z/Lasso\nThese constructions don\u2019t practically allow for lookup tables of truly complex operations like a sha256 hash. Say I had 100 leaves and I wanted to merklize them to compute the root. No existing lookup table construction can help you with this.\nTrusted lookup table oracle\nOne very easy way to increase the performance and utility of a lookup table is to use a trusted source. Imagine I wanted to multiply 2 floating point numbers, I can submit the lookup via an API to a service that returns the lookup table with a snark-friendly signature. In practice we would batch all the look ups together with one signature.\nThe beauty of this approach is you can enable truly arbitrary lookup operations, which would dramatically improve the performance of your circuits. You also get a custom lookup table for each proof you generate. This approach would drastically decrease circuit complexity.\nOne thing to note is that for any sensitive parts of the circuit you may wish to do it without the table oracle to preserve privacy. Alternatively you could include extra unnecessary lookups to obfuscate which were used. This approach lets you offload any parts of the circuit which can be public into a lookup table. Essentially a selectively disclosed computation ZKP.\nHowever, the issue here is that the trusted party can secretly sign incorrect lookups in order to forge a proof and attack a system. The idea I want to present is to solve this malicious lookup oracle problem via crypto-economic methods.\nWhat if there was a better way?\nSecuring with crypto-economic security\nThe obvious place to start is have the lookup oracle stake a large amount of ETH/USDC/etc. If I can coerce the oracle to give me a malicious result, I can generate a fraud proof (ZKP or smart contract) in order to claim a reward.\nThis works well to prevent the oracle from colluding with someone else. This however fails in the situation where the oracle is also the one requesting the lookup. There is no economic incentive to claim your own stake.\nThe only way to solve this problem is to force the oracle to publicly disclose every lookup table that it generates. This can be done with a merkle mountain range (MMR). The key insight you need to see is that existence within the public MMR directly implies correctness. We then construct the circuits to check for existence within a MMR, and we then compare with the root of the trusted, public MMR.\nOverall Solution\nFirst you use a merkle mountain range w/ a zk friendly hash to commit tables into a smart contract. A MMR is advantageous because:\n\nScales virtually infinitely\nShort inclusion proofs\nQuick to update proofs\n\nIn order to avoid confusion, I want to emphasize the MMR does not merklize a full lookup table. Instead each leaf in the MMR can be a custom lookup table(s) for a particular proof generated. Each leaf can be multiple tables, as a circuit might offload multiple operations to a table. The MMR may include duplicate lookups.\nThe lookup table used during proving only needs to include the entries needed. This is the key advantage of this approach. The table(s) can be generated locally by the user before being generating the proof. The tables then can be sent to be added to the global MMR, at which point the proof can be updated to have inclusion in trusted MMR.\nTo save on gas, the actual lookup tables do not need to be put on-chain, we can use an optimistic approach where just the commitments of the tables are stored. The details around off-chain data availability will not be discussed here.\nIn situations where we want quick finality, you can have a contract verify table validity before adding it to the global MMR. The trade off is it will cost more. The optimistic approach actually allows for much more complicated look ups (eg call a smart contract w/ some input at a block height)\nThere are a few variations these tables can be utilized, but the custom lookup table is always an input into the primary circuit. This is nice because in a mobile app setting this enables us to generate a snark and its corresponding tables very quickly. What remains is proving the table used is in the global lookup tree. The mobile app can submit the zkp + lookup table to an infrastructure provider which will finalize the snark.\nThe infrastructure provider can verify the table validity, and submit it for inclusion into the trusted MMR. Once included, an inclusion proof can be generated. The original snark can be recursively updated to verify the table used exists within the global MMR. The root of the MMR then becomes a public input of the resulting ZKP. The location of the table can optionally be disclosed as well.\nThe overall trade-off is pretty simple. For the price of waiting a bit to submit/verify the proof, you get fast and memory efficient snarks client-side. The waiting to submit can be handled for the user with infrastructure.\nComplex fraud proofs are not required for the optimistic variation. A smart contract can be used to check validity of entries when prompted.\nConclusion\nI am very confident the above would work, and yield dramatic performance gains to client-side snark proving.\nFor the optimistic variation we can enable truly complex lookups that simplify ZKPs around EVM storage proofs. The tradeoff for very complex look ups would be a longer settlement time. The settlement time can be decreased if you limit the lookup operations in complexity (sha256, floating point operations etc).\nIn the immediate finality variation, there will be a larger financial cost for each addition to the MMR. I suspect the EVM would be prohibitively expensive, especially at scale. To optimize this approach it would be best to build a new chain. As of the time of writing, Solana is doing 3500 TPS at a tx fee around $0.0002. If you reduced the complexity of the execution environment you would get more TPS for cheaper.\nAlternative Perspective\nIt hit me after my initial post that there is an alternative use case for the technique described above where a MMR is unnecessary.\nFrom the perspective of the prover, generating the main ZKP is done in two parts\n\nCompute lookup table(s) for circuit\nUse table(s) as a public input to generate the ZKP\n\nFrom here we have a ZKP and a list of assumptions the ZKP is made on. The other use case is the prover can offload some of the heavy parts of the circuit to infrastructure to validate the assumptions, and recursively update the ZKP.\nPretty neat\n", "\nThis study was done by @cortze and @yiannisbot from the ProbeLab team (probelab.io), with the feedback of the EF and the PeerDAS community.\nIntroduction\nThe Dencun upgrade was an important milestone for Ethereum\u2019s roadmap. It introduced Blob transactions and Blob sidecars (through EIP-4844). The upgrade included major updates for the underlying network, which now has to share over GossipSub a target of three blob sidecars of 128KBs each per slot, caped to six of them as a maximum (often referenced as \u201c3/6\u201d, referring to target and max blob sidecars, respectively).\nThe current network hasn\u2019t experienced any major outage or finalization difficulty since the upgrade. Nodes in the network could get along with the current blob target and max values with the blob sidecars propagating through the network well within the 4 second propagation window, as can be seen in ProbeLab\u2019s weekly report on Block Arrivals: Week 2024-47 | ProbeLab.\nHowever, with increased demand from L2s for more capacity, questions come up with regard to \u201cwhat is the maximum \u201ctarget/max\u201d blob count that the network can comfortably handle?\u201d, especially for home stakers.\n\nThe motivation and target of this study is to help define how much extra bandwidth is available in the network to support a blob target and max value increase for the next hard fork.\nIn this post, we:\n\nintroduce the measurement methodology and framework we\u2019ve prepared to estimate the available upload bandwidth of nodes in the network.\npresent a comprehensive bandwidth measurement study that we carried out, where we measured the bandwidth availability of most online and reachable nodes in the Ethereum network over a period of 6 days from 2024-11-23 to 2024-11-28.\n\nTL;DR\n\n60% of nodes deployed in Europe or the US have bandwidth availability of more than 20Mbps, while only 20% of nodes deployed in Australia can go above 20Mbps. This is not related to line speed availability in Australia, but rather it is due to the fact that 65% of Ethereum nodes are located in Europe or the US and therefore geographically far from Asia-based nodes.\nSeconds 1 to 4 of the slot, when block propagation takes place, are clearly more loaded in terms of bandwidth availability. Nodes show 9%-13% less bandwidth availability during those seconds.\nHowever, even during the more loaded periods of the slot (1st to 4th second), our measurements from 4 different geographical regions indicate that the mean available bandwidth for nodes (either in cloud or non-cloud infra) stayed between 18 and 23 Mbps. This tells us that there is still space for more blobs to be carried, although this will result in even less bandwidth availability during this loaded part of the slot.\nDuring our 6 day measurement period, we\u2019ve seen that 35% of the slots had no blobs at all, while 42% of the slots included 5 to 6 blobs. Given the current 3/6 target/max blob values, 42% of the bandwidth measurements we did were performed at almost the blob max capacity.\nWith the current discussion around increasing the blob count from a target and max values of 3/6 to 6/9, the presented metrics align with the EthPandaOps recent post (link) that it should be a reasonable increase with the current network state. The network already has 50% of slots at the targeted blob count goal or beyond, while 35% of slots don\u2019t even have associated blobs.\nAt the same time, we expect that nodes will be stressed with regard to bandwidth availability during the 1st and 4th seconds of the slot, especially assuming the number of duplicate messages that Gossipsub inevitably propagates through the network. It is therefore critical to work on and apply bandwidth-saving improvements to Gossipsub, such as the ones discussed in the following GH issues and ethresear.ch, given that further increases in blob count will be needed in the near future:\n\nDifferent Gossipsub versions per topic (class) \u00b7 Issue #4030 \u00b7 ethereum/consensus-specs \u00b7 GitHub\nGossipsub Dynamic Message Diffusion \u00b7 Issue #4031 \u00b7 ethereum/consensus-specs \u00b7 GitHub\nPeerDAS with significantly less bandwidth consumption\nGossipSub Topic Observation (proposed GossipSub 1.3)\n\n\n\nMeasurement Tooling & Study Preparation\nWe\u2019ve built a tool called net-probe that:\n\npulls node information from the Nebula crawler, which crawls the Ethereum discv5 network every 2 hours.\nconnects to each node discovered during the crawl through Hermes.\ndownloads a carefully chosen volume of data from each node Hermes connected to.\n\nBefore executing the full experiment, we had to find out what\u2019s the right amount of data we should be pulling from each node, in order to: i) saturate the node\u2019s uplink and as a result find out how much bandwidth it\u2019s got available, but at the same time, ii) avoid disrupting the node\u2019s operation. Note that these are Ethereum mainnet nodes, so we had to proceed with care to avoid any disruption.\nWe tested the following parameters:\n\nBlock-by-range RPC calls requesting:  1, 5 10, 20, 30 and 40 blocks,\n10 retries for the RPCs with  1, 5, 10, and 20 blocks,\n5 retries for the RPCs with 30 and 40 blocks, to avoid spamming those peers.\n\nSetting the RPC size\nThe following plot shows the bandwidth we could measure (x-axis) for the nodes we managed to connect to for different RPC size requests, i.e., different number of blocks requested per RPC.\nWe extract the following conclusions from the comparison of the different test studies:\n\nSending RPC calls for 1, 5 and 10 blocks gives widely different results, with higher RPC size requests showing more bandwidth availability. This is a clear indicator for increasing the \u201cblocks requested per RPC\u201d value, as we don\u2019t seem to be saturating the nodes\u2019 bandwidth with smaller RPC calls.\nThe CDF plot shows that the BW measurement barely changes when we request 20, 30  or 40 blocks per RPC, indicating that this RPC size seems to generate enough traffic on the TCP connection to saturate the uplink bandwidth of nodes.\n\nimage1000\u00d7600 65.5 KB\nNOTE: the mean size of the 54,463 downloaded blocks was 101.30 KB of serialized data and 47.76 KB of compressed data.\nSetting RPC retries\nAs expected, there are also some differences in the bandwidth availability (y-axis) observed after consecutive RPC retries (x-axis in the following plot):\n\nThe first 2 RPC responses do not max out the available bandwidth in the TCP connection (see upward trend of line-plots).\nAfter the 3rd one, the measurements become pretty stable for most RPC sizes, i.e., the trend is flattening.\nimage1000\u00d7600 54.7 KB\nThe above plot gives the impression that larger RPC-sized requests result in higher BW availability.\nHowever, taking into account the ratio of requests that are successful at each RPC retry, we see that requesting a larger number of blocks does have an impact on the success rate of the responses. For example, requesting 30 or 40 blocks starts failing after the third retry. Furthermore, requesting consecutively larger RPCs also generates a faster decrease in the success rate over the RPC retries.\nimage1000\u00d7600 56.9 KB\n\nFinal parameters\nGiven the above observations, we have chosen the following values for our production study:\n\nRetries: 6 sequential RPC Requests\nRPC size:  20 blocks per retry\nRequest concurrency of 2 nodes at a time\nDates: 2024-11-23 to 2024-11-28\nInfrastructure: AWS EC2 instance from the following regions:\n\nus-west-1  - California\nus-east-2 - Virginia\neu-central-1 - Frankfurt\nap-southeast-1 - Sydney\n\n\nResulting statistical sample:\n\nThe results we present in the following were obtained from a total of 13,023 unique peers from the Nebula Ethereum database for 6 days (unique online peers over those dates).\nWe collected BW measurements from 9,179 nodes, representing 70,48% of the total online and reachable nodes.\n\n\n\nAnalysis of the bandwidth data\nOverall Bandwidth Availability Results\n\nNOTE: The tool can measure 3 different types of BW measurements:\n\nthe effective BW: the bytes of serialized data we could measure,\nthe compressed BW: the bytes of compressed data we could measure,\nthe wire BW: all the bytes shared over the wire.\n\nFor simplicity and completeness, the rest of the report will refer to the \u201cwire BW\u201d.\n\nThe following plot shows the CDF of the mean wire BW we measured from the 9,179 unique nodes net-probe could successfully connect over these 6 days. The graph shows the BW measurements experienced from each region net-probe was running on, so the figure can be read as follows:\n\nThe BW measurements from our net-probe deployments in the US and Europe show that 40% of the network peers served blocks at a rate below ~20Mbps. This also means that the remaining ~60% of nodes had an upload capacity of more than ~20Mbps.\nMeanwhile, our net-probe deployment in Sydney could only achieve 20Mbps upload speed with 20% of network nodes (see 0.8 mark on the y-axis for the ap-southeast-2 region). This is not surprising, given that the geographical distribution of nodes we observe at ProbeLab shows that almost 65% of the network nodes are deployed in the US and Europe.\nimage1000\u00d7600 71 KB\nCDF of the wire bandwidth for each of the nodes 9,179 successfully connected. Please note that the graph is zoomed within the 0 to 150 Mbps range.\n\nBandwidth Availability per Infrastructure Deployment (cloud vs non-cloud)\nIn the following plot, we present the bandwidth observed for different types of node deployments, namely, those that are hosted on public cloud infra versus those that are not.\nWe observe that nodes operating in cloud providers provide blocks at a higher upload BW rate, having approximately 5Mbps of extra bandwidth available, compared to non-cloud deployments.\nThis marginal difference in bandwidth availability between cloud and non-cloud deployments shows that solo stakers (most likely to use non-cloud infra), are putting enough resources into their node deployment.\nimage1000\u00d7600 75.9 KB\nCDF of the wire bandwidth segregated by the nodes\u2019 hosting type ( 4,409 cloud vs 4,770 non-could nodes).\nBandwidth Availability per Client Implementation\nComparing the upload bandwidth by the client implementation of the remote nodes, the following image shows that most clients share similar distributions until the 70th percentile, with the exception of Lodestar, which we discuss below. From that point on, the differences in the distribution\u2019s tail become slightly more visible.\nOur takeaway points from this measurement are:\n\nNo major differences between different client implementations in terms of bandwidth availability.\nLodestar seems to be outperforming the rest by at least 10Mbps at the 60th percentile.\nLodestar\u2019s improved performance is very likely due to the fact that more than 80% of its nodes are based in the US or EU (link), which as we\u2019ve seen earlier present higher bandwidth availability.\nimage1000\u00d7600 68.9 KB\nCDF of the wire bandwidth segregated by the nodes\u2019 client.\n\nBandwidth Availability per Region\nBandwidth measurements, as well as latency measurements, are highly subjected to the location from where the data is being generated. Thus, we\u2019ve chosen our net-probe deployments to represent: the most popular regions for Ethereum node deployments (US and EU), and the one geolocated the furthest (AUS). The following image provides the CDF of the wire BW of nodes from the four regions aggregated by continents. The measurements indicate that, as expected, regions further away present lower bandwidth availability. In this case, we observe that:\n\nOceania and Africa are at least 8 to 10 Mbps behind the rest of the regions, providing a non-desirable 90th percentile below the 20 Mbps mark.\nNodes in Asia and South America achieve 15 to 25 Mbps between the 50th and 80th percentile.\nNodes in Europe match the BW distribution of NA nodes until the 40th percentile, which later diverted towards a median of 24 Mbps up to the 90th percentile of 60 Mbps.\nimage1000\u00d7600 65.4 KB\nCDF of the wire bandwidth segregated by the nodes\u2019 continents\n\nBandwidth Availability per Slot Window\nEach Ethereum slot is split into different parts each of which is allocated to different duties over the 12 seconds slot duration. Trying to correlate the available wire BW across the slot time, the following image shows the mean wire BW availability of nodes for each node hosting type at each second within the slot. We observe the following:\n\nCloud-hosted nodes have a higher mean of available BW, but both cloud and non-cloud nodes\u2019 portion of available bandwidth fluctuates visibly throughout the slot.\nThere is clearly less bandwidth availability from the 1st to the 4th second of the slot, which is the window when blocks and blobs are broadcasted to the network. The bandwidth drop we were able to measure represents a 9,5% to 10% drop for non-cloud users, while it was a 13% of bandwidth reduction for nodes located in data centers.\nDespite this drop in the mean amount of bandwidth available, there is still ~19Mbps for non-cloud hosted nodes and ~23Mbps for cloud hosted nodes available. This is a strong indication that a slight increase in the blob count target and max values should not be disruptive to the network.\nimage1000\u00d7600 71.4 KB\nMean wire bandwidth availability per deployment type over time within the slot.\n\nRPC Success Rates Throughout the Slot\nAs a byproduct of our study, we captured the number of successful RPC replies that we got throughout the slot, presented by the \u201cNumber of data points\u201d available to us. There is a dip in the number of data points (i.e., successful RPCs) between the 2nd and 7th/8th second of the slot when blocks, blobs and attestations are getting broadcasted, propagated and processed.\nimage1000\u00d7600 74.1 KB\nHistogram of the successful data points at each slot-second.\nCurrent blob count per block\nHaving seen node bandwidth availability from several perspectives, it is important to understand how many blob transactions have been included in blocks during the measurement period. This aspect is important in order to asses what are the implications of a blob count increase, i.e., how many blobs resulted in the bandwidth availability that we\u2019ve seen and how much space is there for extra blobs.\nThe following graph shows the number of blobs that were included in each slot throughout the 6 days of study. Here are our observations:\n\n35% of the slots had no blobs at all.\n42% of the slots included 5 to 6 blobs.\nGiven the current 3/6 target/max blob values, 42% of the bandwidth measurements we did were performed at almost the blob max capacity.\nAssuming demand will keep up when the blob count target and max increase to 6/9, we can project that in ~42% of the cases there will be 9 blobs per block and in ~50% there will be 6 blobs per block, or less.\n\nGiven the above, our take is that we don\u2019t see any critical objection to the current desired blob target and max values of 6/9 discussed in the All Core Dev CL Call on Thursday 28th of November.\nimage1000\u00d7600 26.7 KB\nCDF of the number of blobs per slot.\nConclusions\nFrom this extensive set of measurements, we extract the following conclusions:\n\nRepresenting the perspective of at least 65% of the network (nodes in NA and EU), we\u2019ve measured that only 30% to 40% of the measured peers reported an upload link below the 20 Mbps mark.\nOn the other hand, when nodes are located in more remote locations like Sydney, only 20% of connections went above 20 Mbps.\nWe need to keep in mind that this available BW is effectively doubled if we consider that 80% of the blocks achieve at least a snappy compression rate of 2.\nNodes deployed in cloud infra seem to have at least 5 Mbps more bandwidth available than non-cloud nodes.\nThe measured bandwidth throughout the slot time shows that there is, on average, a 9% to 13% drop in bandwidth availability between seconds 1 and 4.. The precise moment where the block plus blobs are meant to be broadcasted to the network.\nWe could spot a slight decrease in the success rate of the RPC requests we sent during the \u201cbusy\u201d moments of the slot, which could affect, to some degree, the sampling of blobs in PeerDAS.\nWith the current discussion around increasing the blob count from a target and max values of 3/6 to 6/9, the presented metrics align with the EthPandaOps recent post (link) that it should be a reasonable increase with the current network state. The network already has 50% of slots at the targeted blob count goal or beyond, while 35% of slots don\u2019t even have associated blobs.\n\n", "\nAgent-based Simulation of Execution Tickets\nby Pascal Stichler (ephema labs)\nMany thanks to Julian, Jonah, Marc and Chris for valuable feedback and special thanks to Barnab\u00e9 for prompting the research in the first place and guiding it.\n\nPlease note: This is only a summary of the findings. The complete research report can be found here and the code is available on Google Colab and Github. Instructions on how to run the simulation are shared here. Simulation results are available in this folder and a recording of a presentation at Devcon 2024 on the topic can be found here.\n\nTL\u2019DR\n\nWe reviewed and holistically scoped the mechanism design space for Execution Tickets and evaluated potential concrete mechanism designs through a structured review and agent-based simulation.\nExecution Tickets are an effective mechanism for capturing MEV rewards at protocol level, however block builder decentralization and resistance to Off-Chain Agreements (OCA-proofness) remains a challenge.\nAuction-based pricing formats perform well on capturing MEV rewards, while an adapted EIP-1559 style pricing shows less favorable dynamics.\nEnabling a secondary market in the simulation fostered decentralization and MEV capture.\nThe outcomes of the simulation on centralization and MEV capture are very sensitive to execution ticket holder attributes, particularly the ability to extract MEV and volatility specialization. This is in line with previous literature.\n\nBackground\nExecution Tickets are currently discussed as a promising next evolutionary step for enhancing Ethereum\u2019s block space allocation mechanism. With Execution Tickets the protocol sells the right for execution block proposing. This is done by offering tickets, which allow ticket holders to participate in a lottery to be drawn as an execution block proposer in the future. It separates consensus rewards such as priority tips paid by users from execution rewards such as Maximum Extractable Value (MEV). It aims to foster decentralization among beacon chain validators by reducing the sophistication requirements for validators by removing optimization shenanigans like timing games [1]. Further, it enables protocol-level capture of MEV. Thereby, it essentially aims to tackle two problems: the \u201callocation\u201d problem that currently MEV rewards leak to block proposers and secondly the \u201ccentralization\u201d problem of the MEV supply chain [2].\nExecution Tickets were initially introduced by Justin Drake here outlining the motivation. He explains the general mechanism design of separating consensus from execution layer rewards and selling Execution Tickets for participating in a lottery to be chosen as the execution block proposer. Mike Neuder formalized and more clearly outlined the mechanism design here and also collected several open questions. We provide a more colloquial introduction on how and why protocol mechanisms evolved here. In a good economic analysis here and here by Jonah Burian, Davide Crapis and Fahad Saleh it is shown that when priced correctly, Execution Tickets can internalize all value generated MEV rewards at protocol level. However, with the important limitation that \u201cthe protocol must be capable of selling tickets at their intrinsic value\u201d. Further, Barnab\u00e9 Monnot provides a good overview of protocol considerations over time and how Execution Auction (EA) as a special implementation of Execution Tickets with a fixed 32-slot advance period could function here. The idea of Execution Auctions is further extended by Thomas Thiery here by introducing a randomization element (randEA).\nAs the previous literature often focuses on the bigger picture and leaves out the knits and grits of the mechanism design we plan to focus on the details of the mechanism design. For example, what is the best pricing mechanism for selling Execution Tickets? Should there be a variable or fixed amount of tickets [3]? Shall tickets be expiring and resaleable? Or maybe returnable to the protocol?\nTo provide insights into answering these questions, we developed a theoretical framework identifying the primary objectives of an Execution Ticket mechanism design, metrics how to measure them, and propose desirable price characteristics. Further, we outlined the main mechanism design parameters and their possible expressions. Based on a theoretical evaluation and an agent-based simulation, we draw primary conclusions on the mechanism design.\nMethodology\nThe approach to validate potential mechanism designs for Execution Tickets is twofold.\nIn the first step, we conduct a theoretical analysis of the objectives that the mechanism aims to optimize and propose several metrics to measure the achievement of these objectives. Next, we outline the design space of possible mechanism attributes and their potential values. This includes properties of the tickets as well as potential pricing and allocation mechanisms (e.g. auction-based formats vs. quoted-price formats). Based on a preliminary theoretical analysis, potential concrete mechanism designs are proposed and evaluated using a theoretical framework.\nIn the second step, these findings are verified using an agent-based simulation. Simulations (e.g. EIP-1559 simulation) have proven to be a suitable tool to estimate the impact of potential mechanism design choices. The simulation emulates the allocation, trading, and redemption processes of Execution Tickets. The scope of the simulation is to run the previously designed configurations and compare them based on the objectives. Furthermore, conclusions can be drawn from the simulation about each parameter to determine favorable choices. The simulation is implemented in Python using existing industry standard frameworks (radCAD). For brevity reasons the simulation assumptions and specifications are not outlined here, but can be found in the research report in Chapter 5.1.\nMechanism Objectives & Design Space\nTo outline the possible design mechanisms, we first outline the desired mechanism behavior and the solution space of different configurations before evaluating them.\n\n\n\n\n\nObjectives\nMeasurement metrics\n\n\n\n\nOptimization Parameters\n1. Decentralization  2. MEV Capture  3. Block Producer Incentive Compatible (BPIC)\n1. Market share, Nakamoto-coefficient & Herfindahl-Hirschman Index  2. MEV-Share Protocol\n\n\nPricing Behavior\n1. Price Predictability  2. Price Smoothness  3. Price Accuracy\n1. Garman-Klass (GK) Measure  2. V(\u0394p) 3. MEV-Share Protocol\n\n\n\nTable 1: Summary of important objectives\nExecution Tickets aim to optimize two key objectives: fostering decentralization among beacon chain proposers and capturing Maximum Extractable Value (MEV) at the protocol level. Thereby it addresses the two key goals of the Ethereum roadmap segment \u201cThe Scourge\u201d: (i) Minimize centralization risks at Ethereum\u2019s staking layer and (ii) Minimize risks of excessive value extraction from users.\nDecentralization is a key aspect, as it prevents several unfavorable dynamics. In the context of Execution Tickets it can be divided into beacon chain validator and execution chain decentralization. Execution chain centralization can happen on ticket holder or on block builder level [2]. Generally, decentralization ensures liveness in the sense that not a single actor can voluntarily or involuntarily halt the chain and impair liveness. Further, it contributes to censorship resistance [4]. For these reasons, beacon chain validator decentralization is paramount. Execution chain proposer decentralization is less critical under the assumption that beacon chain validators can force certain transactions into the block, for example with a version of inclusion lists and JIT top-of-block auctions. In this case, execution chain proposer decentralization is mainly relevant to avoid liveness risk and to foster competitive bidding for ETs.\nCapturing MEV at the protocol level is essential as it removes MEV rewards from beacon chain validator rewards and most likely burning the rewards is the most neutral way to do so. Additionally, from a game theory perspective the mechanism should adhere to certain criteria outlined in [5]. Firstly, block producers must be incentivized to participate and propose non-empty blocks, described as Block Producer Incentive Compatible (BPIC) [6]. Secondly, it should be resistant to Off-Chain Agreements (OCA-proof), meaning that participants cannot mutually benefit from making off-chain agreements. Lastly, it should be Dominant-Strategy Incentive Compatible (DSIC), meaning for each participant there is a dominant strategy they can apply regardless of the behavior of other participants. For example, in a sealed-bid first price auction participants need to theorize about the intrinsic valuations of other participants and their bidding strategies to calculate their bid, making it not DSIC and thereby making it more complicated for participants and potentially not incentivizing to reveal their true intrinsic valuation.\nWe propose to measure decentralization through three metrics: Nakamoto coefficient, Herfindahl-Hirschman Index (HHI) [7], and market share of the largest ticket holder. Further, MEV capture can be assessed by the share of MEV rewards of ticket holders captured at the protocol level.\nExecution Ticket price behavior we see of secondary importance, however still worthwhile to consider. Thereby we focus on three aspects: price predictability, smoothness, and accuracy. Price predictability is crucial for validators to participate in auctions and plan long-term. As summarized in [8], volatility can be a measure for price predictability in financial markets, following methods like the Garman-Klass (GK) measure [9]. The Garman-Klass measure is traditionally used in financial markets to measure volatility by including the daily opening, low, high and closing price. For our purpose the time interval needs to be adjusted, e.g. to epoch-based intervals. Price smoothness ensures stability during market fluctuations, reducing risk for ticket holders, with the variance of consecutive price changes (V(\u0394p), essentially being autocorrelation of prices) proposed as a measurement. Lastly, price accuracy reflects the true value of ETs, aiming to capture the maximum share of MEV while remaining attractive to participants, measured similarly to MEV capture.\nIn table 2 we outline the design space of possible configurations of Execution Tickets.\n\n\n\n\nTicket Attributes\nConfigurations\n\n\n\n\nAmount of tickets\nVariable / Fixed\n\n\nExpiring tickets\nYes / No\n\n\nRefundability\nYes / No (unallocated & allocated)\n\n\nResalability\nYes / No (unallocated & allocated)\n\n\nEnhanced Lookahead\nNo / Yes (x epochs)\n\n\nPossible Pricing Mechanisms\nFPA, SPA, EIP-1559 style, AMM style\n\n\nTarget Amount\n# of tickets (for variable / fixed)\n\n\n\nTable 2: Outline of possible execution ticket configurations\nWhile most attributes are straightforward, we will provide some background on the pricing mechanisms. Unlike MEV-Boost, where rewards may go to block producers, Execution Ticket earnings are intended to benefit protocol token holders by being burned, thereby increasing social welfare [10]. Fixed pricing mechanisms are deemed inefficient for maximizing social welfare [11], so the focus is on dynamic pricing mechanisms.1\nThe pricing mechanisms are categorized into two main categories being auction-based and adaptive quoted price formats:\n\nFirst-Price Auctions (FPA): Bidders submit bids without knowing others\u2019 bids, and the highest bidder wins and pays their bid amount. FPAs often lead to bid shading, where bidders underbid their true valuations, resulting in inefficiencies and high volatility. Sealed-bid first price auctions are not DSIC (Dominant Strategy Incentive Compatible) [5]. Open ascending-bid first price auctions can be DSIC (h/t to Julian for pointing this out!). As they behave similarly to SPAs, we focused on sealed-bid FPAs.\nSecond-Price Auctions (SPA): Also known as Vickrey auctions, bidders submit sealed bids, and the highest bidder wins but pays the second-highest bid. This format encourages truthful bidding since bidders pay less than or equal to their true intrinsic value. While SPAs are almost OCA-proof (Off-Chain Agreement proof), they may be susceptible to manipulation through fake bids [13]. However, since Execution Ticket earnings are burned rather than rewarded to validators, this risk is mitigated but might make them more susceptible to off-chain agreements.\nAdapted EIP-1559 Pricing: An adapted version of EIP-1559 for Execution Tickets involves the protocol quoting a price that adjusts similarly to EIP-1559. However, while for EIP-1559 the adjustment is based on the gas usage, for Execution Tickets it needs to be based on the number of outstanding tickets relative to a target amount. Tickets could either be sold on a continuous basis where ticket holders can always buy a ticket from the protocol if they desire or in a batch process where at each slot between zero and a specified maximum of tickets are sold. While EIP-1559 has been effective in maintaining gas usage near the target [14], its retroactive price adjustments may lag during MEV spikes making it more challenging for Execution Tickets.\nAdapted AMM-like Pricing: The adapted version of an AMM-like pricing entails the protocol dynamically updating the price of the tickets based on a bonding curve and the amount of outstanding tickets. Here as well a target amount of outstanding tickets needs to be defined and the bonding curve function needs to be adapted and carefully designed. In the research paper we outline three options how this might be adapted and implement one in the simulation. However, this remains the scope of future research on how to best adapt it.\n\nPotential Mechanism Designs\nTo substantiate the parameters, several possible mechanism designs are outlined. Given that based on the categorial parameters alone already 512 configurations are possible2, only sample mechanism designs are evaluated. In more detail, the following configurations are evaluated:\n\n\n\n\n\nSimple FPA auction\nJIT second price slot auction\nFlexible 1559-style\nFixed SPA\nFlexible, refundable AMM\nFixed, resellable FPA\n\n\n\n\nAmount of tickets\nFixed\nFixed\nFlexible\nFixed\nFlexible\nFixed\n\n\nExpiring tickets\nYes\nYes\nNo\nNo\nNo\nNo\n\n\nRefundability\nNo (unallocated & allocated)\nNo (unallocated & allocated)\nNo (unallocated & allocated)\nNo (unallocated & allocated)\nYes (unallocated)\nNo (unallocated & allocated)\n\n\nResalability\nNo (unallocated & allocated)\nYes (allocated)\nYes (unallocated & allocated)\nNo (unallocated & allocated)\nNo (unallocated & allocated)\nYes (unallocated & allocated)\n\n\nEnhanced Lookahead\nNo\nReduced\nYes for Execution Validators\nYes for Execution Validators\nNo\nNo\n\n\nPricing Mechanisms\nFPA\nSPA\n1559-style\nSPA\nAMM\nFPA\n\n\nTarget Amount\n32\n1\nundefined\n1024\nundefined\n1024\n\n\n\nTable 3: Overview of possible mechanism design configurations\nSimulation Results\nTable 4: Simulation results on selected mechanism designs1256\u00d7956 95.3 KB\nGenerally, the simulation results of over 300 simulation runs show that in all configurations decentralization remains a challenge. None of the configuration scores particularly well on the decentralization metrics. This is driven by the diverse abilities of ticket holders (based on [15], [16]) and the fact that in most scenarios the bids are based on expected future valuations which leaves out specialization factors. It shows that in cases with a secondary market enabled, the centralization forces are reduced. This derives from specialized ticket holders being able to more accurately estimate the true value of MEV for a slot in just-in-time (JIT) auctions and thereby winning the auction. With regards to MEV capture, we can see different attributes emerge. The auction formats generally score well, similarly the AMM-style pricing scores well. The 1559-style pricing is capturing less MEV due to a step-wise and less dynamic price adaptation mechanism. With regards to the price predictability, smoothness and accuracy we can observe that the auction formats that operate with a longer lookahead are very predictable and smooth, while JIT auctions and a 1559-style pricing are less smooth.\nFindings on Auction Formats\nFirst Price Auctions\nWith regards to first price auctions, we saw a \u201cwinner\u2019s curse\u201d play out, in the terms that assuming that bidders have differing intrinsic value expectations for a ticket which are following a normal distribution, the most optimistic bidders wins. And the most optimistic bidder with the highest valuation overestimates the value the most and thereby makes a loss on the trade. This is a known problem of auctions (e.g. [17]). However, noteworthy to point out, as this leads to higher \u201crisk-adjustments\u201d by bidders which in turn could lead to reduced MEV capture by the protocol.\nWith regards to the simulation it shows that first price auctions generally perform well, however two things need to be critically challenged here. Firstly, we have implemented it as a sealed bid auction, as we assume the operational communication overhead for a leaderless auction with ascending bids is too high. Therefore, holding sealed-bid on-chain auctions must be feasible. As outlined in the research report, several proposals for this are currently being discussed, however still in the earlier stages [18], [19]. Secondly, as sealed-bid first price auctions are not DSIC, no single dominant bidding strategy exists. Hence, in the simulation the bidding is based on a heuristical bidding strategy where bidders have no information on the intrinsic valuations of other bidders. This assumption will not hold true in multi-round scenarios of Execution Ticket selling. So more sophisticated bidding strategies based on historical bids of competitors might emerge that might potentially reduce the captured MEV. So it is unclear yet if first price auctions can be actually designed in this scenario in a way to behave differently than second price auctions.\nSecond Price Auctions\nFor second price auctions we observed that the MEV capture highly depends on the competitiveness of the specific simulation. In cases with at least two similarly strong ticket holders, the MEV capture was high. However, on average it was only medium, given the missing competition.\n1000\u00d7600 53 KB\nFigure 1: Example simulation results for second price auctions with two similarly capable ticket holders (Source: 2024-09-24_10-52 UTC, runs: 10, time steps: 1000)\nEIP-1559 Style Pricing\nAs outlined above, the EIP-1559 pricing needs to be adapted to work with Execution Tickets and we have implemented it as a batch process. However, we observe that this leads to self-reinforcing oscillating ticket prices. Even adjusting the price adjustment factor does not lead to better outcomes in our simulations. This leads to the conclusion that a batch update process is not sufficient. How a continuous price update process can be technically implemented in a decentralized setting remains an open question. Overall, the pricing mechanism needs to be carefully designed to achieve the desired price behavior.\n1000\u00d7600 35.8 KB\nFigure 2: Price curve for EIP-1559 style pricing\nFurther, in certain simulations4 we have observed that if one ticket holder has a significantly higher willingness to pay, the prices stabilize at a point where only this ticket holder is able to purchase tickets, leading to a high centralization.\nAMM-style Pricing\nFor the AMM-style pricing as outlined above it needs to be adapted to be suitable for Execution Tickets. Running configurations with AMM\u2013style pricing shows that the pricing mechanism can be sensitive to the adjustment factor. A too slow adoption does not accurately capture the demand, a too large adaption factor is not granular enough to differentiate the expected valuations and would lead to a latency race.\nHowever, the simulations show promising results that this mechanism is able to capture a high level of MEV. From an operational perspective it remains to be investigated how this could be implemented to suit the selling process needed for Execution Tickets.\nConclusion on Auction Formats\nTaking into consideration the different observations, based on the simulation results we conclude that an auction based format, most probably second price auction, is the most feasible format. It leads to a high captured MEV, is DSIC and leads to favorable price properties in the simulation. An AMM-style pricing seems also to be a promising solution, however more open design questions on mechanism and implementation remain.\nOne relevant question remains open around OCA-proofness, in case the ET earnings are burned. There might be a sybil attack vector where block builders bribe the actor / committee defining the winning bid and thereby being able to achieve a lower price. E.g. if the winning bid is 10 ETH, the block builder however pays the committee members 5 ETH to artificially set the winning bid price at 1 ETH, there could be a 4 ETH profit margin. To avoid this, bids or prices would need to be on-chain which is not feasible given the time horizon of the auction. Another option could be a leaderless auction as outlined by [20].\nFindings on Ticket Attributes\nWe observe that the question of a fixed vs. a variable amount of tickets is closely related to the pricing mechanism. For certain mechanisms, fixed amounts of tickets make more sense (auctions) while for others (EIP-1559 and AMM-style) a flexible amount is better. Hence, we see this more as a secondary attribute that is deducted from the pricing mechanism.\nRegarding expiring tickets, we observe in the simulation that especially for short expiry times the MEV capture is impaired, as ticket buyers need to discount the value of a ticket on the primary market and on the secondary market since the possibility of a ticket expiry without redemption needs to be priced in. This leads to generally lower captured MEV values. Further, we observe that it has secondary complications as the pricing of each actor becomes more sophisticated as the expiry period, outstanding tickets etc. need to be factored in. This leads to the conclusion that non-expiring tickets seem to be the favorable configuration.\nRegarding refundability, we only observe limited effects on the market dynamics with the tested discount (around 20 %). It leads to more security for ticket holders. However, this depends on the discount. Further it is closely related with the secondary market. In case a secondary market exists, this option is often more attractive to dispose of tickets. It shows that allowing for refundability does not influence the mechanism in a substantial way and complicates the design choices as well as the decisions for ticket holders. Hence, the preliminary analysis leads to the conclusion that tickets shall not be refundable.\nRegarding the secondary market, an interesting finding is that this increases decentralization. Due to the ability of more specialized ticket holders to buy tickets just-in-time in periods where they are able to capture higher MEV due to specialization. Further, it leads to overall higher MEV captured due to reduced risks for the primary ticket holders. Additionally, we observe that in some configurations with discrete pricing (e.g. AMM-style pricing) it leads to arbitrage opportunities, if the AMM-pricing is not adapting fine granularly enough and tickets can be bought by the ticket holder with the lowest latency and then be resold at a higher price at the secondary market. Given that also from a technical perspective it is difficult to prevent a secondary market, a preliminary recommendation is rather embrace the benefits of it and try to foster it.\nLimitations\nOur research did not focus on the beacon round attestation and the secondary effects ETs might have on it.\nAdditionally, we did not focus on the specific details of inclusion lists. They are briefly discussed in the research report as a potential mechanism to ensure liveness, but in the simulation and configurations it will not be a focus of the work. This ties closely to multi-block MEV. As we have shown in our previous work, it has historically not been structurally observed, however might be a concern for Execution Tickets. The topic is hence briefly discussed in the research report, but not in-depth evaluated and not implemented in the simulation. Further, timing games are not included in the simulation. Additionally, assuming sealed-bid auctions, we work with static demand functions of the ticket holders that do not take into consideration the bids of other ticket holders. In addition, considerations around private order flow are not modeled in the simulation. Furthermore, the role of relays is left out and we don\u2019t simulate missed blocks and missed block penalties.\nRegarding the pricing mechanisms, we propose initial versions of how they can be designed, however leave the verification and formal definition to future research. This includes the more in-depth research of specific parameters such as adjustment steps for EIP-1559-style pricing and others. We only look at this from an exploratory perspective.\nFurther, we exclude a more in-depth analysis around the burning mechanism of the earnings from the Execution Ticket sales. As outlined in [21], burning mechanisms usually impair the OCA-proofness of mechanisms.\nConclusion\nExecution Tickets present a promising next evolutionary step for enhancing Ethereum\u2019s block space allocation mechanism. It separates consensus rewards from execution rewards and sells the execution rights in an effective manner. It aims to foster decentralization among beacon chain validators and enables protocol-level capture of Maximum Extractable Value (MEV).\nWe developed a theoretical framework identifying three primary objectives of an Execution Ticket mechanism design: decentralization, MEV capture, and Block Producer Incentive Compatibility (BPIC). Further, we propose metrics on how to measure the objectives. For decentralization we propose to use the highest market share, Nakamoto coefficient, and Herfindahl-Hirschman Index, while for MEV capture we propose to measure the MEV share of the protocol from Execution Ticket holder earnings. Further, the three price characteristics of price predictability, smoothness and accuracy are identified as desired attributes.\nTo evaluate the parameters and configurations, we implemented an agent-based simulation and based on over 300 simulation runs several findings are concluded. Results indicate that while none of the mechanisms scores particularly well on decentralization, enabling a secondary market reduces centralization by allowing specialized ticket holders to purchase tickets just-in-time. Regarding MEV capture, auction formats and AMM-style pricing performed well, whereas EIP-1559-style pricing captures less MEV and has stronger price fluctuations. Auction formats with longer lookahead periods demonstrated favorable price predictability and smoothness while scoring slightly less favorable on price accuracy.\nBased on this, a second-price auction format seems most promising as it achieves high MEV capture, adheres to Dominant-Strategy Incentive Compatibility (DSIC) and exhibits favorable price characteristics. First-price auctions and AMM-style pricing formats show promising results in the simulation as well, however leave more questions open from a theoretical mechanism perspective. Non-expiring tickets score better as they avoid impairing MEV capture due to discounted valuations from expiry risk. Refundability was found to have limited impact on market dynamics and adds complexity; thus, non-refundable tickets are suggested. Embracing a secondary market seems favorable, as it enhances decentralization and increases overall MEV capture. Nevertheless, in line with [22] and [23] we observe that the decentralization of the builder market is challenging and highly depends on the MEV extraction capabilities of the top builders.\nOverall, inline with previous theoretical work [2], [22], [24] we conclude that Execution Tickets pose a promising mechanism to foster beacon chain validator decentralization and capture MEV at protocol level. However, questions remain around block builder centralization, proneness to off-chain agreements and multi-block MEV.\nReferences\n[1] C. Schwarz-Schilling, F. Saleh, T. Thiery, J. Pan, N. Shah, and B. Monnot, \u201cTime is Money: Strategic Timing Games in Proof-of-Stake Protocols,\u201d May 2023, [Online]. Available: [2305.09032] Time is Money: Strategic Timing Games in Proof-of-Stake Protocols\n[2] J. Burian, D. Crapis, and F. Saleh, \u201cMEV Capture and Decentralization in Execution Tickets,\u201d Aug. 21, 2024, arXiv: arXiv:2408.11255. Accessed: Oct. 14, 2024. [Online]. Available: [2408.11255] MEV Capture and Decentralization in Execution Tickets\n[3] C. Schlegel, \u201cInelastic vs. Elastic Supply: Why Proof of Stake Could Be Less Centralizing Than Execution Tickets - Research,\u201d The Flashbots Collective. Accessed: Oct. 14, 2024. [Online]. Available: Inelastic vs. Elastic Supply: Why Proof of Stake Could Be Less Centralizing Than Execution Tickets - Research - The Flashbots Collective\n[4] J. Lee, B. Lee, J. Jung, H. Shim, and H. Kim, \u201cDQ: Two approaches to measure the degree of decentralization of blockchain,\u201d ICT Express, vol. 7, no. 3, pp. 278\u2013282, Sep. 2021, doi: 10.1016/j.icte.2021.08.008.\n[5] T. Roughgarden, \u201cTransaction Fee Mechanism Design,\u201d ACM SIGecom Exch., vol. 19, no. 1, pp. 52\u201355, 2021, doi: 10.1145/3476436.3476445.\n[6] M. Bahrani, P. Garimidi, and T. Roughgarden, \u201cTransaction Fee Mechanism Design with Active Block Producers,\u201d 2023, [Online]. Available: [2307.01686v2] Transaction Fee Mechanism Design with Active Block Producers\n[7] L. Heimbach, L. Kiffer, C. Ferreira Torres, and R. Wattenhofer, \u201cEthereum\u2019s Proposer-Builder Separation: Promises and Realities,\u201d Proc. ACM SIGCOMM Internet Meas. Conf. IMC, pp. 406\u2013420, May 2023, doi: 10.1145/3618257.3624824.\n[8] S.-H. Poon and C. W. J. Granger, \u201cForecasting Volatility in Financial Markets: A Review,\u201d J. Econ. Lit., vol. 41, no. 2, pp. 478\u2013539, Jun. 2003, doi: 10.1257/002205103765762743.\n[9] S.-K. Tan, J. S.-K. Chan, and K.-H. Ng, \u201cOn the speculative nature of cryptocurrencies: A study on Garman and Klass volatility measure,\u201d Finance Res. Lett., vol. 32, p. 101075, Jan. 2020, doi: 10.1016/j.frl.2018.12.023.\n[10] A. Kiayias, P. Lazos, and J. C. Schlegel, \u201cWould Friedman Burn your Tokens?,\u201d Papers, 2023, [Online]. Available: Would Friedman Burn your Tokens?\n[11] V. Buterin, \u201cBlockchain Resources Pricing.\u201d 2019. Accessed: Mar. 21, 2024. [Online]. Available: research/papers/pricing/ethpricing.pdf at 139e3dd83b06fae918792c495b8ccd0d1635b0d4 \u00b7 ethereum/research \u00b7 GitHub\n[12] M. Neuder, P. Garimidi, and T. Roughgarden, \u201cOn block-space distribution mechanisms - Proof-of-Stake / Block proposer,\u201d Ethereum Research. Accessed: Oct. 28, 2024. [Online]. Available: On block-space distribution mechanisms\n[13] M. Akbarpour and S. Li, \u201cCredible Auctions: A Trilemma,\u201d Econometrica, vol. 88, no. 2, pp. 425\u2013467, 2020, doi: 10.3982/ECTA15925.\n[14] Y. Liu, Y. Lu, K. Nayak, F. Zhang, L. Zhang, and Y. Zhao, \u201cEmpirical Analysis of EIP-1559: Transaction Fees, Waiting Time, and Consensus Security,\u201d Proc. ACM Conf. Comput. Commun. Secur., pp. 2099\u20132113, 2022, doi: 10.1145/3548606.3559341.\n[15] S. Yang, K. Nayak, and F. Zhang, \u201cDecentralization of Ethereum\u2019s Builder Market,\u201d May 2024, [Online]. Available: [2405.01329v3] Decentralization of Ethereum's Builder Market\n[16] B. \u00d6z, D. Sui, T. Thiery, and F. Matthes, \u201cWho Wins Ethereum Block Building Auctions and Why?,\u201d Jul. 18, 2024, arXiv: arXiv:2407.13931. Accessed: Oct. 02, 2024. [Online]. Available: [2407.13931] Who Wins Ethereum Block Building Auctions and Why?\n[17] M. H. Bazerman and W. F. Samuelson, \u201cI Won the Auction But Don\u2019t Want the Prize,\u201d http://dx.doi.org/10.1177/0022002783027004003, vol. 27, no. 4, pp. 618\u2013634, 1983, doi: 10.1177/0022002783027004003.\n[18] H. S. Galal and A. M. Youssef, \u201cVerifiable Sealed-Bid Auction on the Ethereum Blockchain,\u201d 2018, 2018/704. Accessed: Oct. 14, 2024. [Online]. Available: Verifiable Sealed-Bid Auction on the Ethereum Blockchain\n[19] P. Momeni, S. Gorbunov, and B. Zhang, \u201cFairBlock: Preventing Blockchain Front-Running with Minimal Overheads,\u201d in Security and Privacy in Communication Networks, vol. 462, F. Li, K. Liang, Z. Lin, and S. K. Katsikas, Eds., in Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering, vol. 462. , Cham: Springer Nature Switzerland, 2023, pp. 250\u2013271. doi: 10.1007/978-3-031-25538-0_14.\n[20] D. White, D. Robinson, L. Thouvenin, and K. Srinivasan, \u201cLeaderless Auctions,\u201d Paradigm. Accessed: Oct. 04, 2024. [Online]. Available: Leaderless Auctions - Paradigm\n[21] T. Roughgarden, \u201cTransaction Fee Mechanism Design for the Ethereum Blockchain: An Economic Analysis of EIP-1559,\u201d 2020, [Online]. Available: [2012.00854v1] Transaction Fee Mechanism Design for the Ethereum Blockchain: An Economic Analysis of EIP-1559\n[22] M. Bahrani, P. Garimidi, and T. Roughgarden, \u201cCentralization in Block Building and Proposer-Builder Separation,\u201d Jan. 2024, [Online]. Available: [2401.12120] Centralization in Block Building and Proposer-Builder Separation\n[23] M. Pan, A. Mamageishvili, and C. Schlegel, \u201cOn sybil-proof mechanisms,\u201d Jul. 22, 2024, arXiv: arXiv:2407.14485. Accessed: Oct. 28, 2024. [Online]. Available: [2407.14485] On sybil-proof mechanisms\n[24] J. Burian, \u201cThe Future of MEV - An Analysis of Ethereum Execution Tickets,\u201d 2024. [Online]. Available: [2404.04262] The Future of MEV\n\n1 Note that further pricing mechanism proposals exist with winning changes being proportional to the paid price as e.g. outlined in [12]\n2 2 (Amount of Tickets) * 2 (Expiring Tickets) * 4 (Refundability) * 4 (Resalability) * 2 (Enhanced Lookahead) * 4 (Pricing Mechanisms)\n3 Results based on 10 runs with 1000 timesteps for each configuration. Color coding of results based on literature and subjective judgment\n4 E.g. see simulation results 2024-05-14_18-09_1_1000_EIP-1559 for details\n"]